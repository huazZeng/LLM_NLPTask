{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PJ5\n",
    "## 实验概述\n",
    "- Task1：NEG\n",
    "  **效果不如分类器模型，即使是GPT3.5**\n",
    "  - Test1：采用Qwen/Qwen2.5-0.5B-Instruct进行提示词工程测试\n",
    "  - Test2：采用OpenAI API进行提示词工程测试\n",
    "\n",
    "- Task2：Summary\n",
    "  \n",
    "  **由于数据量过大，难以完全测试，总是会测试一半然后CUDA OOM，因此只测试了部分数据**\n",
    "  **结果提高有限**\n",
    "  - Test1：采用Qwen/Qwen2.5-0.5B-Instruct进行few-shot提示词测试\n",
    "  - Test2：基于Qwen/Qwen2.5-0.5B-Instruct进行微调进行测试\n",
    "\n",
    "## 定义数据集\n",
    "* 定义Dataset：包含CHisIECDataset与 SummaryDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T15:24:16.065696Z",
     "iopub.status.busy": "2024-11-16T15:24:16.065330Z",
     "iopub.status.idle": "2024-11-16T15:24:17.096676Z",
     "shell.execute_reply": "2024-11-16T15:24:17.096088Z",
     "shell.execute_reply.started": "2024-11-16T15:24:16.065672Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from typing import List, Optional\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import json\n",
    "class CHisIECDataset(Dataset):\n",
    "    label_label_id_mapping = {\n",
    "        \"O\": 0,\n",
    "        \"B-PER\": 1,\n",
    "        \"I-PER\": 2,\n",
    "        \"E-PER\": 3,\n",
    "        \"S-PER\": 4,\n",
    "        \"B-LOC\": 5,\n",
    "        \"I-LOC\": 6,\n",
    "        \"E-LOC\": 7,\n",
    "        \"S-LOC\": 8,\n",
    "        \"B-OFI\": 9,\n",
    "        \"I-OFI\": 10,\n",
    "        \"E-OFI\": 11,\n",
    "        \"S-OFI\": 12,\n",
    "        \"B-BOOK\": 13,\n",
    "        \"I-BOOK\": 14,\n",
    "        \"E-BOOK\": 15,\n",
    "        \"S-BOOK\": 16,\n",
    "    }\n",
    "    \n",
    "    def __init__(self, path) -> None:\n",
    "        super().__init__()\n",
    "        self.data = []\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            d = ['', '']\n",
    "            while line := f.readline():\n",
    "                line = line.strip()\n",
    "                if line:\n",
    "                    word, label = line.split()\n",
    "                    d[0]+=word\n",
    "                    \n",
    "                    d[1]+=' ' + label+  ' '\n",
    "                elif d[0]:\n",
    "                    self.data.append(tuple(d))\n",
    "                    d = ['', '']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "    \n",
    "class SummaryDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, path):\n",
    "        super().__init__()\n",
    "        self.data = []\n",
    "        # 解析 JSON 数据\n",
    "        with open(path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                    # 解析每一行的 JSON 对象\n",
    "                    item = json.loads(line.strip())\n",
    "                    \n",
    "                    # 提取所需字段\n",
    "                    article = item['article']\n",
    "                    summary = item['summary']\n",
    "                    id = item['id']\n",
    "                    label = item['label']\n",
    "                    \n",
    "                    # 将解析的数据添加到 self.data 列表中\n",
    "                    self.data.append({\n",
    "                        'article': article,\n",
    "                        'summary': summary,\n",
    "                        'id': id,\n",
    "                        'label': label\n",
    "                    })\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 加载数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-11-16T15:24:33.523487Z",
     "iopub.status.busy": "2024-11-16T15:24:33.523146Z",
     "iopub.status.idle": "2024-11-16T15:24:33.627001Z",
     "shell.execute_reply": "2024-11-16T15:24:33.626514Z",
     "shell.execute_reply.started": "2024-11-16T15:24:33.523455Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.functional import one_hot\n",
    "\n",
    "## 定义get_dataloader创建Dataload,用于处理从数据集中获取的批次数据\n",
    "## batchsize（？）\n",
    "def get_CHisIECtest_dataloader(dataset, shuffle=True):\n",
    "    def collect_fn(batch):\n",
    "        t = batch[0][0]\n",
    "        l = batch[0][1]\n",
    "        return t, l\n",
    "\n",
    "    return DataLoader(\n",
    "        dataset,\n",
    "        shuffle=shuffle,\n",
    "        batch_size=1,\n",
    "        collate_fn=collect_fn,\n",
    "    )\n",
    "\n",
    "def get_Summary_dataloader(dataset, shuffle=True):\n",
    "    def collect_fn(batch):\n",
    "        t = batch[0]['article']\n",
    "        l = batch[0]['summary']\n",
    "        return t, l\n",
    "\n",
    "    return DataLoader(\n",
    "        dataset,\n",
    "        shuffle=shuffle,\n",
    "        batch_size=1,\n",
    "        collate_fn=collect_fn,\n",
    "    )\n",
    "   \n",
    "CHisIECtest_set = CHisIECDataset(\"ner.txt\")\n",
    "Summary_set = SummaryDataset(\"summary.jsonl\")\n",
    "\n",
    "CHisIECtest_loader = get_CHisIECtest_dataloader(CHisIECtest_set, shuffle=False)\n",
    "#输出元素\n",
    "# for i, (data, label) in enumerate(CHisIECtest_loader):\n",
    "#     print(data)\n",
    "#     print(label)\n",
    "    \n",
    "Summarytest_loader = get_Summary_dataloader(Summary_set,False)\n",
    "# for i, (data, label) in enumerate(Summarytest_loader):\n",
    "    # print(data)\n",
    "    # print(label)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型测试\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前设备: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import re\n",
    "\n",
    "\n",
    "# 创建一个设备对象\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"当前设备: {device}\")\n",
    "model_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "cache_directory = \"F:/model\"  # 指定你想要的缓存目录\n",
    "\n",
    "# 加载模型和分词器\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=device,\n",
    "    cache_dir=cache_directory  # 添加此行\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=cache_directory,device_map=device)  # 添加此行"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NER\n",
    "最终结果都很差 很难高于0.1\n",
    "#### Test 1\n",
    "* 采用不同形式的prompt，多次尝试\n",
    "  * zero-shot ： 只给任务提示\n",
    "    结果很差，乱生成\n",
    "  * few-shot ： 给出两个示例\n",
    "    结果变好，但是部分样本会受到示例的影响，结果与样本结果相似\n",
    "    但是最后的F1-SCORE 很低 因为大部分都在生成 O\n",
    "\n",
    "* 结论：黑盒很难去调整，最后效果很大取决于预训练模型的能力\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "all_true_labels = []\n",
    "all_pred_labels = []\n",
    "# 假设你已经定义了 test_loader\n",
    "prompt_task = \"我需要你去做实体标记任务 给以上序列的每个词打上标注,输出长度应该与句子长度相同  \"\n",
    "\n",
    "for i, (data, label) in enumerate(CHisIECtest_set):\n",
    "    print(i)\n",
    "    q =  data + prompt_task  # 将数据添加到 prompt 中\n",
    "    messages = [\n",
    "        # {\"role\": \"system\", \"content\":  },\n",
    "        {\"role\": \"user\", \"content\": '''以下我需要你做实体标记任务,为每个字打上以下标签,以下是标签的介绍:B-PER(个人名称实体的开始), I-PER(个人名称实体的内部), E-PER(个人名称实体的结束), S-PER(单独的个人名称), B-LOC(地点实体的开始), I-LOC(地点实体的内部), E-LOC(地点实体的结束), S-LOC(单独的地点), B-OFI(组织机构实体的开始), I-OFI(组织机构实体的内部), E-OFI(组织机构实体的结束), S-OFI(单独的组织机构), B-BOOK(书名实体的开始), I-BOOK(书名实体的内部), E-BOOK(书名实体的结束), S-BOOK(单独的书名) \n",
    "        训旣作相，以守澄为六军十二卫观军容使，罢其禁旅之权，寻赐酖杀之。训愈承恩顾，每别殿奏对，他宰相莫不顺成其言，黄门禁军迎拜戢敛。'''+prompt_task+'''\n",
    "    Answer : S-PER  O  O  S-OFI  O  O  B-PER  E-PER  O  B-OFI  I-OFI  I-OFI  I-OFI  I-OFI  I-OFI  I-OFI  I-OFI  E-OFI  O  O  O  O  O  O  O  O  O  O  O  O  O  O  S-PER  O  O  O  O  O  O  O  O  O  O  O  O  B-OFI  E-OFI  O  O  O  O  O  O  O  O  O  O  O  O  O  O  O  O \n",
    "    赜与洛阳元善、河东柳[巩言，凡改几]、太原王劭、吴兴姚察、琅邪诸葛颍、信都刘焯、河间刘炫相善，每因休假，清谈竟日。'''+prompt_task+'''\n",
    "    Answer :  S-PER  O  B-LOC  E-LOC  B-PER  E-PER  O  B-LOC  E-LOC  S-PER  O  O  O  O  O  O  O  O  O  B-LOC  E-LOC  B-PER  E-PER  O  B-LOC  E-LOC  B-PER  E-PER  O  B-LOC  E-LOC  B-PER  I-PER  E-PER  O  B-LOC  E-LOC  B-PER  E-PER  O  B-LOC  E-LOC  B-PER  E-PER  O  O  O  O  O  O  O  O  O  O  O  O  O \n",
    "    '''+q+'''\\nAnswer:'''}\n",
    "    ]\n",
    "\n",
    "    # 将消息格式化为模型输入\n",
    "    text = tokenizer(messages[0][\"content\"], return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # 生成输出\n",
    "    generated_ids = model.generate(\n",
    "        **text,\n",
    "        max_new_tokens=512\n",
    "    )\n",
    "\n",
    "    # 解码生成的文本\n",
    "    response = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    # 使用正则表达式找到所有 'Answer' 后的字符组\n",
    "    matches = re.findall(r'Answer:\\s*(.*?)(?=Answer:|$)', response, re.DOTALL)\n",
    "\n",
    "    # 提取最后一个匹配\n",
    "    answer_text = matches[-1].strip()    \n",
    "    pred_labels = answer_text.split('  ')  # 将模型输出分割成标签列表\n",
    "    \n",
    "    # 真实标签\n",
    "    true_labels = label.split('  ')  # 真实标签从字符串转换为列表\n",
    "    if len(pred_labels) > len(true_labels):\n",
    "        pred_labels = pred_labels[:len(true_labels)]\n",
    "    elif len(pred_labels) < len(true_labels):\n",
    "        \n",
    "        pred_labels += [\"None\"] * (len(true_labels) - len(pred_labels))\n",
    "    pred_labels = [label.strip() for label in pred_labels if label.strip()]\n",
    "    true_labels = [label.strip() for label in true_labels if label.strip()]\n",
    "    # 存储到全局列表\n",
    "    all_true_labels.extend(true_labels)\n",
    "    all_pred_labels.extend(pred_labels)\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.0239\n"
     ]
    }
   ],
   "source": [
    "## fewshot\n",
    "from sklearn.metrics import f1_score\n",
    "f1 = f1_score(all_true_labels, all_pred_labels, average='macro')  # 或者选择'micro' 或 'macro'作为 average 参数\n",
    "print(f\"F1 Score: {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test 2\n",
    "**使用API进行测试，基于更大模型**\n",
    "prompt: \n",
    "* 1:直接用句子做fewshot 输出格式正确 但是F1准确率低下，稳定在0.1以内\n",
    "  ```py\n",
    "  q ='''以下我需要你做实体标记任务,为每个字打上以下标签,以下是标签的介绍:B-PER(个人名称实体的开始), I-PER(个人名称实体的内部), E-PER(个人名称实体的结束), S-PER(单独的个人名称), B-LOC(地点实体的开始), I-LOC(地点实体的内部), E-LOC(地点实体的结束), S-LOC(单独的地点), B-OFI(组织机构实体的开始), I-OFI(组织机构实体的内部), E-OFI(组织机构实体的结束), S-OFI(单独的组织机构), B-BOOK(书名实体的开始), I-BOOK(书名实体的内部), E-BOOK(书名实体的结束), S-BOOK(单独的书名) \n",
    "    \n",
    "    \n",
    "    Sentence:训旣作相，以守澄为六军十二卫观军容使，罢其禁旅之权，寻赐酖杀之。训愈承恩顾，每别殿奏对，他宰相莫不顺成其言，黄门禁军迎拜戢敛。'''+'''\n",
    "    Answer : S-PER  O  O  S-OFI  O  O  B-PER  E-PER  O  B-OFI  I-OFI  I-OFI  I-OFI  I-OFI  I-OFI  I-OFI  I-OFI  E-OFI  O  O  O  O  O  O  O  O  O  O  O  O  O  O  S-PER  O  O  O  O  O  O  O  O  O  O  O  O  B-OFI  E-OFI  O  O  O  O  O  O  O  O  O  O  O  O  O  O  O  O \n",
    "    '''+q+'''\\nAnswer :'''\n",
    "  ```\n",
    "* 2：尝试对每个标签做出用例解释，之后在做用句子做fewshot \n",
    "  * 效果更差了，可能由于样例与文本不够契合\n",
    "```py\n",
    "'''以下我需要你做实体标记任务,为每个字打上以下标签,以下是标签的介绍:\n",
    "    B-PER：表示个人名称实体的开始。例如，“李”在“李华”中是名字的开始。\n",
    "    I-PER：表示个人名称实体的内部部分。例如，“华”在“李华”中是名字的中间部分。\n",
    "    E-PER：表示个人名称实体的结束。例如，在“王小明”中，“明”就是这个个人名称实体的结尾。\n",
    "    S-PER：表示单独的个人名称。例如，“李”如果作为一个独立的名字出现。\n",
    "    B-LOC：表示地点实体的开始。例如，“北”在“北京”中是地点名称的开始。\n",
    "    I-LOC：表示地点实体的内部部分。例如，“京”在“北京”中是地点名称的内部。\n",
    "    E-LOC：表示地点实体的结束。例如，在“上海市”中，“市”就是这个地点名称的结尾。\n",
    "    S-LOC：表示单独的地点。例如，“京”如果作为单独的地点出现。\n",
    "    B-ORG：表示组织机构实体的开始。例如，“中”在“中国科学院”中是机构名称的开始。\n",
    "    I-ORG：表示组织机构实体的内部部分。例如，“国”在“中国科学院”中是机构名称的内部。\n",
    "    E-ORG：表示组织机构实体的结束。例如，在“中国科学院”中，“院”就是这个机构名称的结尾。\n",
    "    S-ORG：表示单独的组织机构。例如，“中科院”如果作为单独的简称出现。\n",
    "    B-BOOK：表示书名实体的开始。例如，“三”在“三国演义”中是书名的开始。\n",
    "    I-BOOK：表示书名实体的内部部分。例如，“国”在“三国演义”中是书名的内部。\n",
    "    E-BOOK：表示书名实体的结束。例如，在“三国演义”中，“义”就是这个书名的结尾。\n",
    "    S-BOOK：表示单独的书名。例如，“经”如果作为一个独立的书名出现\n",
    "    \n",
    "    Sentence:训旣作相，以守澄为六军十二卫观军容使，罢其禁旅之权，寻赐酖杀之。训愈承恩顾，每别殿奏对，他宰相莫不顺成其言，黄门禁军迎拜戢敛。'''+'''\n",
    "    Answer : S-PER  O  O  S-OFI  O  O  B-PER  E-PER  O  B-OFI  I-OFI  I-OFI  I-OFI  I-OFI  I-OFI  I-OFI  I-OFI  E-OFI  O  O  O  O  O  O  O  O  O  O  O  O  O  O  S-PER  O  O  O  O  O  O  O  O  O  O  O  O  B-OFI  E-OFI  O  O  O  O  O  O  O  O  O  O  O  O  O  O  O  O \n",
    "    '''+q+'''\\nAnswer :'''\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "93\n",
      "93\n",
      "F1 Score: 0.0452\n",
      "1\n",
      "144\n",
      "144\n",
      "F1 Score: 0.0386\n",
      "2\n",
      "210\n",
      "210\n",
      "F1 Score: 0.0361\n",
      "3\n",
      "266\n",
      "266\n",
      "F1 Score: 0.0352\n",
      "4\n",
      "307\n",
      "307\n",
      "F1 Score: 0.0349\n",
      "5\n",
      "317\n",
      "317\n",
      "F1 Score: 0.0478\n",
      "6\n",
      "384\n",
      "384\n",
      "F1 Score: 0.0498\n",
      "7\n",
      "459\n",
      "459\n",
      "F1 Score: 0.0433\n",
      "8\n",
      "519\n",
      "519\n",
      "F1 Score: 0.0412\n",
      "9\n",
      "534\n",
      "534\n",
      "F1 Score: 0.0384\n",
      "10\n",
      "628\n",
      "628\n",
      "F1 Score: 0.0407\n",
      "11\n",
      "668\n",
      "668\n",
      "F1 Score: 0.0416\n",
      "12\n",
      "756\n",
      "756\n",
      "F1 Score: 0.0450\n",
      "13\n",
      "774\n",
      "774\n",
      "F1 Score: 0.0512\n",
      "14\n",
      "800\n",
      "800\n",
      "F1 Score: 0.0501\n",
      "15\n",
      "865\n",
      "865\n",
      "F1 Score: 0.0488\n",
      "16\n",
      "954\n",
      "954\n",
      "F1 Score: 0.0514\n",
      "17\n",
      "992\n",
      "992\n",
      "F1 Score: 0.0494\n",
      "18\n",
      "1064\n",
      "1064\n",
      "F1 Score: 0.0459\n",
      "19\n",
      "1117\n",
      "1117\n",
      "F1 Score: 0.0446\n",
      "20\n",
      "1188\n",
      "1188\n",
      "F1 Score: 0.0441\n",
      "21\n",
      "1262\n",
      "1262\n",
      "F1 Score: 0.0432\n",
      "22\n",
      "1295\n",
      "1295\n",
      "F1 Score: 0.0427\n",
      "23\n",
      "1325\n",
      "1325\n",
      "F1 Score: 0.0423\n",
      "24\n",
      "1408\n",
      "1408\n",
      "F1 Score: 0.0415\n",
      "25\n",
      "1425\n",
      "1425\n",
      "F1 Score: 0.0471\n",
      "26\n",
      "1495\n",
      "1495\n",
      "F1 Score: 0.0458\n",
      "27\n",
      "1536\n",
      "1536\n",
      "F1 Score: 0.0450\n",
      "28\n",
      "1609\n",
      "1609\n",
      "F1 Score: 0.0454\n",
      "29\n",
      "1645\n",
      "1645\n",
      "F1 Score: 0.0464\n",
      "30\n",
      "1682\n",
      "1682\n",
      "F1 Score: 0.0454\n",
      "31\n",
      "1771\n",
      "1771\n",
      "F1 Score: 0.0453\n",
      "32\n",
      "1847\n",
      "1847\n",
      "F1 Score: 0.0465\n",
      "33\n",
      "1924\n",
      "1924\n",
      "F1 Score: 0.0449\n",
      "34\n",
      "1994\n",
      "1994\n",
      "F1 Score: 0.0445\n",
      "35\n",
      "2079\n",
      "2079\n",
      "F1 Score: 0.0442\n",
      "36\n",
      "2160\n",
      "2160\n",
      "F1 Score: 0.0439\n",
      "37\n",
      "2246\n",
      "2246\n",
      "F1 Score: 0.0386\n",
      "38\n",
      "2323\n",
      "2323\n",
      "F1 Score: 0.0384\n",
      "39\n",
      "2406\n",
      "2406\n",
      "F1 Score: 0.0382\n",
      "40\n",
      "2470\n",
      "2470\n",
      "F1 Score: 0.0394\n",
      "41\n",
      "2551\n",
      "2551\n",
      "F1 Score: 0.0387\n",
      "42\n",
      "2630\n",
      "2630\n",
      "F1 Score: 0.0382\n",
      "43\n",
      "2674\n",
      "2674\n",
      "F1 Score: 0.0394\n",
      "44\n",
      "2720\n",
      "2720\n",
      "F1 Score: 0.0393\n",
      "45\n",
      "2784\n",
      "2784\n",
      "F1 Score: 0.0390\n",
      "46\n",
      "2827\n",
      "2827\n",
      "F1 Score: 0.0392\n",
      "47\n",
      "2869\n",
      "2869\n",
      "F1 Score: 0.0387\n",
      "48\n",
      "2925\n",
      "2925\n",
      "F1 Score: 0.0396\n",
      "49\n",
      "2966\n",
      "2966\n",
      "F1 Score: 0.0397\n",
      "50\n",
      "3071\n",
      "3071\n",
      "F1 Score: 0.0378\n",
      "51\n",
      "3096\n",
      "3096\n",
      "F1 Score: 0.0377\n",
      "52\n",
      "3154\n",
      "3154\n",
      "F1 Score: 0.0371\n",
      "53\n",
      "3232\n",
      "3232\n",
      "F1 Score: 0.0366\n",
      "54\n",
      "3259\n",
      "3259\n",
      "F1 Score: 0.0352\n",
      "55\n",
      "3324\n",
      "3324\n",
      "F1 Score: 0.0349\n",
      "56\n",
      "3372\n",
      "3372\n",
      "F1 Score: 0.0311\n",
      "57\n",
      "3419\n",
      "3419\n",
      "F1 Score: 0.0311\n",
      "58\n",
      "3515\n",
      "3515\n",
      "F1 Score: 0.0313\n",
      "59\n",
      "3601\n",
      "3601\n",
      "F1 Score: 0.0312\n",
      "60\n",
      "3701\n",
      "3701\n",
      "F1 Score: 0.0315\n",
      "61\n",
      "3738\n",
      "3738\n",
      "F1 Score: 0.0313\n",
      "62\n",
      "3807\n",
      "3807\n",
      "F1 Score: 0.0315\n",
      "63\n",
      "3851\n",
      "3851\n",
      "F1 Score: 0.0312\n",
      "64\n",
      "3945\n",
      "3945\n",
      "F1 Score: 0.0315\n",
      "65\n",
      "3974\n",
      "3974\n",
      "F1 Score: 0.0314\n",
      "66\n",
      "4040\n",
      "4040\n",
      "F1 Score: 0.0312\n",
      "67\n",
      "4092\n",
      "4092\n",
      "F1 Score: 0.0311\n",
      "68\n",
      "4128\n",
      "4128\n",
      "F1 Score: 0.0310\n",
      "69\n",
      "4210\n",
      "4210\n",
      "F1 Score: 0.0309\n",
      "70\n",
      "4282\n",
      "4282\n",
      "F1 Score: 0.0308\n",
      "71\n",
      "4330\n",
      "4330\n",
      "F1 Score: 0.0307\n",
      "72\n",
      "4372\n",
      "4372\n",
      "F1 Score: 0.0306\n",
      "73\n",
      "4383\n",
      "4383\n",
      "F1 Score: 0.0306\n",
      "74\n",
      "4426\n",
      "4426\n",
      "F1 Score: 0.0305\n",
      "75\n",
      "4516\n",
      "4516\n",
      "F1 Score: 0.0310\n",
      "76\n",
      "4582\n",
      "4582\n",
      "F1 Score: 0.0309\n",
      "77\n",
      "4661\n",
      "4661\n",
      "F1 Score: 0.0297\n",
      "78\n",
      "4698\n",
      "4698\n",
      "F1 Score: 0.0296\n",
      "79\n",
      "4716\n",
      "4716\n",
      "F1 Score: 0.0296\n",
      "80\n",
      "4837\n",
      "4837\n",
      "F1 Score: 0.0302\n",
      "81\n",
      "4887\n",
      "4887\n",
      "F1 Score: 0.0302\n",
      "82\n",
      "4938\n",
      "4938\n",
      "F1 Score: 0.0300\n",
      "83\n",
      "4975\n",
      "4975\n",
      "F1 Score: 0.0299\n",
      "84\n",
      "5034\n",
      "5034\n",
      "F1 Score: 0.0299\n",
      "85\n",
      "5156\n",
      "5156\n",
      "F1 Score: 0.0299\n",
      "86\n",
      "5239\n",
      "5239\n",
      "F1 Score: 0.0298\n",
      "87\n",
      "5270\n",
      "5270\n",
      "F1 Score: 0.0298\n",
      "88\n",
      "5354\n",
      "5354\n",
      "F1 Score: 0.0296\n",
      "89\n",
      "5398\n",
      "5398\n",
      "F1 Score: 0.0298\n",
      "90\n",
      "5466\n",
      "5466\n",
      "F1 Score: 0.0300\n",
      "91\n",
      "5551\n",
      "5551\n",
      "F1 Score: 0.0300\n",
      "92\n",
      "5620\n",
      "5620\n",
      "F1 Score: 0.0299\n",
      "93\n",
      "5685\n",
      "5685\n",
      "F1 Score: 0.0298\n",
      "94\n",
      "5695\n",
      "5695\n",
      "F1 Score: 0.0298\n",
      "95\n",
      "5728\n",
      "5728\n",
      "F1 Score: 0.0298\n",
      "96\n",
      "5796\n",
      "5796\n",
      "F1 Score: 0.0296\n",
      "97\n",
      "5886\n",
      "5886\n",
      "F1 Score: 0.0295\n",
      "98\n",
      "6001\n",
      "6001\n",
      "F1 Score: 0.0295\n",
      "99\n",
      "6088\n",
      "6088\n",
      "F1 Score: 0.0293\n",
      "100\n",
      "6184\n",
      "6184\n",
      "F1 Score: 0.0293\n",
      "101\n",
      "6234\n",
      "6234\n",
      "F1 Score: 0.0300\n",
      "102\n",
      "6285\n",
      "6285\n",
      "F1 Score: 0.0298\n",
      "103\n",
      "6293\n",
      "6293\n",
      "F1 Score: 0.0298\n",
      "104\n",
      "6377\n",
      "6377\n",
      "F1 Score: 0.0304\n",
      "105\n",
      "6418\n",
      "6418\n",
      "F1 Score: 0.0305\n",
      "106\n",
      "6475\n",
      "6475\n",
      "F1 Score: 0.0308\n",
      "107\n",
      "6592\n",
      "6592\n",
      "F1 Score: 0.0308\n",
      "108\n",
      "6627\n",
      "6627\n",
      "F1 Score: 0.0307\n",
      "109\n",
      "6726\n",
      "6726\n",
      "F1 Score: 0.0309\n",
      "110\n",
      "6736\n",
      "6736\n",
      "F1 Score: 0.0309\n",
      "111\n",
      "6792\n",
      "6792\n",
      "F1 Score: 0.0308\n",
      "112\n",
      "6870\n",
      "6870\n",
      "F1 Score: 0.0308\n",
      "113\n",
      "6940\n",
      "6940\n",
      "F1 Score: 0.0310\n",
      "114\n",
      "6981\n",
      "6981\n",
      "F1 Score: 0.0309\n",
      "115\n",
      "6985\n",
      "6985\n",
      "F1 Score: 0.0442\n",
      "116\n",
      "7060\n",
      "7060\n",
      "F1 Score: 0.0443\n",
      "117\n",
      "7145\n",
      "7145\n",
      "F1 Score: 0.0442\n",
      "118\n",
      "7187\n",
      "7187\n",
      "F1 Score: 0.0445\n",
      "119\n",
      "7247\n",
      "7247\n",
      "F1 Score: 0.0444\n",
      "120\n",
      "7320\n",
      "7320\n",
      "F1 Score: 0.0442\n",
      "121\n",
      "7347\n",
      "7347\n",
      "F1 Score: 0.0443\n",
      "122\n",
      "7436\n",
      "7436\n",
      "F1 Score: 0.0442\n",
      "123\n",
      "7506\n",
      "7506\n",
      "F1 Score: 0.0442\n",
      "124\n",
      "7594\n",
      "7594\n",
      "F1 Score: 0.0445\n",
      "125\n",
      "7682\n",
      "7682\n",
      "F1 Score: 0.0444\n",
      "126\n",
      "7748\n",
      "7748\n",
      "F1 Score: 0.0444\n",
      "127\n",
      "7868\n",
      "7868\n",
      "F1 Score: 0.0444\n",
      "128\n",
      "7928\n",
      "7928\n",
      "F1 Score: 0.0442\n",
      "129\n",
      "7970\n",
      "7970\n",
      "F1 Score: 0.0453\n",
      "130\n",
      "8028\n",
      "8028\n",
      "F1 Score: 0.0456\n",
      "131\n",
      "8101\n",
      "8101\n",
      "F1 Score: 0.0459\n",
      "132\n",
      "8164\n",
      "8164\n",
      "F1 Score: 0.0456\n",
      "133\n",
      "8222\n",
      "8222\n",
      "F1 Score: 0.0456\n",
      "134\n",
      "8240\n",
      "8240\n",
      "F1 Score: 0.0455\n",
      "135\n",
      "8249\n",
      "8249\n",
      "F1 Score: 0.0454\n",
      "136\n",
      "8351\n",
      "8351\n",
      "F1 Score: 0.0439\n",
      "137\n",
      "8398\n",
      "8398\n",
      "F1 Score: 0.0445\n",
      "138\n",
      "8500\n",
      "8500\n",
      "F1 Score: 0.0445\n",
      "139\n",
      "8554\n",
      "8554\n",
      "F1 Score: 0.0395\n",
      "140\n",
      "8576\n",
      "8576\n",
      "F1 Score: 0.0394\n",
      "141\n",
      "8637\n",
      "8637\n",
      "F1 Score: 0.0393\n",
      "142\n",
      "8707\n",
      "8707\n",
      "F1 Score: 0.0393\n",
      "143\n",
      "8785\n",
      "8785\n",
      "F1 Score: 0.0394\n",
      "144\n",
      "8845\n",
      "8845\n",
      "F1 Score: 0.0393\n",
      "145\n",
      "8905\n",
      "8905\n",
      "F1 Score: 0.0394\n",
      "146\n",
      "8977\n",
      "8977\n",
      "F1 Score: 0.0393\n",
      "147\n",
      "9014\n",
      "9014\n",
      "F1 Score: 0.0392\n",
      "148\n",
      "9093\n",
      "9093\n",
      "F1 Score: 0.0393\n",
      "149\n",
      "9123\n",
      "9123\n",
      "F1 Score: 0.0393\n",
      "150\n",
      "9178\n",
      "9178\n",
      "F1 Score: 0.0392\n",
      "151\n",
      "9242\n",
      "9242\n",
      "F1 Score: 0.0391\n",
      "152\n",
      "9347\n",
      "9347\n",
      "F1 Score: 0.0392\n",
      "153\n",
      "9430\n",
      "9430\n",
      "F1 Score: 0.0393\n",
      "154\n",
      "9455\n",
      "9455\n",
      "F1 Score: 0.0393\n",
      "155\n",
      "9509\n",
      "9509\n",
      "F1 Score: 0.0392\n",
      "156\n",
      "9529\n",
      "9529\n",
      "F1 Score: 0.0381\n",
      "157\n",
      "9562\n",
      "9562\n",
      "F1 Score: 0.0381\n",
      "158\n",
      "9626\n",
      "9626\n",
      "F1 Score: 0.0380\n",
      "159\n",
      "9704\n",
      "9704\n",
      "F1 Score: 0.0379\n",
      "160\n",
      "9788\n",
      "9788\n",
      "F1 Score: 0.0378\n",
      "161\n",
      "9848\n",
      "9848\n",
      "F1 Score: 0.0378\n",
      "162\n",
      "9899\n",
      "9899\n",
      "F1 Score: 0.0380\n",
      "163\n",
      "9984\n",
      "9984\n",
      "F1 Score: 0.0382\n",
      "164\n",
      "10057\n",
      "10057\n",
      "F1 Score: 0.0381\n",
      "165\n",
      "10088\n",
      "10088\n",
      "F1 Score: 0.0381\n",
      "166\n",
      "10158\n",
      "10158\n",
      "F1 Score: 0.0381\n",
      "167\n",
      "10220\n",
      "10220\n",
      "F1 Score: 0.0382\n",
      "168\n",
      "10295\n",
      "10295\n",
      "F1 Score: 0.0382\n",
      "169\n",
      "10364\n",
      "10364\n",
      "F1 Score: 0.0380\n",
      "170\n",
      "10410\n",
      "10410\n",
      "F1 Score: 0.0380\n",
      "171\n",
      "10452\n",
      "10452\n",
      "F1 Score: 0.0380\n",
      "172\n",
      "10514\n",
      "10514\n",
      "F1 Score: 0.0381\n",
      "173\n",
      "10614\n",
      "10614\n",
      "F1 Score: 0.0380\n",
      "174\n",
      "10718\n",
      "10718\n",
      "F1 Score: 0.0380\n",
      "175\n",
      "10733\n",
      "10733\n",
      "F1 Score: 0.0380\n",
      "176\n",
      "10783\n",
      "10783\n",
      "F1 Score: 0.0379\n",
      "177\n",
      "10836\n",
      "10836\n",
      "F1 Score: 0.0378\n",
      "178\n",
      "10893\n",
      "10893\n",
      "F1 Score: 0.0378\n",
      "179\n",
      "10966\n",
      "10966\n",
      "F1 Score: 0.0380\n",
      "180\n",
      "11043\n",
      "11043\n",
      "F1 Score: 0.0379\n",
      "181\n",
      "11119\n",
      "11119\n",
      "F1 Score: 0.0379\n",
      "182\n",
      "11137\n",
      "11137\n",
      "F1 Score: 0.0379\n",
      "183\n",
      "11202\n",
      "11202\n",
      "F1 Score: 0.0384\n",
      "184\n",
      "11283\n",
      "11283\n",
      "F1 Score: 0.0358\n",
      "185\n",
      "11340\n",
      "11340\n",
      "F1 Score: 0.0358\n",
      "186\n",
      "11386\n",
      "11386\n",
      "F1 Score: 0.0357\n",
      "187\n",
      "11457\n",
      "11457\n",
      "F1 Score: 0.0356\n",
      "188\n",
      "11469\n",
      "11469\n",
      "F1 Score: 0.0351\n",
      "189\n",
      "11487\n",
      "11487\n",
      "F1 Score: 0.0351\n",
      "190\n",
      "11541\n",
      "11541\n",
      "F1 Score: 0.0350\n",
      "191\n",
      "11620\n",
      "11620\n",
      "F1 Score: 0.0349\n",
      "192\n",
      "11634\n",
      "11634\n",
      "F1 Score: 0.0349\n",
      "193\n",
      "11677\n",
      "11677\n",
      "F1 Score: 0.0350\n",
      "194\n",
      "11723\n",
      "11723\n",
      "F1 Score: 0.0350\n",
      "195\n",
      "11747\n",
      "11747\n",
      "F1 Score: 0.0350\n",
      "196\n",
      "11816\n",
      "11816\n",
      "F1 Score: 0.0349\n",
      "197\n",
      "11892\n",
      "11892\n",
      "F1 Score: 0.0349\n",
      "198\n",
      "11952\n",
      "11952\n",
      "F1 Score: 0.0348\n",
      "199\n",
      "12000\n",
      "12000\n",
      "F1 Score: 0.0352\n",
      "200\n",
      "12091\n",
      "12091\n",
      "F1 Score: 0.0351\n",
      "201\n",
      "12175\n",
      "12175\n",
      "F1 Score: 0.0350\n",
      "202\n",
      "12261\n",
      "12261\n",
      "F1 Score: 0.0349\n",
      "203\n",
      "12298\n",
      "12298\n",
      "F1 Score: 0.0349\n",
      "204\n",
      "12406\n",
      "12406\n",
      "F1 Score: 0.0348\n",
      "205\n",
      "12497\n",
      "12497\n",
      "F1 Score: 0.0348\n",
      "206\n",
      "12544\n",
      "12544\n",
      "F1 Score: 0.0347\n",
      "207\n",
      "12642\n",
      "12642\n",
      "F1 Score: 0.0349\n",
      "208\n",
      "12655\n",
      "12655\n",
      "F1 Score: 0.0349\n",
      "209\n",
      "12726\n",
      "12726\n",
      "F1 Score: 0.0349\n",
      "210\n",
      "12795\n",
      "12795\n",
      "F1 Score: 0.0349\n",
      "211\n",
      "12838\n",
      "12838\n",
      "F1 Score: 0.0349\n",
      "212\n",
      "12872\n",
      "12872\n",
      "F1 Score: 0.0349\n",
      "213\n",
      "12884\n",
      "12884\n",
      "F1 Score: 0.0344\n",
      "214\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'choices'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 59\u001b[0m\n\u001b[0;32m     57\u001b[0m response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mrequest(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPOST\u001b[39m\u001b[38;5;124m\"\u001b[39m, url, headers\u001b[38;5;241m=\u001b[39mheaders, data\u001b[38;5;241m=\u001b[39mpayload)\n\u001b[0;32m     58\u001b[0m response_json \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(response\u001b[38;5;241m.\u001b[39mtext)\n\u001b[1;32m---> 59\u001b[0m reply \u001b[38;5;241m=\u001b[39m \u001b[43mresponse_json\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mchoices\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m]  \u001b[38;5;66;03m# 这里是返回结果\u001b[39;00m\n\u001b[0;32m     63\u001b[0m pred_labels \u001b[38;5;241m=\u001b[39m reply\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# 将模型输出分割成标签列表\u001b[39;00m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;66;03m# 真实标签\u001b[39;00m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'choices'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "all_true_labels = []\n",
    "all_pred_labels = []\n",
    "# 假设你已经定义了 test_loader\n",
    "prompt_task = \"Sentence: \"\n",
    "url = \"https://www.dmxapi.com/v1/chat/completions\"\n",
    "\n",
    "for i, (data, label) in enumerate(CHisIECtest_set):\n",
    "    print(i)\n",
    "    q =  data + prompt_task  # 将数据添加到 prompt 中\n",
    "    q ='''以下我需要你做实体标记任务,为每个字打上以下标签,以下是标签的介绍:B-PER(个人名称实体的开始), I-PER(个人名称实体的内部), E-PER(个人名称实体的结束), S-PER(单独的个人名称), B-LOC(地点实体的开始), I-LOC(地点实体的内部), E-LOC(地点实体的结束), S-LOC(单独的地点), B-OFI(组织机构实体的开始), I-OFI(组织机构实体的内部), E-OFI(组织机构实体的结束), S-OFI(单独的组织机构), B-BOOK(书名实体的开始), I-BOOK(书名实体的内部), E-BOOK(书名实体的结束), S-BOOK(单独的书名) \n",
    "    \n",
    "    \n",
    "    Sentence:训旣作相，以守澄为六军十二卫观军容使，罢其禁旅之权，寻赐酖杀之。训愈承恩顾，每别殿奏对，他宰相莫不顺成其言，黄门禁军迎拜戢敛。'''+'''\n",
    "    Answer : S-PER  O  O  S-OFI  O  O  B-PER  E-PER  O  B-OFI  I-OFI  I-OFI  I-OFI  I-OFI  I-OFI  I-OFI  I-OFI  E-OFI  O  O  O  O  O  O  O  O  O  O  O  O  O  O  S-PER  O  O  O  O  O  O  O  O  O  O  O  O  B-OFI  E-OFI  O  O  O  O  O  O  O  O  O  O  O  O  O  O  O  O \n",
    "    '''+q+'''\\nAnswer :'''\n",
    "    # q ='''以下我需要你做实体标记任务,为每个字打上以下标签,以下是标签的介绍:\n",
    "    # B-PER：表示个人名称实体的开始。例如，“李”在“李华”中是名字的开始。\n",
    "    # I-PER：表示个人名称实体的内部部分。例如，“华”在“李华”中是名字的中间部分。\n",
    "    # E-PER：表示个人名称实体的结束。例如，在“王小明”中，“明”就是这个个人名称实体的结尾。\n",
    "    # S-PER：表示单独的个人名称。例如，“李”如果作为一个独立的名字出现。\n",
    "    # B-LOC：表示地点实体的开始。例如，“北”在“北京”中是地点名称的开始。\n",
    "    # I-LOC：表示地点实体的内部部分。例如，“京”在“北京”中是地点名称的内部。\n",
    "    # E-LOC：表示地点实体的结束。例如，在“上海市”中，“市”就是这个地点名称的结尾。\n",
    "    # S-LOC：表示单独的地点。例如，“京”如果作为单独的地点出现。\n",
    "    # B-ORG：表示组织机构实体的开始。例如，“中”在“中国科学院”中是机构名称的开始。\n",
    "    # I-ORG：表示组织机构实体的内部部分。例如，“国”在“中国科学院”中是机构名称的内部。\n",
    "    # E-ORG：表示组织机构实体的结束。例如，在“中国科学院”中，“院”就是这个机构名称的结尾。\n",
    "    # S-ORG：表示单独的组织机构。例如，“中科院”如果作为单独的简称出现。\n",
    "    # B-BOOK：表示书名实体的开始。例如，“三”在“三国演义”中是书名的开始。\n",
    "    # I-BOOK：表示书名实体的内部部分。例如，“国”在“三国演义”中是书名的内部。\n",
    "    # E-BOOK：表示书名实体的结束。例如，在“三国演义”中，“义”就是这个书名的结尾。\n",
    "    # S-BOOK：表示单独的书名。例如，“经”如果作为一个独立的书名出现\n",
    "    \n",
    "    # Sentence:训旣作相，以守澄为六军十二卫观军容使，罢其禁旅之权，寻赐酖杀之。训愈承恩顾，每别殿奏对，他宰相莫不顺成其言，黄门禁军迎拜戢敛。'''+'''\n",
    "    # Answer : S-PER  O  O  S-OFI  O  O  B-PER  E-PER  O  B-OFI  I-OFI  I-OFI  I-OFI  I-OFI  I-OFI  I-OFI  I-OFI  E-OFI  O  O  O  O  O  O  O  O  O  O  O  O  O  O  S-PER  O  O  O  O  O  O  O  O  O  O  O  O  B-OFI  E-OFI  O  O  O  O  O  O  O  O  O  O  O  O  O  O  O  O \n",
    "    # '''+q+'''\\nAnswer :'''\n",
    "    payload = json.dumps({\n",
    "                \"model\": \"gpt-3.5-turbo\",  # 这里是你需要访问的模型，改成上面你需要测试的模型名称就可以了。\n",
    "                \"messages\": [\n",
    "                    {\n",
    "                        \"role\": \"system\",\n",
    "                        \"content\": \"你是命名实体识别任务的模型，只需要输出标签，以空格分隔，标点符号也作为识别的对象\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": q\n",
    "                    }\n",
    "                ]\n",
    "            })\n",
    "    headers = {\n",
    "                'Accept': 'application/json',\n",
    "                'Authorization': 'sk-kxRQ7ArIeazjscgawqVHPMCE2MJqYpM7LdT2RcMYlE7sq5i9',  # 这里放你的 DMXapi key\n",
    "                'User-Agent': 'DMXAPI/1.0.0 (https://www.dmxapi.com)',  # 这里也改成 DMXAPI 的中转URL https://www.dmxapi.com，已经改好\n",
    "                'Content-Type': 'application/json'\n",
    "            }\n",
    "    response = requests.request(\"POST\", url, headers=headers, data=payload)\n",
    "    response_json = json.loads(response.text)\n",
    "    reply = response_json['choices'][0]['message']['content']  # 这里是返回结果\n",
    "   \n",
    "    \n",
    "    \n",
    "    pred_labels = reply.split(' ')  # 将模型输出分割成标签列表\n",
    "    \n",
    "    # 真实标签\n",
    "    true_labels = label.split('  ')  # 真实标签从字符串转换为列表\n",
    "    if len(pred_labels) > len(true_labels):\n",
    "        pred_labels = pred_labels[:len(true_labels)]\n",
    "    elif len(pred_labels) < len(true_labels):\n",
    "        pred_labels += [\"None\"] * (len(true_labels) - len(pred_labels))\n",
    "    # 存储到全局列表\n",
    "    all_true_labels.extend(true_labels)\n",
    "    all_pred_labels.extend(pred_labels)\n",
    "    print(len(all_true_labels))\n",
    "    print(len(all_pred_labels))\n",
    "    # if(i==100):\n",
    "    #     break\n",
    "    from sklearn.metrics import f1_score\n",
    "    f1 = f1_score(all_true_labels, all_pred_labels, average='macro')  # 或者选择'micro' 或 'macro'作为 average 参数\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12884\n",
      "12884\n",
      "F1 Score: 0.0344\n"
     ]
    }
   ],
   "source": [
    "## fewshot gpt\n",
    "from sklearn.metrics import f1_score\n",
    "print(len(all_true_labels))\n",
    "print(len(all_pred_labels))\n",
    "f1 = f1_score(all_true_labels, all_pred_labels, average='macro')  # 或者选择'micro' 或 'macro'作为 average 参数\n",
    "print(f\"F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "#### Test1：qwen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**测试过程中总会出现cuda out of memory的错误**\n",
    "\n",
    "**测试输出结果具有随机性，容易匹配不到内容，这和模型能力有关，GPT3.5就输出格式很规范**\n",
    "* 采用不同prompt测试\n",
    "  * zero-shot\n",
    "    会输出概括，但是相对较长\n",
    "  * few-shot\n",
    "    输出长度更短，指标更高\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-11-16T15:25:56.110800Z",
     "iopub.status.busy": "2024-11-16T15:25:56.110298Z",
     "iopub.status.idle": "2024-11-16T15:53:30.049031Z",
     "shell.execute_reply": "2024-11-16T15:53:30.048500Z",
     "shell.execute_reply.started": "2024-11-16T15:25:56.110780Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Model to directory: /mnt/workspace/.cache/modelscope/hub/Qwen/Qwen2.5-0.5B-Instruct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:modelscope] Creating symbolic link /mnt/workspace/.cache/modelscope/hub/Qwen/Qwen2___5-0___5B-Instruct -> /mnt/workspace/.cache/modelscope/hub/Qwen/Qwen2.5-0.5B-Instruct.\n",
      "[WARNING:modelscope] Failed to create symbolic link /mnt/workspace/.cache/modelscope/hub/Qwen/Qwen2___5-0___5B-Instruct -> /mnt/workspace/.cache/modelscope/hub/Qwen/Qwen2.5-0.5B-Instruct: [Errno 17] File exists: '/mnt/workspace/.cache/modelscope/hub/Qwen/Qwen2___5-0___5B-Instruct' -> '/mnt/workspace/.cache/modelscope/hub/Qwen/Qwen2.5-0.5B-Instruct'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Model to directory: /mnt/workspace/.cache/modelscope/hub/Qwen/Qwen2.5-0.5B-Instruct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:modelscope] Creating symbolic link /mnt/workspace/.cache/modelscope/hub/Qwen/Qwen2___5-0___5B-Instruct -> /mnt/workspace/.cache/modelscope/hub/Qwen/Qwen2.5-0.5B-Instruct.\n",
      "[WARNING:modelscope] Failed to create symbolic link /mnt/workspace/.cache/modelscope/hub/Qwen/Qwen2___5-0___5B-Instruct -> /mnt/workspace/.cache/modelscope/hub/Qwen/Qwen2.5-0.5B-Instruct: [Errno 17] File exists: '/mnt/workspace/.cache/modelscope/hub/Qwen/Qwen2___5-0___5B-Instruct' -> '/mnt/workspace/.cache/modelscope/hub/Qwen/Qwen2.5-0.5B-Instruct'\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1 个数据未匹配到足够的内容，重新提问...\n",
      "第 2 个数据未匹配到足够的内容，重新提问...\n",
      "第 3 个数据未匹配到足够的内容，重新提问...\n",
      "第 6 个数据未匹配到足够的内容，重新提问...\n",
      "第 7 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 10 个数据未匹配到足够的内容，重新提问...\n",
      "第 12 个数据未匹配到足够的内容，重新提问...\n",
      "第 13 个数据未匹配到足够的内容，重新提问...\n",
      "第 15 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 18 个数据未匹配到足够的内容，重新提问...\n",
      "第 22 个数据未匹配到足够的内容，重新提问...\n",
      "第 23 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 24 个数据未匹配到足够的内容，重新提问...\n",
      "第 25 个数据未匹配到足够的内容，重新提问...\n",
      "第 27 个数据未匹配到足够的内容，重新提问...\n",
      "第 29 个数据未匹配到足够的内容，重新提问...\n",
      "第 30 个数据未匹配到足够的内容，重新提问...\n",
      "第 31 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 32 个数据未匹配到足够的内容，重新提问...\n",
      "第 34 个数据未匹配到足够的内容，重新提问...\n",
      "第 35 个数据未匹配到足够的内容，重新提问...\n",
      "第 36 个数据未匹配到足够的内容，重新提问...\n",
      "第 37 个数据未匹配到足够的内容，重新提问...\n",
      "第 38 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 6...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 40 个数据未匹配到足够的内容，重新提问...\n",
      "第 42 个数据未匹配到足够的内容，重新提问...\n",
      "第 43 个数据未匹配到足够的内容，重新提问...\n",
      "第 44 个数据未匹配到足够的内容，重新提问...\n",
      "第 46 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 7...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 49 个数据未匹配到足够的内容，重新提问...\n",
      "第 50 个数据未匹配到足够的内容，重新提问...\n",
      "第 53 个数据未匹配到足够的内容，重新提问...\n",
      "第 54 个数据未匹配到足够的内容，重新提问...\n",
      "第 55 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 8...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 56 个数据未匹配到足够的内容，重新提问...\n",
      "第 57 个数据未匹配到足够的内容，重新提问...\n",
      "第 58 个数据未匹配到足够的内容，重新提问...\n",
      "第 60 个数据未匹配到足够的内容，重新提问...\n",
      "第 62 个数据未匹配到足够的内容，重新提问...\n",
      "第 63 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 9...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 64 个数据未匹配到足够的内容，重新提问...\n",
      "第 65 个数据未匹配到足够的内容，重新提问...\n",
      "第 66 个数据未匹配到足够的内容，重新提问...\n",
      "第 67 个数据未匹配到足够的内容，重新提问...\n",
      "第 68 个数据未匹配到足够的内容，重新提问...\n",
      "第 69 个数据未匹配到足够的内容，重新提问...\n",
      "第 71 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 10...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 72 个数据未匹配到足够的内容，重新提问...\n",
      "第 73 个数据未匹配到足够的内容，重新提问...\n",
      "第 75 个数据未匹配到足够的内容，重新提问...\n",
      "第 76 个数据未匹配到足够的内容，重新提问...\n",
      "第 78 个数据未匹配到足够的内容，重新提问...\n",
      "第 79 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 11...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 81 个数据未匹配到足够的内容，重新提问...\n",
      "第 83 个数据未匹配到足够的内容，重新提问...\n",
      "第 84 个数据未匹配到足够的内容，重新提问...\n",
      "第 87 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 12...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 88 个数据未匹配到足够的内容，重新提问...\n",
      "第 89 个数据未匹配到足够的内容，重新提问...\n",
      "第 90 个数据未匹配到足够的内容，重新提问...\n",
      "第 91 个数据未匹配到足够的内容，重新提问...\n",
      "第 92 个数据未匹配到足够的内容，重新提问...\n",
      "第 93 个数据未匹配到足够的内容，重新提问...\n",
      "第 95 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 13...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 96 个数据未匹配到足够的内容，重新提问...\n",
      "第 97 个数据未匹配到足够的内容，重新提问...\n",
      "第 98 个数据未匹配到足够的内容，重新提问...\n",
      "第 100 个数据未匹配到足够的内容，重新提问...\n",
      "第 101 个数据未匹配到足够的内容，重新提问...\n",
      "第 102 个数据未匹配到足够的内容，重新提问...\n",
      "第 103 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 14...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 104 个数据未匹配到足够的内容，重新提问...\n",
      "第 106 个数据未匹配到足够的内容，重新提问...\n",
      "第 107 个数据未匹配到足够的内容，重新提问...\n",
      "第 108 个数据未匹配到足够的内容，重新提问...\n",
      "第 109 个数据未匹配到足够的内容，重新提问...\n",
      "第 111 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 15...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 113 个数据未匹配到足够的内容，重新提问...\n",
      "第 114 个数据未匹配到足够的内容，重新提问...\n",
      "第 115 个数据未匹配到足够的内容，重新提问...\n",
      "第 116 个数据未匹配到足够的内容，重新提问...\n",
      "第 117 个数据未匹配到足够的内容，重新提问...\n",
      "第 118 个数据未匹配到足够的内容，重新提问...\n",
      "第 119 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 16...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 120 个数据未匹配到足够的内容，重新提问...\n",
      "第 121 个数据未匹配到足够的内容，重新提问...\n",
      "第 122 个数据未匹配到足够的内容，重新提问...\n",
      "第 123 个数据未匹配到足够的内容，重新提问...\n",
      "第 127 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 17...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 129 个数据未匹配到足够的内容，重新提问...\n",
      "第 133 个数据未匹配到足够的内容，重新提问...\n",
      "第 134 个数据未匹配到足够的内容，重新提问...\n",
      "第 135 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 18...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 137 个数据未匹配到足够的内容，重新提问...\n",
      "第 139 个数据未匹配到足够的内容，重新提问...\n",
      "第 140 个数据未匹配到足够的内容，重新提问...\n",
      "第 141 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 19...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 144 个数据未匹配到足够的内容，重新提问...\n",
      "第 145 个数据未匹配到足够的内容，重新提问...\n",
      "第 147 个数据未匹配到足够的内容，重新提问...\n",
      "第 148 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 20...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 152 个数据未匹配到足够的内容，重新提问...\n",
      "第 153 个数据未匹配到足够的内容，重新提问...\n",
      "第 155 个数据未匹配到足够的内容，重新提问...\n",
      "第 156 个数据未匹配到足够的内容，重新提问...\n",
      "第 157 个数据未匹配到足够的内容，重新提问...\n",
      "第 158 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 21...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 162 个数据未匹配到足够的内容，重新提问...\n",
      "第 163 个数据未匹配到足够的内容，重新提问...\n",
      "第 166 个数据未匹配到足够的内容，重新提问...\n",
      "第 167 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 22...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 169 个数据未匹配到足够的内容，重新提问...\n",
      "第 170 个数据未匹配到足够的内容，重新提问...\n",
      "第 171 个数据未匹配到足够的内容，重新提问...\n",
      "第 173 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 23...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 178 个数据未匹配到足够的内容，重新提问...\n",
      "第 180 个数据未匹配到足够的内容，重新提问...\n",
      "第 183 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 24...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 185 个数据未匹配到足够的内容，重新提问...\n",
      "第 186 个数据未匹配到足够的内容，重新提问...\n",
      "第 190 个数据未匹配到足够的内容，重新提问...\n",
      "第 191 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 25...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 194 个数据未匹配到足够的内容，重新提问...\n",
      "第 195 个数据未匹配到足够的内容，重新提问...\n",
      "第 198 个数据未匹配到足够的内容，重新提问...\n",
      "第 199 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 26...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 201 个数据未匹配到足够的内容，重新提问...\n",
      "第 203 个数据未匹配到足够的内容，重新提问...\n",
      "第 205 个数据未匹配到足够的内容，重新提问...\n",
      "第 206 个数据未匹配到足够的内容，重新提问...\n",
      "第 207 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 27...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 208 个数据未匹配到足够的内容，重新提问...\n",
      "第 209 个数据未匹配到足够的内容，重新提问...\n",
      "第 210 个数据未匹配到足够的内容，重新提问...\n",
      "第 212 个数据未匹配到足够的内容，重新提问...\n",
      "第 213 个数据未匹配到足够的内容，重新提问...\n",
      "第 214 个数据未匹配到足够的内容，重新提问...\n",
      "第 215 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 28...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 217 个数据未匹配到足够的内容，重新提问...\n",
      "第 219 个数据未匹配到足够的内容，重新提问...\n",
      "第 222 个数据未匹配到足够的内容，重新提问...\n",
      "第 223 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 29...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 224 个数据未匹配到足够的内容，重新提问...\n",
      "第 225 个数据未匹配到足够的内容，重新提问...\n",
      "第 227 个数据未匹配到足够的内容，重新提问...\n",
      "第 229 个数据未匹配到足够的内容，重新提问...\n",
      "第 230 个数据未匹配到足够的内容，重新提问...\n",
      "第 231 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 30...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 232 个数据未匹配到足够的内容，重新提问...\n",
      "第 234 个数据未匹配到足够的内容，重新提问...\n",
      "第 235 个数据未匹配到足够的内容，重新提问...\n",
      "第 236 个数据未匹配到足够的内容，重新提问...\n",
      "第 237 个数据未匹配到足够的内容，重新提问...\n",
      "第 238 个数据未匹配到足够的内容，重新提问...\n",
      "第 239 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 31...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 242 个数据未匹配到足够的内容，重新提问...\n",
      "第 243 个数据未匹配到足够的内容，重新提问...\n",
      "第 244 个数据未匹配到足够的内容，重新提问...\n",
      "第 245 个数据未匹配到足够的内容，重新提问...\n",
      "第 246 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 32...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 248 个数据未匹配到足够的内容，重新提问...\n",
      "第 249 个数据未匹配到足够的内容，重新提问...\n",
      "第 251 个数据未匹配到足够的内容，重新提问...\n",
      "第 253 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 33...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 257 个数据未匹配到足够的内容，重新提问...\n",
      "第 258 个数据未匹配到足够的内容，重新提问...\n",
      "第 260 个数据未匹配到足够的内容，重新提问...\n",
      "第 261 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 34...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 264 个数据未匹配到足够的内容，重新提问...\n",
      "第 265 个数据未匹配到足够的内容，重新提问...\n",
      "第 266 个数据未匹配到足够的内容，重新提问...\n",
      "第 267 个数据未匹配到足够的内容，重新提问...\n",
      "第 268 个数据未匹配到足够的内容，重新提问...\n",
      "第 269 个数据未匹配到足够的内容，重新提问...\n",
      "第 271 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 35...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 273 个数据未匹配到足够的内容，重新提问...\n",
      "第 274 个数据未匹配到足够的内容，重新提问...\n",
      "第 275 个数据未匹配到足够的内容，重新提问...\n",
      "第 277 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 36...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 280 个数据未匹配到足够的内容，重新提问...\n",
      "第 281 个数据未匹配到足够的内容，重新提问...\n",
      "第 283 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 37...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 288 个数据未匹配到足够的内容，重新提问...\n",
      "第 291 个数据未匹配到足够的内容，重新提问...\n",
      "第 294 个数据未匹配到足够的内容，重新提问...\n",
      "第 295 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 38...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 297 个数据未匹配到足够的内容，重新提问...\n",
      "第 301 个数据未匹配到足够的内容，重新提问...\n",
      "第 303 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 39...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 304 个数据未匹配到足够的内容，重新提问...\n",
      "第 305 个数据未匹配到足够的内容，重新提问...\n",
      "第 306 个数据未匹配到足够的内容，重新提问...\n",
      "第 307 个数据未匹配到足够的内容，重新提问...\n",
      "第 308 个数据未匹配到足够的内容，重新提问...\n",
      "第 311 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 40...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 312 个数据未匹配到足够的内容，重新提问...\n",
      "第 314 个数据未匹配到足够的内容，重新提问...\n",
      "第 316 个数据未匹配到足够的内容，重新提问...\n",
      "第 317 个数据未匹配到足够的内容，重新提问...\n",
      "第 319 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 41...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 320 个数据未匹配到足够的内容，重新提问...\n",
      "第 323 个数据未匹配到足够的内容，重新提问...\n",
      "第 325 个数据未匹配到足够的内容，重新提问...\n",
      "第 326 个数据未匹配到足够的内容，重新提问...\n",
      "第 327 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 42...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 328 个数据未匹配到足够的内容，重新提问...\n",
      "第 331 个数据未匹配到足够的内容，重新提问...\n",
      "第 333 个数据未匹配到足够的内容，重新提问...\n",
      "第 334 个数据未匹配到足够的内容，重新提问...\n",
      "第 335 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 43...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 337 个数据未匹配到足够的内容，重新提问...\n",
      "第 338 个数据未匹配到足够的内容，重新提问...\n",
      "第 339 个数据未匹配到足够的内容，重新提问...\n",
      "第 340 个数据未匹配到足够的内容，重新提问...\n",
      "第 341 个数据未匹配到足够的内容，重新提问...\n",
      "第 343 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 44...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 344 个数据未匹配到足够的内容，重新提问...\n",
      "第 345 个数据未匹配到足够的内容，重新提问...\n",
      "第 348 个数据未匹配到足够的内容，重新提问...\n",
      "第 349 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 45...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 352 个数据未匹配到足够的内容，重新提问...\n",
      "第 354 个数据未匹配到足够的内容，重新提问...\n",
      "第 355 个数据未匹配到足够的内容，重新提问...\n",
      "第 359 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 46...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 360 个数据未匹配到足够的内容，重新提问...\n",
      "第 363 个数据未匹配到足够的内容，重新提问...\n",
      "第 365 个数据未匹配到足够的内容，重新提问...\n",
      "第 366 个数据未匹配到足够的内容，重新提问...\n",
      "第 367 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 47...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 370 个数据未匹配到足够的内容，重新提问...\n",
      "第 371 个数据未匹配到足够的内容，重新提问...\n",
      "第 372 个数据未匹配到足够的内容，重新提问...\n",
      "第 374 个数据未匹配到足够的内容，重新提问...\n",
      "第 375 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 48...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 376 个数据未匹配到足够的内容，重新提问...\n",
      "第 378 个数据未匹配到足够的内容，重新提问...\n",
      "第 379 个数据未匹配到足够的内容，重新提问...\n",
      "第 381 个数据未匹配到足够的内容，重新提问...\n",
      "第 382 个数据未匹配到足够的内容，重新提问...\n",
      "第 383 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 49...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 385 个数据未匹配到足够的内容，重新提问...\n",
      "第 386 个数据未匹配到足够的内容，重新提问...\n",
      "第 387 个数据未匹配到足够的内容，重新提问...\n",
      "第 391 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 50...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 393 个数据未匹配到足够的内容，重新提问...\n",
      "第 394 个数据未匹配到足够的内容，重新提问...\n",
      "第 396 个数据未匹配到足够的内容，重新提问...\n",
      "第 397 个数据未匹配到足够的内容，重新提问...\n",
      "第 398 个数据未匹配到足够的内容，重新提问...\n",
      "第 399 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 51...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 402 个数据未匹配到足够的内容，重新提问...\n",
      "第 403 个数据未匹配到足够的内容，重新提问...\n",
      "第 404 个数据未匹配到足够的内容，重新提问...\n",
      "第 405 个数据未匹配到足够的内容，重新提问...\n",
      "第 406 个数据未匹配到足够的内容，重新提问...\n",
      "第 407 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 52...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 410 个数据未匹配到足够的内容，重新提问...\n",
      "第 411 个数据未匹配到足够的内容，重新提问...\n",
      "第 413 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 53...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 416 个数据未匹配到足够的内容，重新提问...\n",
      "第 418 个数据未匹配到足够的内容，重新提问...\n",
      "第 419 个数据未匹配到足够的内容，重新提问...\n",
      "第 421 个数据未匹配到足够的内容，重新提问...\n",
      "第 423 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 54...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 424 个数据未匹配到足够的内容，重新提问...\n",
      "第 425 个数据未匹配到足够的内容，重新提问...\n",
      "第 429 个数据未匹配到足够的内容，重新提问...\n",
      "第 430 个数据未匹配到足够的内容，重新提问...\n",
      "第 431 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 55...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 433 个数据未匹配到足够的内容，重新提问...\n",
      "第 434 个数据未匹配到足够的内容，重新提问...\n",
      "第 436 个数据未匹配到足够的内容，重新提问...\n",
      "第 437 个数据未匹配到足够的内容，重新提问...\n",
      "第 439 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 56...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 441 个数据未匹配到足够的内容，重新提问...\n",
      "第 442 个数据未匹配到足够的内容，重新提问...\n",
      "第 444 个数据未匹配到足够的内容，重新提问...\n",
      "第 445 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 57...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 448 个数据未匹配到足够的内容，重新提问...\n",
      "第 452 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 58...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 456 个数据未匹配到足够的内容，重新提问...\n",
      "第 457 个数据未匹配到足够的内容，重新提问...\n",
      "第 458 个数据未匹配到足够的内容，重新提问...\n",
      "第 459 个数据未匹配到足够的内容，重新提问...\n",
      "第 461 个数据未匹配到足够的内容，重新提问...\n",
      "第 462 个数据未匹配到足够的内容，重新提问...\n",
      "第 463 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 59...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 464 个数据未匹配到足够的内容，重新提问...\n",
      "第 465 个数据未匹配到足够的内容，重新提问...\n",
      "第 466 个数据未匹配到足够的内容，重新提问...\n",
      "第 467 个数据未匹配到足够的内容，重新提问...\n",
      "第 469 个数据未匹配到足够的内容，重新提问...\n",
      "第 470 个数据未匹配到足够的内容，重新提问...\n",
      "第 471 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 60...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 472 个数据未匹配到足够的内容，重新提问...\n",
      "第 473 个数据未匹配到足够的内容，重新提问...\n",
      "第 476 个数据未匹配到足够的内容，重新提问...\n",
      "第 478 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 61...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 480 个数据未匹配到足够的内容，重新提问...\n",
      "第 482 个数据未匹配到足够的内容，重新提问...\n",
      "第 483 个数据未匹配到足够的内容，重新提问...\n",
      "第 484 个数据未匹配到足够的内容，重新提问...\n",
      "第 485 个数据未匹配到足够的内容，重新提问...\n",
      "第 486 个数据未匹配到足够的内容，重新提问...\n",
      "第 487 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 62...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 488 个数据未匹配到足够的内容，重新提问...\n",
      "第 489 个数据未匹配到足够的内容，重新提问...\n",
      "第 491 个数据未匹配到足够的内容，重新提问...\n",
      "第 492 个数据未匹配到足够的内容，重新提问...\n",
      "第 493 个数据未匹配到足够的内容，重新提问...\n",
      "第 494 个数据未匹配到足够的内容，重新提问...\n",
      "第 495 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 63...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 496 个数据未匹配到足够的内容，重新提问...\n",
      "第 497 个数据未匹配到足够的内容，重新提问...\n",
      "第 499 个数据未匹配到足够的内容，重新提问...\n",
      "第 500 个数据未匹配到足够的内容，重新提问...\n",
      "第 501 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 64...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 504 个数据未匹配到足够的内容，重新提问...\n",
      "第 506 个数据未匹配到足够的内容，重新提问...\n",
      "第 507 个数据未匹配到足够的内容，重新提问...\n",
      "第 510 个数据未匹配到足够的内容，重新提问...\n",
      "第 511 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 65...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 512 个数据未匹配到足够的内容，重新提问...\n",
      "第 513 个数据未匹配到足够的内容，重新提问...\n",
      "第 514 个数据未匹配到足够的内容，重新提问...\n",
      "第 515 个数据未匹配到足够的内容，重新提问...\n",
      "第 517 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 66...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 520 个数据未匹配到足够的内容，重新提问...\n",
      "第 521 个数据未匹配到足够的内容，重新提问...\n",
      "第 522 个数据未匹配到足够的内容，重新提问...\n",
      "第 525 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 67...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 528 个数据未匹配到足够的内容，重新提问...\n",
      "第 529 个数据未匹配到足够的内容，重新提问...\n",
      "第 532 个数据未匹配到足够的内容，重新提问...\n",
      "第 534 个数据未匹配到足够的内容，重新提问...\n",
      "第 535 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 68...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 538 个数据未匹配到足够的内容，重新提问...\n",
      "第 539 个数据未匹配到足够的内容，重新提问...\n",
      "第 541 个数据未匹配到足够的内容，重新提问...\n",
      "第 543 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 69...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 544 个数据未匹配到足够的内容，重新提问...\n",
      "第 545 个数据未匹配到足够的内容，重新提问...\n",
      "第 546 个数据未匹配到足够的内容，重新提问...\n",
      "第 549 个数据未匹配到足够的内容，重新提问...\n",
      "第 551 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 70...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 552 个数据未匹配到足够的内容，重新提问...\n",
      "第 553 个数据未匹配到足够的内容，重新提问...\n",
      "第 554 个数据未匹配到足够的内容，重新提问...\n",
      "第 555 个数据未匹配到足够的内容，重新提问...\n",
      "第 556 个数据未匹配到足够的内容，重新提问...\n",
      "第 557 个数据未匹配到足够的内容，重新提问...\n",
      "第 559 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 71...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 560 个数据未匹配到足够的内容，重新提问...\n",
      "第 562 个数据未匹配到足够的内容，重新提问...\n",
      "第 563 个数据未匹配到足够的内容，重新提问...\n",
      "第 564 个数据未匹配到足够的内容，重新提问...\n",
      "第 565 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 72...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 569 个数据未匹配到足够的内容，重新提问...\n",
      "第 570 个数据未匹配到足够的内容，重新提问...\n",
      "第 571 个数据未匹配到足够的内容，重新提问...\n",
      "第 573 个数据未匹配到足够的内容，重新提问...\n",
      "第 574 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 73...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 577 个数据未匹配到足够的内容，重新提问...\n",
      "第 579 个数据未匹配到足够的内容，重新提问...\n",
      "第 580 个数据未匹配到足够的内容，重新提问...\n",
      "第 581 个数据未匹配到足够的内容，重新提问...\n",
      "第 582 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 74...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 584 个数据未匹配到足够的内容，重新提问...\n",
      "第 588 个数据未匹配到足够的内容，重新提问...\n",
      "第 590 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 75...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 592 个数据未匹配到足够的内容，重新提问...\n",
      "第 593 个数据未匹配到足够的内容，重新提问...\n",
      "第 594 个数据未匹配到足够的内容，重新提问...\n",
      "第 595 个数据未匹配到足够的内容，重新提问...\n",
      "第 596 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 76...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 600 个数据未匹配到足够的内容，重新提问...\n",
      "第 601 个数据未匹配到足够的内容，重新提问...\n",
      "第 602 个数据未匹配到足够的内容，重新提问...\n",
      "第 603 个数据未匹配到足够的内容，重新提问...\n",
      "第 605 个数据未匹配到足够的内容，重新提问...\n",
      "第 606 个数据未匹配到足够的内容，重新提问...\n",
      "第 607 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 77...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 608 个数据未匹配到足够的内容，重新提问...\n",
      "第 609 个数据未匹配到足够的内容，重新提问...\n",
      "第 610 个数据未匹配到足够的内容，重新提问...\n",
      "第 611 个数据未匹配到足够的内容，重新提问...\n",
      "第 612 个数据未匹配到足够的内容，重新提问...\n",
      "第 613 个数据未匹配到足够的内容，重新提问...\n",
      "第 614 个数据未匹配到足够的内容，重新提问...\n",
      "第 615 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 78...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 616 个数据未匹配到足够的内容，重新提问...\n",
      "第 618 个数据未匹配到足够的内容，重新提问...\n",
      "第 619 个数据未匹配到足够的内容，重新提问...\n",
      "第 620 个数据未匹配到足够的内容，重新提问...\n",
      "第 621 个数据未匹配到足够的内容，重新提问...\n",
      "第 623 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 79...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 624 个数据未匹配到足够的内容，重新提问...\n",
      "第 625 个数据未匹配到足够的内容，重新提问...\n",
      "第 628 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 80...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 634 个数据未匹配到足够的内容，重新提问...\n",
      "第 635 个数据未匹配到足够的内容，重新提问...\n",
      "第 637 个数据未匹配到足够的内容，重新提问...\n",
      "第 638 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 81...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 640 个数据未匹配到足够的内容，重新提问...\n",
      "第 641 个数据未匹配到足够的内容，重新提问...\n",
      "第 642 个数据未匹配到足够的内容，重新提问...\n",
      "第 643 个数据未匹配到足够的内容，重新提问...\n",
      "第 645 个数据未匹配到足够的内容，重新提问...\n",
      "第 646 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 82...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 651 个数据未匹配到足够的内容，重新提问...\n",
      "第 652 个数据未匹配到足够的内容，重新提问...\n",
      "第 653 个数据未匹配到足够的内容，重新提问...\n",
      "第 654 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 83...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 656 个数据未匹配到足够的内容，重新提问...\n",
      "第 658 个数据未匹配到足够的内容，重新提问...\n",
      "第 660 个数据未匹配到足够的内容，重新提问...\n",
      "第 661 个数据未匹配到足够的内容，重新提问...\n",
      "第 662 个数据未匹配到足够的内容，重新提问...\n",
      "第 663 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 84...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 667 个数据未匹配到足够的内容，重新提问...\n",
      "第 668 个数据未匹配到足够的内容，重新提问...\n",
      "第 670 个数据未匹配到足够的内容，重新提问...\n",
      "第 671 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 85...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 672 个数据未匹配到足够的内容，重新提问...\n",
      "第 673 个数据未匹配到足够的内容，重新提问...\n",
      "第 675 个数据未匹配到足够的内容，重新提问...\n",
      "第 676 个数据未匹配到足够的内容，重新提问...\n",
      "第 678 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 86...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 681 个数据未匹配到足够的内容，重新提问...\n",
      "第 683 个数据未匹配到足够的内容，重新提问...\n",
      "第 684 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 87...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 688 个数据未匹配到足够的内容，重新提问...\n",
      "第 689 个数据未匹配到足够的内容，重新提问...\n",
      "第 690 个数据未匹配到足够的内容，重新提问...\n",
      "第 691 个数据未匹配到足够的内容，重新提问...\n",
      "第 692 个数据未匹配到足够的内容，重新提问...\n",
      "第 693 个数据未匹配到足够的内容，重新提问...\n",
      "第 694 个数据未匹配到足够的内容，重新提问...\n",
      "第 695 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 88...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 698 个数据未匹配到足够的内容，重新提问...\n",
      "第 699 个数据未匹配到足够的内容，重新提问...\n",
      "第 700 个数据未匹配到足够的内容，重新提问...\n",
      "第 701 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 89...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 709 个数据未匹配到足够的内容，重新提问...\n",
      "第 710 个数据未匹配到足够的内容，重新提问...\n",
      "第 711 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 90...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 712 个数据未匹配到足够的内容，重新提问...\n",
      "第 713 个数据未匹配到足够的内容，重新提问...\n",
      "第 714 个数据未匹配到足够的内容，重新提问...\n",
      "第 716 个数据未匹配到足够的内容，重新提问...\n",
      "第 717 个数据未匹配到足够的内容，重新提问...\n",
      "第 718 个数据未匹配到足够的内容，重新提问...\n",
      "第 719 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 91...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 722 个数据未匹配到足够的内容，重新提问...\n",
      "第 723 个数据未匹配到足够的内容，重新提问...\n",
      "第 724 个数据未匹配到足够的内容，重新提问...\n",
      "第 726 个数据未匹配到足够的内容，重新提问...\n",
      "第 727 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 92...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 729 个数据未匹配到足够的内容，重新提问...\n",
      "第 731 个数据未匹配到足够的内容，重新提问...\n",
      "第 734 个数据未匹配到足够的内容，重新提问...\n",
      "第 735 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 93...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 736 个数据未匹配到足够的内容，重新提问...\n",
      "第 737 个数据未匹配到足够的内容，重新提问...\n",
      "第 739 个数据未匹配到足够的内容，重新提问...\n",
      "第 741 个数据未匹配到足够的内容，重新提问...\n",
      "第 742 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 94...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 744 个数据未匹配到足够的内容，重新提问...\n",
      "第 745 个数据未匹配到足够的内容，重新提问...\n",
      "第 746 个数据未匹配到足够的内容，重新提问...\n",
      "第 750 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 95...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 752 个数据未匹配到足够的内容，重新提问...\n",
      "第 757 个数据未匹配到足够的内容，重新提问...\n",
      "第 759 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 96...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 760 个数据未匹配到足够的内容，重新提问...\n",
      "第 761 个数据未匹配到足够的内容，重新提问...\n",
      "第 762 个数据未匹配到足够的内容，重新提问...\n",
      "第 763 个数据未匹配到足够的内容，重新提问...\n",
      "第 764 个数据未匹配到足够的内容，重新提问...\n",
      "第 765 个数据未匹配到足够的内容，重新提问...\n",
      "第 766 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 97...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 768 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 98...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 776 个数据未匹配到足够的内容，重新提问...\n",
      "第 779 个数据未匹配到足够的内容，重新提问...\n",
      "第 781 个数据未匹配到足够的内容，重新提问...\n",
      "第 782 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 99...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 785 个数据未匹配到足够的内容，重新提问...\n",
      "第 786 个数据未匹配到足够的内容，重新提问...\n",
      "第 787 个数据未匹配到足够的内容，重新提问...\n",
      "第 788 个数据未匹配到足够的内容，重新提问...\n",
      "第 790 个数据未匹配到足够的内容，重新提问...\n",
      "第 791 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 792 个数据未匹配到足够的内容，重新提问...\n",
      "第 793 个数据未匹配到足够的内容，重新提问...\n",
      "第 794 个数据未匹配到足够的内容，重新提问...\n",
      "第 795 个数据未匹配到足够的内容，重新提问...\n",
      "第 796 个数据未匹配到足够的内容，重新提问...\n",
      "第 797 个数据未匹配到足够的内容，重新提问...\n",
      "第 799 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 101...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 803 个数据未匹配到足够的内容，重新提问...\n",
      "第 804 个数据未匹配到足够的内容，重新提问...\n",
      "第 805 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 102...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 808 个数据未匹配到足够的内容，重新提问...\n",
      "第 809 个数据未匹配到足够的内容，重新提问...\n",
      "第 810 个数据未匹配到足够的内容，重新提问...\n",
      "第 815 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 103...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 820 个数据未匹配到足够的内容，重新提问...\n",
      "第 821 个数据未匹配到足够的内容，重新提问...\n",
      "第 822 个数据未匹配到足够的内容，重新提问...\n",
      "第 823 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 104...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 824 个数据未匹配到足够的内容，重新提问...\n",
      "第 825 个数据未匹配到足够的内容，重新提问...\n",
      "第 827 个数据未匹配到足够的内容，重新提问...\n",
      "第 828 个数据未匹配到足够的内容，重新提问...\n",
      "第 830 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 105...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 832 个数据未匹配到足够的内容，重新提问...\n",
      "第 837 个数据未匹配到足够的内容，重新提问...\n",
      "第 838 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 106...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 844 个数据未匹配到足够的内容，重新提问...\n",
      "第 845 个数据未匹配到足够的内容，重新提问...\n",
      "第 846 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 107...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 850 个数据未匹配到足够的内容，重新提问...\n",
      "第 851 个数据未匹配到足够的内容，重新提问...\n",
      "第 852 个数据未匹配到足够的内容，重新提问...\n",
      "第 853 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 108...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 856 个数据未匹配到足够的内容，重新提问...\n",
      "第 858 个数据未匹配到足够的内容，重新提问...\n",
      "第 859 个数据未匹配到足够的内容，重新提问...\n",
      "第 862 个数据未匹配到足够的内容，重新提问...\n",
      "第 863 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 109...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 864 个数据未匹配到足够的内容，重新提问...\n",
      "第 865 个数据未匹配到足够的内容，重新提问...\n",
      "第 866 个数据未匹配到足够的内容，重新提问...\n",
      "第 869 个数据未匹配到足够的内容，重新提问...\n",
      "第 870 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 110...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 872 个数据未匹配到足够的内容，重新提问...\n",
      "第 874 个数据未匹配到足够的内容，重新提问...\n",
      "第 877 个数据未匹配到足够的内容，重新提问...\n",
      "第 878 个数据未匹配到足够的内容，重新提问...\n",
      "第 879 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 111...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 881 个数据未匹配到足够的内容，重新提问...\n",
      "第 882 个数据未匹配到足够的内容，重新提问...\n",
      "第 883 个数据未匹配到足够的内容，重新提问...\n",
      "第 885 个数据未匹配到足够的内容，重新提问...\n",
      "第 887 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 112...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 888 个数据未匹配到足够的内容，重新提问...\n",
      "第 890 个数据未匹配到足够的内容，重新提问...\n",
      "第 894 个数据未匹配到足够的内容，重新提问...\n",
      "第 895 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 113...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 897 个数据未匹配到足够的内容，重新提问...\n",
      "第 898 个数据未匹配到足够的内容，重新提问...\n",
      "第 899 个数据未匹配到足够的内容，重新提问...\n",
      "第 900 个数据未匹配到足够的内容，重新提问...\n",
      "第 901 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 114...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 904 个数据未匹配到足够的内容，重新提问...\n",
      "第 906 个数据未匹配到足够的内容，重新提问...\n",
      "第 910 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 115...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 912 个数据未匹配到足够的内容，重新提问...\n",
      "第 914 个数据未匹配到足够的内容，重新提问...\n",
      "第 915 个数据未匹配到足够的内容，重新提问...\n",
      "第 917 个数据未匹配到足够的内容，重新提问...\n",
      "第 918 个数据未匹配到足够的内容，重新提问...\n",
      "第 919 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 116...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 921 个数据未匹配到足够的内容，重新提问...\n",
      "第 922 个数据未匹配到足够的内容，重新提问...\n",
      "第 924 个数据未匹配到足够的内容，重新提问...\n",
      "第 926 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 117...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 928 个数据未匹配到足够的内容，重新提问...\n",
      "第 932 个数据未匹配到足够的内容，重新提问...\n",
      "第 934 个数据未匹配到足够的内容，重新提问...\n",
      "第 935 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 118...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 936 个数据未匹配到足够的内容，重新提问...\n",
      "第 937 个数据未匹配到足够的内容，重新提问...\n",
      "第 938 个数据未匹配到足够的内容，重新提问...\n",
      "第 939 个数据未匹配到足够的内容，重新提问...\n",
      "第 940 个数据未匹配到足够的内容，重新提问...\n",
      "第 942 个数据未匹配到足够的内容，重新提问...\n",
      "第 943 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 119...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 946 个数据未匹配到足够的内容，重新提问...\n",
      "第 947 个数据未匹配到足够的内容，重新提问...\n",
      "第 950 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 120...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 952 个数据未匹配到足够的内容，重新提问...\n",
      "第 953 个数据未匹配到足够的内容，重新提问...\n",
      "第 954 个数据未匹配到足够的内容，重新提问...\n",
      "第 956 个数据未匹配到足够的内容，重新提问...\n",
      "第 957 个数据未匹配到足够的内容，重新提问...\n",
      "第 959 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 121...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 961 个数据未匹配到足够的内容，重新提问...\n",
      "第 963 个数据未匹配到足够的内容，重新提问...\n",
      "第 965 个数据未匹配到足够的内容，重新提问...\n",
      "第 967 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 122...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 970 个数据未匹配到足够的内容，重新提问...\n",
      "第 971 个数据未匹配到足够的内容，重新提问...\n",
      "第 972 个数据未匹配到足够的内容，重新提问...\n",
      "第 974 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 123...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 978 个数据未匹配到足够的内容，重新提问...\n",
      "第 979 个数据未匹配到足够的内容，重新提问...\n",
      "第 980 个数据未匹配到足够的内容，重新提问...\n",
      "第 981 个数据未匹配到足够的内容，重新提问...\n",
      "第 982 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 124...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 984 个数据未匹配到足够的内容，重新提问...\n",
      "第 985 个数据未匹配到足够的内容，重新提问...\n",
      "第 986 个数据未匹配到足够的内容，重新提问...\n",
      "第 987 个数据未匹配到足够的内容，重新提问...\n",
      "第 991 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 125...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 994 个数据未匹配到足够的内容，重新提问...\n",
      "第 995 个数据未匹配到足够的内容，重新提问...\n",
      "第 996 个数据未匹配到足够的内容，重新提问...\n",
      "第 998 个数据未匹配到足够的内容，重新提问...\n",
      "第 999 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 126...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1002 个数据未匹配到足够的内容，重新提问...\n",
      "第 1004 个数据未匹配到足够的内容，重新提问...\n",
      "第 1005 个数据未匹配到足够的内容，重新提问...\n",
      "第 1007 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 127...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1008 个数据未匹配到足够的内容，重新提问...\n",
      "第 1009 个数据未匹配到足够的内容，重新提问...\n",
      "第 1010 个数据未匹配到足够的内容，重新提问...\n",
      "第 1012 个数据未匹配到足够的内容，重新提问...\n",
      "第 1013 个数据未匹配到足够的内容，重新提问...\n",
      "第 1015 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 128...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1016 个数据未匹配到足够的内容，重新提问...\n",
      "第 1017 个数据未匹配到足够的内容，重新提问...\n",
      "第 1018 个数据未匹配到足够的内容，重新提问...\n",
      "第 1019 个数据未匹配到足够的内容，重新提问...\n",
      "第 1021 个数据未匹配到足够的内容，重新提问...\n",
      "第 1022 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 129...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1024 个数据未匹配到足够的内容，重新提问...\n",
      "第 1025 个数据未匹配到足够的内容，重新提问...\n",
      "第 1027 个数据未匹配到足够的内容，重新提问...\n",
      "第 1029 个数据未匹配到足够的内容，重新提问...\n",
      "第 1030 个数据未匹配到足够的内容，重新提问...\n",
      "第 1031 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 130...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1032 个数据未匹配到足够的内容，重新提问...\n",
      "第 1036 个数据未匹配到足够的内容，重新提问...\n",
      "第 1037 个数据未匹配到足够的内容，重新提问...\n",
      "第 1038 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 131...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1040 个数据未匹配到足够的内容，重新提问...\n",
      "第 1041 个数据未匹配到足够的内容，重新提问...\n",
      "第 1043 个数据未匹配到足够的内容，重新提问...\n",
      "第 1044 个数据未匹配到足够的内容，重新提问...\n",
      "第 1045 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 132...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1048 个数据未匹配到足够的内容，重新提问...\n",
      "第 1050 个数据未匹配到足够的内容，重新提问...\n",
      "第 1052 个数据未匹配到足够的内容，重新提问...\n",
      "第 1053 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 133...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1056 个数据未匹配到足够的内容，重新提问...\n",
      "第 1057 个数据未匹配到足够的内容，重新提问...\n",
      "第 1058 个数据未匹配到足够的内容，重新提问...\n",
      "第 1060 个数据未匹配到足够的内容，重新提问...\n",
      "第 1061 个数据未匹配到足够的内容，重新提问...\n",
      "第 1063 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 134...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1064 个数据未匹配到足够的内容，重新提问...\n",
      "第 1065 个数据未匹配到足够的内容，重新提问...\n",
      "第 1068 个数据未匹配到足够的内容，重新提问...\n",
      "第 1070 个数据未匹配到足够的内容，重新提问...\n",
      "第 1071 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 135...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1072 个数据未匹配到足够的内容，重新提问...\n",
      "第 1074 个数据未匹配到足够的内容，重新提问...\n",
      "第 1075 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 136...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1080 个数据未匹配到足够的内容，重新提问...\n",
      "第 1081 个数据未匹配到足够的内容，重新提问...\n",
      "第 1082 个数据未匹配到足够的内容，重新提问...\n",
      "第 1083 个数据未匹配到足够的内容，重新提问...\n",
      "第 1085 个数据未匹配到足够的内容，重新提问...\n",
      "第 1086 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 137...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1088 个数据未匹配到足够的内容，重新提问...\n",
      "第 1091 个数据未匹配到足够的内容，重新提问...\n",
      "第 1092 个数据未匹配到足够的内容，重新提问...\n",
      "第 1093 个数据未匹配到足够的内容，重新提问...\n",
      "第 1095 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 138...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1097 个数据未匹配到足够的内容，重新提问...\n",
      "第 1098 个数据未匹配到足够的内容，重新提问...\n",
      "第 1099 个数据未匹配到足够的内容，重新提问...\n",
      "第 1100 个数据未匹配到足够的内容，重新提问...\n",
      "第 1101 个数据未匹配到足够的内容，重新提问...\n",
      "第 1102 个数据未匹配到足够的内容，重新提问...\n",
      "第 1103 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 139...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1104 个数据未匹配到足够的内容，重新提问...\n",
      "第 1105 个数据未匹配到足够的内容，重新提问...\n",
      "第 1107 个数据未匹配到足够的内容，重新提问...\n",
      "第 1109 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 140...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1112 个数据未匹配到足够的内容，重新提问...\n",
      "第 1113 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 141...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1121 个数据未匹配到足够的内容，重新提问...\n",
      "第 1123 个数据未匹配到足够的内容，重新提问...\n",
      "第 1124 个数据未匹配到足够的内容，重新提问...\n",
      "第 1125 个数据未匹配到足够的内容，重新提问...\n",
      "第 1126 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 142...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1128 个数据未匹配到足够的内容，重新提问...\n",
      "第 1129 个数据未匹配到足够的内容，重新提问...\n",
      "第 1130 个数据未匹配到足够的内容，重新提问...\n",
      "第 1131 个数据未匹配到足够的内容，重新提问...\n",
      "第 1134 个数据未匹配到足够的内容，重新提问...\n",
      "第 1135 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 143...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1136 个数据未匹配到足够的内容，重新提问...\n",
      "第 1138 个数据未匹配到足够的内容，重新提问...\n",
      "第 1139 个数据未匹配到足够的内容，重新提问...\n",
      "第 1140 个数据未匹配到足够的内容，重新提问...\n",
      "第 1141 个数据未匹配到足够的内容，重新提问...\n",
      "第 1142 个数据未匹配到足够的内容，重新提问...\n",
      "第 1143 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 144...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1146 个数据未匹配到足够的内容，重新提问...\n",
      "第 1147 个数据未匹配到足够的内容，重新提问...\n",
      "第 1149 个数据未匹配到足够的内容，重新提问...\n",
      "第 1150 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 145...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1153 个数据未匹配到足够的内容，重新提问...\n",
      "第 1154 个数据未匹配到足够的内容，重新提问...\n",
      "第 1155 个数据未匹配到足够的内容，重新提问...\n",
      "第 1156 个数据未匹配到足够的内容，重新提问...\n",
      "第 1157 个数据未匹配到足够的内容，重新提问...\n",
      "第 1159 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 146...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1160 个数据未匹配到足够的内容，重新提问...\n",
      "第 1161 个数据未匹配到足够的内容，重新提问...\n",
      "第 1162 个数据未匹配到足够的内容，重新提问...\n",
      "第 1163 个数据未匹配到足够的内容，重新提问...\n",
      "第 1164 个数据未匹配到足够的内容，重新提问...\n",
      "第 1165 个数据未匹配到足够的内容，重新提问...\n",
      "第 1166 个数据未匹配到足够的内容，重新提问...\n",
      "第 1167 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 147...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1168 个数据未匹配到足够的内容，重新提问...\n",
      "第 1169 个数据未匹配到足够的内容，重新提问...\n",
      "第 1170 个数据未匹配到足够的内容，重新提问...\n",
      "第 1171 个数据未匹配到足够的内容，重新提问...\n",
      "第 1174 个数据未匹配到足够的内容，重新提问...\n",
      "第 1175 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 148...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1176 个数据未匹配到足够的内容，重新提问...\n",
      "第 1177 个数据未匹配到足够的内容，重新提问...\n",
      "第 1181 个数据未匹配到足够的内容，重新提问...\n",
      "第 1183 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 149...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1184 个数据未匹配到足够的内容，重新提问...\n",
      "第 1187 个数据未匹配到足够的内容，重新提问...\n",
      "第 1189 个数据未匹配到足够的内容，重新提问...\n",
      "第 1190 个数据未匹配到足够的内容，重新提问...\n",
      "第 1191 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 150...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1192 个数据未匹配到足够的内容，重新提问...\n",
      "第 1193 个数据未匹配到足够的内容，重新提问...\n",
      "第 1194 个数据未匹配到足够的内容，重新提问...\n",
      "第 1195 个数据未匹配到足够的内容，重新提问...\n",
      "第 1196 个数据未匹配到足够的内容，重新提问...\n",
      "第 1197 个数据未匹配到足够的内容，重新提问...\n",
      "第 1198 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 151...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1200 个数据未匹配到足够的内容，重新提问...\n",
      "第 1201 个数据未匹配到足够的内容，重新提问...\n",
      "第 1202 个数据未匹配到足够的内容，重新提问...\n",
      "第 1203 个数据未匹配到足够的内容，重新提问...\n",
      "第 1206 个数据未匹配到足够的内容，重新提问...\n",
      "第 1207 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 152...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1208 个数据未匹配到足够的内容，重新提问...\n",
      "第 1210 个数据未匹配到足够的内容，重新提问...\n",
      "第 1212 个数据未匹配到足够的内容，重新提问...\n",
      "第 1213 个数据未匹配到足够的内容，重新提问...\n",
      "第 1214 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 153...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1216 个数据未匹配到足够的内容，重新提问...\n",
      "第 1218 个数据未匹配到足够的内容，重新提问...\n",
      "第 1219 个数据未匹配到足够的内容，重新提问...\n",
      "第 1220 个数据未匹配到足够的内容，重新提问...\n",
      "第 1221 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 154...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1224 个数据未匹配到足够的内容，重新提问...\n",
      "第 1228 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 155...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1232 个数据未匹配到足够的内容，重新提问...\n",
      "第 1233 个数据未匹配到足够的内容，重新提问...\n",
      "第 1234 个数据未匹配到足够的内容，重新提问...\n",
      "第 1237 个数据未匹配到足够的内容，重新提问...\n",
      "第 1238 个数据未匹配到足够的内容，重新提问...\n",
      "第 1239 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 156...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1240 个数据未匹配到足够的内容，重新提问...\n",
      "第 1244 个数据未匹配到足够的内容，重新提问...\n",
      "第 1245 个数据未匹配到足够的内容，重新提问...\n",
      "第 1246 个数据未匹配到足够的内容，重新提问...\n",
      "第 1247 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 157...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1249 个数据未匹配到足够的内容，重新提问...\n",
      "第 1251 个数据未匹配到足够的内容，重新提问...\n",
      "第 1252 个数据未匹配到足够的内容，重新提问...\n",
      "第 1253 个数据未匹配到足够的内容，重新提问...\n",
      "第 1254 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 158...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1256 个数据未匹配到足够的内容，重新提问...\n",
      "第 1257 个数据未匹配到足够的内容，重新提问...\n",
      "第 1259 个数据未匹配到足够的内容，重新提问...\n",
      "第 1260 个数据未匹配到足够的内容，重新提问...\n",
      "第 1263 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 159...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1266 个数据未匹配到足够的内容，重新提问...\n",
      "第 1267 个数据未匹配到足够的内容，重新提问...\n",
      "第 1268 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 160...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1272 个数据未匹配到足够的内容，重新提问...\n",
      "第 1273 个数据未匹配到足够的内容，重新提问...\n",
      "第 1275 个数据未匹配到足够的内容，重新提问...\n",
      "第 1276 个数据未匹配到足够的内容，重新提问...\n",
      "第 1277 个数据未匹配到足够的内容，重新提问...\n",
      "第 1279 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 161...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1280 个数据未匹配到足够的内容，重新提问...\n",
      "第 1281 个数据未匹配到足够的内容，重新提问...\n",
      "第 1282 个数据未匹配到足够的内容，重新提问...\n",
      "第 1284 个数据未匹配到足够的内容，重新提问...\n",
      "第 1285 个数据未匹配到足够的内容，重新提问...\n",
      "第 1286 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 162...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1290 个数据未匹配到足够的内容，重新提问...\n",
      "第 1291 个数据未匹配到足够的内容，重新提问...\n",
      "第 1294 个数据未匹配到足够的内容，重新提问...\n",
      "第 1295 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 163...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1299 个数据未匹配到足够的内容，重新提问...\n",
      "第 1300 个数据未匹配到足够的内容，重新提问...\n",
      "第 1302 个数据未匹配到足够的内容，重新提问...\n",
      "第 1303 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 164...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1304 个数据未匹配到足够的内容，重新提问...\n",
      "第 1306 个数据未匹配到足够的内容，重新提问...\n",
      "第 1307 个数据未匹配到足够的内容，重新提问...\n",
      "第 1310 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 165...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1312 个数据未匹配到足够的内容，重新提问...\n",
      "第 1313 个数据未匹配到足够的内容，重新提问...\n",
      "第 1314 个数据未匹配到足够的内容，重新提问...\n",
      "第 1315 个数据未匹配到足够的内容，重新提问...\n",
      "第 1316 个数据未匹配到足够的内容，重新提问...\n",
      "第 1317 个数据未匹配到足够的内容，重新提问...\n",
      "第 1318 个数据未匹配到足够的内容，重新提问...\n",
      "第 1319 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 166...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1320 个数据未匹配到足够的内容，重新提问...\n",
      "第 1321 个数据未匹配到足够的内容，重新提问...\n",
      "第 1324 个数据未匹配到足够的内容，重新提问...\n",
      "第 1325 个数据未匹配到足够的内容，重新提问...\n",
      "第 1326 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 167...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1328 个数据未匹配到足够的内容，重新提问...\n",
      "第 1329 个数据未匹配到足够的内容，重新提问...\n",
      "第 1331 个数据未匹配到足够的内容，重新提问...\n",
      "第 1332 个数据未匹配到足够的内容，重新提问...\n",
      "第 1333 个数据未匹配到足够的内容，重新提问...\n",
      "第 1335 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 168...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1339 个数据未匹配到足够的内容，重新提问...\n",
      "第 1340 个数据未匹配到足够的内容，重新提问...\n",
      "第 1341 个数据未匹配到足够的内容，重新提问...\n",
      "第 1342 个数据未匹配到足够的内容，重新提问...\n",
      "第 1343 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 169...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1344 个数据未匹配到足够的内容，重新提问...\n",
      "第 1346 个数据未匹配到足够的内容，重新提问...\n",
      "第 1348 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 170...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1352 个数据未匹配到足够的内容，重新提问...\n",
      "第 1355 个数据未匹配到足够的内容，重新提问...\n",
      "第 1356 个数据未匹配到足够的内容，重新提问...\n",
      "第 1357 个数据未匹配到足够的内容，重新提问...\n",
      "第 1358 个数据未匹配到足够的内容，重新提问...\n",
      "第 1359 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 171...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1362 个数据未匹配到足够的内容，重新提问...\n",
      "第 1363 个数据未匹配到足够的内容，重新提问...\n",
      "第 1364 个数据未匹配到足够的内容，重新提问...\n",
      "第 1365 个数据未匹配到足够的内容，重新提问...\n",
      "第 1367 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 172...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1368 个数据未匹配到足够的内容，重新提问...\n",
      "第 1369 个数据未匹配到足够的内容，重新提问...\n",
      "第 1370 个数据未匹配到足够的内容，重新提问...\n",
      "第 1371 个数据未匹配到足够的内容，重新提问...\n",
      "第 1372 个数据未匹配到足够的内容，重新提问...\n",
      "第 1373 个数据未匹配到足够的内容，重新提问...\n",
      "第 1374 个数据未匹配到足够的内容，重新提问...\n",
      "第 1375 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 173...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1376 个数据未匹配到足够的内容，重新提问...\n",
      "第 1377 个数据未匹配到足够的内容，重新提问...\n",
      "第 1378 个数据未匹配到足够的内容，重新提问...\n",
      "第 1381 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 174...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1384 个数据未匹配到足够的内容，重新提问...\n",
      "第 1386 个数据未匹配到足够的内容，重新提问...\n",
      "第 1387 个数据未匹配到足够的内容，重新提问...\n",
      "第 1388 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 175...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1392 个数据未匹配到足够的内容，重新提问...\n",
      "第 1395 个数据未匹配到足够的内容，重新提问...\n",
      "第 1397 个数据未匹配到足够的内容，重新提问...\n",
      "第 1398 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 176...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1400 个数据未匹配到足够的内容，重新提问...\n",
      "第 1401 个数据未匹配到足够的内容，重新提问...\n",
      "第 1403 个数据未匹配到足够的内容，重新提问...\n",
      "第 1405 个数据未匹配到足够的内容，重新提问...\n",
      "第 1406 个数据未匹配到足够的内容，重新提问...\n",
      "第 1407 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 177...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1408 个数据未匹配到足够的内容，重新提问...\n",
      "第 1409 个数据未匹配到足够的内容，重新提问...\n",
      "第 1410 个数据未匹配到足够的内容，重新提问...\n",
      "第 1411 个数据未匹配到足够的内容，重新提问...\n",
      "第 1412 个数据未匹配到足够的内容，重新提问...\n",
      "第 1415 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 178...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1416 个数据未匹配到足够的内容，重新提问...\n",
      "第 1417 个数据未匹配到足够的内容，重新提问...\n",
      "第 1419 个数据未匹配到足够的内容，重新提问...\n",
      "第 1421 个数据未匹配到足够的内容，重新提问...\n",
      "第 1423 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 179...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1424 个数据未匹配到足够的内容，重新提问...\n",
      "第 1425 个数据未匹配到足够的内容，重新提问...\n",
      "第 1426 个数据未匹配到足够的内容，重新提问...\n",
      "第 1428 个数据未匹配到足够的内容，重新提问...\n",
      "第 1430 个数据未匹配到足够的内容，重新提问...\n",
      "第 1431 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 180...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1432 个数据未匹配到足够的内容，重新提问...\n",
      "第 1435 个数据未匹配到足够的内容，重新提问...\n",
      "第 1436 个数据未匹配到足够的内容，重新提问...\n",
      "第 1437 个数据未匹配到足够的内容，重新提问...\n",
      "第 1438 个数据未匹配到足够的内容，重新提问...\n",
      "第 1439 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 181...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1440 个数据未匹配到足够的内容，重新提问...\n",
      "第 1442 个数据未匹配到足够的内容，重新提问...\n",
      "第 1443 个数据未匹配到足够的内容，重新提问...\n",
      "第 1444 个数据未匹配到足够的内容，重新提问...\n",
      "第 1446 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 182...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1452 个数据未匹配到足够的内容，重新提问...\n",
      "第 1453 个数据未匹配到足够的内容，重新提问...\n",
      "第 1454 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 183...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1459 个数据未匹配到足够的内容，重新提问...\n",
      "第 1460 个数据未匹配到足够的内容，重新提问...\n",
      "第 1461 个数据未匹配到足够的内容，重新提问...\n",
      "第 1463 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 184...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1465 个数据未匹配到足够的内容，重新提问...\n",
      "第 1466 个数据未匹配到足够的内容，重新提问...\n",
      "第 1467 个数据未匹配到足够的内容，重新提问...\n",
      "第 1468 个数据未匹配到足够的内容，重新提问...\n",
      "第 1469 个数据未匹配到足够的内容，重新提问...\n",
      "第 1470 个数据未匹配到足够的内容，重新提问...\n",
      "第 1471 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 185...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1475 个数据未匹配到足够的内容，重新提问...\n",
      "第 1477 个数据未匹配到足够的内容，重新提问...\n",
      "第 1479 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 186...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1480 个数据未匹配到足够的内容，重新提问...\n",
      "第 1481 个数据未匹配到足够的内容，重新提问...\n",
      "第 1482 个数据未匹配到足够的内容，重新提问...\n",
      "第 1484 个数据未匹配到足够的内容，重新提问...\n",
      "第 1486 个数据未匹配到足够的内容，重新提问...\n",
      "第 1487 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 187...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1489 个数据未匹配到足够的内容，重新提问...\n",
      "第 1490 个数据未匹配到足够的内容，重新提问...\n",
      "第 1491 个数据未匹配到足够的内容，重新提问...\n",
      "第 1492 个数据未匹配到足够的内容，重新提问...\n",
      "第 1493 个数据未匹配到足够的内容，重新提问...\n",
      "第 1495 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 188...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1502 个数据未匹配到足够的内容，重新提问...\n",
      "第 1503 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 189...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1504 个数据未匹配到足够的内容，重新提问...\n",
      "第 1505 个数据未匹配到足够的内容，重新提问...\n",
      "第 1506 个数据未匹配到足够的内容，重新提问...\n",
      "第 1507 个数据未匹配到足够的内容，重新提问...\n",
      "第 1508 个数据未匹配到足够的内容，重新提问...\n",
      "第 1510 个数据未匹配到足够的内容，重新提问...\n",
      "第 1511 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 190...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1512 个数据未匹配到足够的内容，重新提问...\n",
      "第 1513 个数据未匹配到足够的内容，重新提问...\n",
      "第 1514 个数据未匹配到足够的内容，重新提问...\n",
      "第 1515 个数据未匹配到足够的内容，重新提问...\n",
      "第 1516 个数据未匹配到足够的内容，重新提问...\n",
      "第 1517 个数据未匹配到足够的内容，重新提问...\n",
      "第 1519 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1520 个数据未匹配到足够的内容，重新提问...\n",
      "第 1523 个数据未匹配到足够的内容，重新提问...\n",
      "第 1524 个数据未匹配到足够的内容，重新提问...\n",
      "第 1525 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 192...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1528 个数据未匹配到足够的内容，重新提问...\n",
      "第 1529 个数据未匹配到足够的内容，重新提问...\n",
      "第 1531 个数据未匹配到足够的内容，重新提问...\n",
      "第 1533 个数据未匹配到足够的内容，重新提问...\n",
      "第 1534 个数据未匹配到足够的内容，重新提问...\n",
      "第 1535 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 193...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1536 个数据未匹配到足够的内容，重新提问...\n",
      "第 1537 个数据未匹配到足够的内容，重新提问...\n",
      "第 1538 个数据未匹配到足够的内容，重新提问...\n",
      "第 1541 个数据未匹配到足够的内容，重新提问...\n",
      "第 1542 个数据未匹配到足够的内容，重新提问...\n",
      "第 1543 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 194...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1544 个数据未匹配到足够的内容，重新提问...\n",
      "第 1546 个数据未匹配到足够的内容，重新提问...\n",
      "第 1547 个数据未匹配到足够的内容，重新提问...\n",
      "第 1548 个数据未匹配到足够的内容，重新提问...\n",
      "第 1549 个数据未匹配到足够的内容，重新提问...\n",
      "第 1550 个数据未匹配到足够的内容，重新提问...\n",
      "第 1551 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 195...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1552 个数据未匹配到足够的内容，重新提问...\n",
      "第 1553 个数据未匹配到足够的内容，重新提问...\n",
      "第 1554 个数据未匹配到足够的内容，重新提问...\n",
      "第 1556 个数据未匹配到足够的内容，重新提问...\n",
      "第 1557 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 196...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1561 个数据未匹配到足够的内容，重新提问...\n",
      "第 1563 个数据未匹配到足够的内容，重新提问...\n",
      "第 1564 个数据未匹配到足够的内容，重新提问...\n",
      "第 1565 个数据未匹配到足够的内容，重新提问...\n",
      "第 1566 个数据未匹配到足够的内容，重新提问...\n",
      "第 1567 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 197...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1568 个数据未匹配到足够的内容，重新提问...\n",
      "第 1569 个数据未匹配到足够的内容，重新提问...\n",
      "第 1571 个数据未匹配到足够的内容，重新提问...\n",
      "第 1573 个数据未匹配到足够的内容，重新提问...\n",
      "第 1574 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 198...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1577 个数据未匹配到足够的内容，重新提问...\n",
      "第 1579 个数据未匹配到足够的内容，重新提问...\n",
      "第 1580 个数据未匹配到足够的内容，重新提问...\n",
      "第 1583 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 199...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1584 个数据未匹配到足够的内容，重新提问...\n",
      "第 1585 个数据未匹配到足够的内容，重新提问...\n",
      "第 1586 个数据未匹配到足够的内容，重新提问...\n",
      "第 1587 个数据未匹配到足够的内容，重新提问...\n",
      "第 1588 个数据未匹配到足够的内容，重新提问...\n",
      "第 1590 个数据未匹配到足够的内容，重新提问...\n",
      "第 1591 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 200...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1593 个数据未匹配到足够的内容，重新提问...\n",
      "第 1594 个数据未匹配到足够的内容，重新提问...\n",
      "第 1595 个数据未匹配到足够的内容，重新提问...\n",
      "第 1596 个数据未匹配到足够的内容，重新提问...\n",
      "第 1598 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 201...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1600 个数据未匹配到足够的内容，重新提问...\n",
      "第 1601 个数据未匹配到足够的内容，重新提问...\n",
      "第 1602 个数据未匹配到足够的内容，重新提问...\n",
      "第 1603 个数据未匹配到足够的内容，重新提问...\n",
      "第 1605 个数据未匹配到足够的内容，重新提问...\n",
      "第 1607 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 202...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1608 个数据未匹配到足够的内容，重新提问...\n",
      "第 1609 个数据未匹配到足够的内容，重新提问...\n",
      "第 1610 个数据未匹配到足够的内容，重新提问...\n",
      "第 1611 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 203...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1616 个数据未匹配到足够的内容，重新提问...\n",
      "第 1619 个数据未匹配到足够的内容，重新提问...\n",
      "第 1621 个数据未匹配到足够的内容，重新提问...\n",
      "第 1622 个数据未匹配到足够的内容，重新提问...\n",
      "第 1623 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 204...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1627 个数据未匹配到足够的内容，重新提问...\n",
      "第 1628 个数据未匹配到足够的内容，重新提问...\n",
      "第 1629 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 205...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1632 个数据未匹配到足够的内容，重新提问...\n",
      "第 1633 个数据未匹配到足够的内容，重新提问...\n",
      "第 1634 个数据未匹配到足够的内容，重新提问...\n",
      "第 1635 个数据未匹配到足够的内容，重新提问...\n",
      "第 1636 个数据未匹配到足够的内容，重新提问...\n",
      "第 1639 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 206...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1641 个数据未匹配到足够的内容，重新提问...\n",
      "第 1643 个数据未匹配到足够的内容，重新提问...\n",
      "第 1644 个数据未匹配到足够的内容，重新提问...\n",
      "第 1645 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 207...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1649 个数据未匹配到足够的内容，重新提问...\n",
      "第 1650 个数据未匹配到足够的内容，重新提问...\n",
      "第 1653 个数据未匹配到足够的内容，重新提问...\n",
      "第 1654 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 208...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1658 个数据未匹配到足够的内容，重新提问...\n",
      "第 1660 个数据未匹配到足够的内容，重新提问...\n",
      "第 1663 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 209...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1664 个数据未匹配到足够的内容，重新提问...\n",
      "第 1665 个数据未匹配到足够的内容，重新提问...\n",
      "第 1667 个数据未匹配到足够的内容，重新提问...\n",
      "第 1669 个数据未匹配到足够的内容，重新提问...\n",
      "第 1671 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 210...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1673 个数据未匹配到足够的内容，重新提问...\n",
      "第 1674 个数据未匹配到足够的内容，重新提问...\n",
      "第 1675 个数据未匹配到足够的内容，重新提问...\n",
      "第 1676 个数据未匹配到足够的内容，重新提问...\n",
      "第 1677 个数据未匹配到足够的内容，重新提问...\n",
      "第 1678 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 211...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1680 个数据未匹配到足够的内容，重新提问...\n",
      "第 1682 个数据未匹配到足够的内容，重新提问...\n",
      "第 1683 个数据未匹配到足够的内容，重新提问...\n",
      "第 1685 个数据未匹配到足够的内容，重新提问...\n",
      "第 1686 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 212...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1688 个数据未匹配到足够的内容，重新提问...\n",
      "第 1689 个数据未匹配到足够的内容，重新提问...\n",
      "第 1690 个数据未匹配到足够的内容，重新提问...\n",
      "第 1691 个数据未匹配到足够的内容，重新提问...\n",
      "第 1692 个数据未匹配到足够的内容，重新提问...\n",
      "第 1693 个数据未匹配到足够的内容，重新提问...\n",
      "第 1694 个数据未匹配到足够的内容，重新提问...\n",
      "第 1695 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 213...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1698 个数据未匹配到足够的内容，重新提问...\n",
      "第 1699 个数据未匹配到足够的内容，重新提问...\n",
      "第 1700 个数据未匹配到足够的内容，重新提问...\n",
      "第 1701 个数据未匹配到足够的内容，重新提问...\n",
      "第 1702 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 214...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1704 个数据未匹配到足够的内容，重新提问...\n",
      "第 1705 个数据未匹配到足够的内容，重新提问...\n",
      "第 1706 个数据未匹配到足够的内容，重新提问...\n",
      "第 1707 个数据未匹配到足够的内容，重新提问...\n",
      "第 1708 个数据未匹配到足够的内容，重新提问...\n",
      "第 1709 个数据未匹配到足够的内容，重新提问...\n",
      "第 1710 个数据未匹配到足够的内容，重新提问...\n",
      "第 1711 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 215...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1712 个数据未匹配到足够的内容，重新提问...\n",
      "第 1713 个数据未匹配到足够的内容，重新提问...\n",
      "第 1714 个数据未匹配到足够的内容，重新提问...\n",
      "第 1715 个数据未匹配到足够的内容，重新提问...\n",
      "第 1716 个数据未匹配到足够的内容，重新提问...\n",
      "第 1718 个数据未匹配到足够的内容，重新提问...\n",
      "第 1719 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 216...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1721 个数据未匹配到足够的内容，重新提问...\n",
      "第 1722 个数据未匹配到足够的内容，重新提问...\n",
      "第 1723 个数据未匹配到足够的内容，重新提问...\n",
      "第 1724 个数据未匹配到足够的内容，重新提问...\n",
      "第 1726 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 217...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1728 个数据未匹配到足够的内容，重新提问...\n",
      "第 1729 个数据未匹配到足够的内容，重新提问...\n",
      "第 1730 个数据未匹配到足够的内容，重新提问...\n",
      "第 1732 个数据未匹配到足够的内容，重新提问...\n",
      "第 1733 个数据未匹配到足够的内容，重新提问...\n",
      "第 1734 个数据未匹配到足够的内容，重新提问...\n",
      "第 1735 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 218...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1736 个数据未匹配到足够的内容，重新提问...\n",
      "第 1740 个数据未匹配到足够的内容，重新提问...\n",
      "第 1741 个数据未匹配到足够的内容，重新提问...\n",
      "第 1742 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 219...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1744 个数据未匹配到足够的内容，重新提问...\n",
      "第 1745 个数据未匹配到足够的内容，重新提问...\n",
      "第 1747 个数据未匹配到足够的内容，重新提问...\n",
      "第 1748 个数据未匹配到足够的内容，重新提问...\n",
      "第 1749 个数据未匹配到足够的内容，重新提问...\n",
      "第 1750 个数据未匹配到足够的内容，重新提问...\n",
      "第 1751 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 220...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1752 个数据未匹配到足够的内容，重新提问...\n",
      "第 1755 个数据未匹配到足够的内容，重新提问...\n",
      "第 1757 个数据未匹配到足够的内容，重新提问...\n",
      "第 1758 个数据未匹配到足够的内容，重新提问...\n",
      "第 1759 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 221...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1760 个数据未匹配到足够的内容，重新提问...\n",
      "第 1761 个数据未匹配到足够的内容，重新提问...\n",
      "第 1762 个数据未匹配到足够的内容，重新提问...\n",
      "第 1763 个数据未匹配到足够的内容，重新提问...\n",
      "第 1764 个数据未匹配到足够的内容，重新提问...\n",
      "第 1765 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 222...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1768 个数据未匹配到足够的内容，重新提问...\n",
      "第 1769 个数据未匹配到足够的内容，重新提问...\n",
      "第 1774 个数据未匹配到足够的内容，重新提问...\n",
      "第 1775 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 223...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1776 个数据未匹配到足够的内容，重新提问...\n",
      "第 1778 个数据未匹配到足够的内容，重新提问...\n",
      "第 1779 个数据未匹配到足够的内容，重新提问...\n",
      "第 1780 个数据未匹配到足够的内容，重新提问...\n",
      "第 1781 个数据未匹配到足够的内容，重新提问...\n",
      "第 1782 个数据未匹配到足够的内容，重新提问...\n",
      "第 1783 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 224...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1784 个数据未匹配到足够的内容，重新提问...\n",
      "第 1787 个数据未匹配到足够的内容，重新提问...\n",
      "第 1791 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 225...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1793 个数据未匹配到足够的内容，重新提问...\n",
      "第 1794 个数据未匹配到足够的内容，重新提问...\n",
      "第 1795 个数据未匹配到足够的内容，重新提问...\n",
      "第 1797 个数据未匹配到足够的内容，重新提问...\n",
      "第 1798 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 226...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1800 个数据未匹配到足够的内容，重新提问...\n",
      "第 1801 个数据未匹配到足够的内容，重新提问...\n",
      "第 1802 个数据未匹配到足够的内容，重新提问...\n",
      "第 1803 个数据未匹配到足够的内容，重新提问...\n",
      "第 1804 个数据未匹配到足够的内容，重新提问...\n",
      "第 1805 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 227...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1809 个数据未匹配到足够的内容，重新提问...\n",
      "第 1810 个数据未匹配到足够的内容，重新提问...\n",
      "第 1813 个数据未匹配到足够的内容，重新提问...\n",
      "第 1815 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 228...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1816 个数据未匹配到足够的内容，重新提问...\n",
      "第 1817 个数据未匹配到足够的内容，重新提问...\n",
      "第 1818 个数据未匹配到足够的内容，重新提问...\n",
      "第 1821 个数据未匹配到足够的内容，重新提问...\n",
      "第 1822 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 229...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1824 个数据未匹配到足够的内容，重新提问...\n",
      "第 1825 个数据未匹配到足够的内容，重新提问...\n",
      "第 1826 个数据未匹配到足够的内容，重新提问...\n",
      "第 1828 个数据未匹配到足够的内容，重新提问...\n",
      "第 1829 个数据未匹配到足够的内容，重新提问...\n",
      "第 1830 个数据未匹配到足够的内容，重新提问...\n",
      "第 1831 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 230...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1832 个数据未匹配到足够的内容，重新提问...\n",
      "第 1834 个数据未匹配到足够的内容，重新提问...\n",
      "第 1835 个数据未匹配到足够的内容，重新提问...\n",
      "第 1837 个数据未匹配到足够的内容，重新提问...\n",
      "第 1838 个数据未匹配到足够的内容，重新提问...\n",
      "第 1839 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 231...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1841 个数据未匹配到足够的内容，重新提问...\n",
      "第 1842 个数据未匹配到足够的内容，重新提问...\n",
      "第 1843 个数据未匹配到足够的内容，重新提问...\n",
      "第 1844 个数据未匹配到足够的内容，重新提问...\n",
      "第 1845 个数据未匹配到足够的内容，重新提问...\n",
      "第 1846 个数据未匹配到足够的内容，重新提问...\n",
      "第 1847 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 232...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1849 个数据未匹配到足够的内容，重新提问...\n",
      "第 1850 个数据未匹配到足够的内容，重新提问...\n",
      "第 1853 个数据未匹配到足够的内容，重新提问...\n",
      "第 1855 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 233...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1856 个数据未匹配到足够的内容，重新提问...\n",
      "第 1857 个数据未匹配到足够的内容，重新提问...\n",
      "第 1858 个数据未匹配到足够的内容，重新提问...\n",
      "第 1859 个数据未匹配到足够的内容，重新提问...\n",
      "第 1860 个数据未匹配到足够的内容，重新提问...\n",
      "第 1862 个数据未匹配到足够的内容，重新提问...\n",
      "第 1863 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 234...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1865 个数据未匹配到足够的内容，重新提问...\n",
      "第 1866 个数据未匹配到足够的内容，重新提问...\n",
      "第 1868 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 235...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1873 个数据未匹配到足够的内容，重新提问...\n",
      "第 1874 个数据未匹配到足够的内容，重新提问...\n",
      "第 1876 个数据未匹配到足够的内容，重新提问...\n",
      "第 1877 个数据未匹配到足够的内容，重新提问...\n",
      "第 1878 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 236...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1883 个数据未匹配到足够的内容，重新提问...\n",
      "第 1887 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 237...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1889 个数据未匹配到足够的内容，重新提问...\n",
      "第 1890 个数据未匹配到足够的内容，重新提问...\n",
      "第 1892 个数据未匹配到足够的内容，重新提问...\n",
      "第 1893 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 238...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1896 个数据未匹配到足够的内容，重新提问...\n",
      "第 1897 个数据未匹配到足够的内容，重新提问...\n",
      "第 1899 个数据未匹配到足够的内容，重新提问...\n",
      "第 1901 个数据未匹配到足够的内容，重新提问...\n",
      "第 1902 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 239...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1904 个数据未匹配到足够的内容，重新提问...\n",
      "第 1905 个数据未匹配到足够的内容，重新提问...\n",
      "第 1906 个数据未匹配到足够的内容，重新提问...\n",
      "第 1909 个数据未匹配到足够的内容，重新提问...\n",
      "第 1910 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 240...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1912 个数据未匹配到足够的内容，重新提问...\n",
      "第 1915 个数据未匹配到足够的内容，重新提问...\n",
      "第 1916 个数据未匹配到足够的内容，重新提问...\n",
      "第 1918 个数据未匹配到足够的内容，重新提问...\n",
      "第 1919 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 241...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1924 个数据未匹配到足够的内容，重新提问...\n",
      "第 1926 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 242...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1930 个数据未匹配到足够的内容，重新提问...\n",
      "第 1932 个数据未匹配到足够的内容，重新提问...\n",
      "第 1933 个数据未匹配到足够的内容，重新提问...\n",
      "第 1934 个数据未匹配到足够的内容，重新提问...\n",
      "第 1935 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 243...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1937 个数据未匹配到足够的内容，重新提问...\n",
      "第 1938 个数据未匹配到足够的内容，重新提问...\n",
      "第 1939 个数据未匹配到足够的内容，重新提问...\n",
      "第 1940 个数据未匹配到足够的内容，重新提问...\n",
      "第 1941 个数据未匹配到足够的内容，重新提问...\n",
      "第 1942 个数据未匹配到足够的内容，重新提问...\n",
      "第 1943 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 244...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1944 个数据未匹配到足够的内容，重新提问...\n",
      "第 1947 个数据未匹配到足够的内容，重新提问...\n",
      "第 1948 个数据未匹配到足够的内容，重新提问...\n",
      "第 1950 个数据未匹配到足够的内容，重新提问...\n",
      "第 1951 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 245...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1954 个数据未匹配到足够的内容，重新提问...\n",
      "第 1957 个数据未匹配到足够的内容，重新提问...\n",
      "第 1958 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 246...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1961 个数据未匹配到足够的内容，重新提问...\n",
      "第 1962 个数据未匹配到足够的内容，重新提问...\n",
      "第 1965 个数据未匹配到足够的内容，重新提问...\n",
      "第 1966 个数据未匹配到足够的内容，重新提问...\n",
      "第 1967 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 247...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1969 个数据未匹配到足够的内容，重新提问...\n",
      "第 1971 个数据未匹配到足够的内容，重新提问...\n",
      "第 1972 个数据未匹配到足够的内容，重新提问...\n",
      "第 1973 个数据未匹配到足够的内容，重新提问...\n",
      "第 1975 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 248...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1977 个数据未匹配到足够的内容，重新提问...\n",
      "第 1980 个数据未匹配到足够的内容，重新提问...\n",
      "第 1982 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 249...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1984 个数据未匹配到足够的内容，重新提问...\n",
      "第 1985 个数据未匹配到足够的内容，重新提问...\n",
      "第 1987 个数据未匹配到足够的内容，重新提问...\n",
      "第 1988 个数据未匹配到足够的内容，重新提问...\n",
      "第 1989 个数据未匹配到足够的内容，重新提问...\n",
      "第 1990 个数据未匹配到足够的内容，重新提问...\n",
      "第 1991 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 250...\n",
      "第 1992 个数据未匹配到足够的内容，重新提问...\n",
      "第 1993 个数据未匹配到足够的内容，重新提问...\n",
      "第 1994 个数据未匹配到足够的内容，重新提问...\n",
      "第 1997 个数据未匹配到足够的内容，重新提问...\n",
      "第 1998 个数据未匹配到足够的内容，重新提问...\n",
      "第 1999 个数据未匹配到足够的内容，重新提问...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "prompt_task = '''我需要你做中文概括任务，以下是一个示例\n",
    "原句 : 24岁 的 男子 小罗 ( 化名 ) 因 患有 精神 病 被 送进 医院 , 为 防止 他 伤害 自己 , 医院 用 保护 带 将 其 绑 在床上 。第二天 , 护士 发现 小罗 被 同室 病人 掐死 。\", \"为此 , 小罗 母亲 提起 刑事 自诉 , 要求 追究 当班 护士 的 刑事责任 。\", \"法 晚 记者 上午 获悉 , 市 一中 院 终审 判处 当班 护士 杨某 犯 医疗 事故 罪 , 但 由于 医院 已 与 家属 达成 协议 , 赔偿 了 50万 元 , 且 杨某 已 被 医院 开除 , 故 对 其 免予 刑事 处罚 。\", \"住院 当晚 被 同室 病人 掐死 1999年 11月 29 日 , 24岁 的 被害人 小罗 因 精神 受到 刺激 被 送往 本市 海淀区 北京大学 第六 医院 治疗 。\", \"经 诊断 系 “ 急性 精神病 症 ” , 医嘱 为 精神科 特护 ( 防 冲动 、 巡视 )\", \"当晚 , 因 小罗 拒绝 服药 打针 , 医院 用 保护 带 将 其 绑 在床上 , 后 又 因其 吵闹 , 医师 给 其 注射 了 安定 10mg 。\", \"次日 早晨 6 点 10分 , 小罗 被 发现 已 死亡 。\", \"海淀 法院 一审 判决书 查明 , 1999年 11月 30 日 , 杨某 在 本市 海淀区 北京 医科 大学 附属 第六 医院 住院部 , 作为 大夜班 副 班 护士 , 未 严格 履行 巡视 职责 , 对 已 约束 的 病人 未 按 规定 定时 松解 保护 带 。\", \"导致 小罗 在 杨某 值班 期间 , 被 同室 39 岁 的 精神病人 唐某 扼 颈 机械 性窒息 死亡 。\", \"经 司法 精神病 鉴定 , 唐某 实施 违法行为 时 丧失 辨认 、 控制 能力 , 无责任 能力 。\", \"2000年 1 月 19日 , 北京 医科 大学 附属 第六 医院 与 被害人 家属 达成 协议 , 赔偿 被害人 家属 50万 元 并 承担 其他 损失 费用 。\", \"医院 对 护士 杨某 和 郝某 予以 行政 处分 , 决定 对 杨某 给予 开除 行政 处分 。\", \"当班 护士 认为 “ 应 由 医院 担责 ” 杨某 供述 称 , 1999年 11月 29 日 下午 1 时 许 , 护士 站 急 收 了 病人 小罗 , 有 3 名 家属 护送 。\", \"当时 , 小罗 比较 兴奋 吵闹 , 杨某 就 腾出 一个 单间 安排 其 入住 , 并 对 小罗 采取 了 保护 措施 , 用 绑带 把 他 绑 在 病床 上 。\", \"把 门锁 上后 , 杨某 于 当日 下午 3 时 就 下班 了 。\", \"晚上 11点 多 , 杨某 到 单位 上夜班 。\", \"到了 次日 凌晨 1点 30 分 , 他 和 另外 一 名 实习 护士 郝某 与 前 班 护士 办理 交接 。\", \"杨某 首先 隔着 门 玻璃 查看 清点 了 一下 人数 , 郝某 清点 医用 物品 , 后 一起 听取 了 前 一班 护士 的 交班 报告 。\", \"在 清点 人数 时 , 杨某 发现 小罗 的 病房 里 又新 进 了 一个 病人 唐某 。\", \"当晚 , 杨某 在 交接班 之后 , 只 在 凌晨 2 点 左右 巡视 过 1 次 , 只是 清点 了 人数 , 没有 进 病房 查看 , 之后 就 再 没有 巡视 查看 过 。\", \"次日 早上 6 点 多钟 , 郝某 找到 杨某 说 小罗 有些 不对劲 。\", \"杨某 到 小罗 床 前 掀开 被子 , 发现 小罗 还 被 捆着 , 身体 已经 僵 了 。\", \"杨某 认为 自己 虽有 过错 , 但 尚 不 构成 犯罪 , 受害人 小罗 的 死亡 结果 应当 由 医院 承担 责任 。\", \"杨某 辩护人 认为 , 受害人 死亡 的 原因 是 多 方面 的 , 既有 唐某 的 直接 原因 , 也 有 医院 管理 混乱 , 工作 交接 脱节 , 上 一班 医护 人员 错误 安排 病房 、 没有 及时 松解 保护 带 等 方面 原因 。\", \"杨某 未 按 规定 巡视 并不 必然 导致 被害人 小罗 死亡 的 结果 发生 。\", \"家属 向 护士 提起 刑事 自诉 郝某 称 , 按规定 , 应该 对 病人 每隔 10分钟 巡视 1 次 , 对 小罗 这样 的 病人 应该 重点 观察 。\", \"此外 , 对于 小罗 这样 有 被 约束 病人 的 病房 里 , 是 不能 再 入住 没有 被 约束 的 病人 的 。\", \"医院 出具 的 特护 记录 显示 , 杨某 的 前 一班 护士 对 小罗 都 有 详细 且 明确 的 护理 记录 , 但 从 当晚 1 点 半 杨某 接班 以后 , 就 没有 任何 关于 对 小罗 的 护理 记录 , 杨某 一 晚上 都 没有 对 小罗 进行 过 巡查 , 也 没有 按规定 护理 。\", \"2006年 , 北京 医学会 、 中华 医学会 两次 进行 医疗 事故 鉴定 , 结论 均 为 : 本 例 医疗 事故 争议 属于 一级 甲等 医疗 事故 , 医院 在 患者 的 损伤 结果 中 承担 次要 责任 。\", \"检察机关 认定 被告人 杨某 涉嫌 医疗 事故 罪 , 但 情节 轻微 , 决定 对 被告人 杨某 不起诉 。\", \"为此 , 小罗 的 母亲 赵某 向 海淀 法院 提起 刑事 自诉 。\", \"构成 医疗 事故 罪 免予 刑事 处罚 海淀 法院 认为 , 多项 证据 显示 , 虽然 被害人 小罗 的 死亡 是 由 第三人 病态 行为 直接 造成 , 但 该 死亡 结果 是 在 被害人 就诊 期间 发生 , 由于 医院 管理 不当 和 严重 失职 所 导致 的 。\", \"杨某 身为 当晚 值班 副 班 护士 , 是 事发 当时 代表 医院 承担 巡视 、 护理 职责 , 其 严重 不 负责 的 行为 是 医院 没有 履行 好 保护 患者 安全 职责 的 表现 之一 , 与 该 死亡 结果 存在 着 重要 的 因果关系 , 应当 以 医疗 事故 罪 追究 其 刑事责任 。\", \"鉴于 此次 医疗 事故 系 由 多 原因 所 造成 , 医院 多 方面 的 过失 并非 仅限于 杨某 单个 因素 。\", \"故 杨某 虽 构成 犯罪 , 但 情节 相对 轻微 , 且 系 初犯 , 到案 后 能 如实 供认 其 失职 行为 , 具有 悔罪 表现 , 同时 事后 杨某 已 被 医院 处以 开除 的 行政 处分 , 受到 相应 惩罚 , 已 无 判处 刑罚 之 必要 , 故 法院 认为 可以 对 其 免予 刑事 处罚 。\", \"据此 , 海淀 法院 作出 一审 判决 , 杨某 犯 医疗 事故 罪 , 免予 刑事 处罚 。\", \"一审 判决 后 , 小罗 家属 提起 上诉 , 认为 原判 对 杨某 的 处罚 过轻 。\", \"杨某 也 提起 了 上诉 , 杨某 认为 被害人 的 死亡 后果 并非 其 造成 的 , 其 不 构成 犯罪 。\", \"市 一中 院 审理 后 , 驳回 杨某 和 赵 女士 的 上诉 , 维持 原判 。\"\n",
    "概括 : 北京 精神病人 被 用 束缚 带 绑 床上 , 遭 病友 掐死 ; 法院 认定 医院 管理 不当 、 值班 护士 失职 , 构成 医疗 事故 罪 。<END>\n",
    "用简短的语句概括以下内容,概括的句子 <END> 结尾\n",
    "原句 :'''\n",
    "import re\n",
    "model_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "from modelscope import AutoModelForCausalLM, AutoTokenizer\n",
    "batch_size = 8  # 设置批处理大小（可以根据GPU的内存进行调整）\n",
    "all_answer = []\n",
    "all_pre = []\n",
    "\n",
    "from modelscope import Model\n",
    "from swift import Swift\n",
    "import torch\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, \n",
    "    torch_dtype=torch.bfloat16, \n",
    "    device_map='auto', \n",
    "    trust_remote_code=True\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "# 将数据分成小批次\n",
    "for i in range(0, len(Summary_set), batch_size):\n",
    "    batch_data = Summary_set[i:i+batch_size]\n",
    "    print(f\"Processing batch {i//batch_size + 1}...\")\n",
    "\n",
    "    # 准备输入数据\n",
    "    batch_q = [prompt_task + ''.join(data['article']) + \"\\n概括 :\" for data in batch_data]\n",
    "    batch_a = [data['summary'] for data in batch_data]\n",
    "    # print(batch_q)\n",
    "    # 将批量数据转化为模型输入\n",
    "    inputs = tokenizer(batch_q, padding=True, truncation=True, return_tensors=\"pt\").to('cuda')\n",
    "    \n",
    "    # 生成输出\n",
    "    generated_ids = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=128\n",
    "    )\n",
    "\n",
    "    # 解码生成的文本\n",
    "    responses = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "   \n",
    "    \n",
    "    # 处理每个生成的文本\n",
    "    for j, response in enumerate(responses):\n",
    "        matches = re.findall(r\"概括 :(.*?)\\<END\\>\", response, re.DOTALL)\n",
    "        if len(matches) > 1:\n",
    "            all_pre.append(matches[1])\n",
    "            all_answer.append(batch_a[j])\n",
    "        else:\n",
    "            print(f\"第 {i + j} 个数据未匹配到足够的内容，重新提问...\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('predictions_test1.json', 'w') as f:\n",
    "    json.dump(all_pre, f)\n",
    "\n",
    "with open('references_test1.json', 'w') as f:\n",
    "    json.dump(all_answer, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "746\n",
      "ROUGE Scores:\n",
      "ROUGE-1: 0.0695\n",
      "ROUGE-2: 0.0099\n",
      "ROUGE-L: 0.0690\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import evaluate\n",
    "\n",
    "# 从文件中读取数据\n",
    "with open('predictions_test1.json', 'r') as f:\n",
    "    all_pre = json.load(f)\n",
    "\n",
    "with open('references_test1.json', 'r') as f:\n",
    "    all_answer = json.load(f)\n",
    "\n",
    "# 加载 ROUGE 评估器\n",
    "rouge = evaluate.load('rouge')\n",
    "\n",
    "# 确保 all_pre 和 all_answer 已包含所有预测和真实摘要\n",
    "# all_pre.append(matches[0])\n",
    "\n",
    "# 计算 ROUGE 分数\n",
    "print(len(all_answer))\n",
    "results = rouge.compute(predictions=all_pre, references=all_answer)\n",
    "\n",
    "# 输出结果\n",
    "print(\"ROUGE Scores:\")\n",
    "print(f\"ROUGE-1: {results['rouge1']:.4f}\")\n",
    "print(f\"ROUGE-2: {results['rouge2']:.4f}\")\n",
    "print(f\"ROUGE-L: {results['rougeL']:.4f}\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test 2\n",
    "**微调预训练模型**\n",
    "\n",
    "基于魔搭平台的modelscope库，使用预训练模型进行微调，并使用自定义数据集进行模型评估。\n",
    "##### 微调数据集构造"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "path = 'data\\\\test2017.simple.label.jsonl'\n",
    "prompt_task = '''\\n我需要你做中文概括任务。用简短的语句概括以下内容,概括的句子 <END> 结尾\n",
    "原句 :'''\n",
    "# 用于存储转换后的内容\n",
    "converted_data = []\n",
    "\n",
    "# 读取并转换 jsonl 文件\n",
    "with open(path, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        # 解析每一行的 JSON 对象\n",
    "        item = json.loads(line.strip())\n",
    "                    \n",
    "        # 提取所需字段\n",
    "        article = item['article']\n",
    "        artical = ''.join(article)\n",
    "        summary = item['summary']\n",
    "        summary = ''.join(summary)\n",
    "        \n",
    "        q = prompt_task + artical + \"\\n概括 :\"\n",
    "        a = \"概括 :\" + summary            \n",
    "        \n",
    "        converted_data.append({'query': q, 'answer': a})\n",
    "\n",
    "output_file_path = 'data\\CNewSum_v2\\\\final\\\\fineture.jsonl'\n",
    "# 输出转换后的内容\n",
    "with open(output_file_path, 'w', encoding='utf-8') as f:\n",
    "    for item in converted_data:\n",
    "        json.dump(item, f, ensure_ascii=False)\n",
    "        f.write('\\n')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 微调过程\n",
    "**不使用全量微调，采用LoRA微调降低对设备的要求**\n",
    "\n",
    "**数据来自CNewSum_v2的其他test集合**\n",
    "\n",
    "\n",
    "**回答格式与提示词要求符合度有提高**\n",
    "\n",
    "**最后结果提高有限，可能由于微调数据集有限，同时构建较为粗糙**\n",
    "\n",
    "\n",
    "* 微调"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-11-16T14:05:00.395863Z",
     "iopub.status.busy": "2024-11-16T14:05:00.395511Z",
     "iopub.status.idle": "2024-11-16T14:14:28.097724Z",
     "shell.execute_reply": "2024-11-16T14:14:28.097219Z",
     "shell.execute_reply.started": "2024-11-16T14:05:00.395839Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6c634720e3d4afbaf99f735b7705099",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Model to directory: /mnt/workspace/.cache/modelscope/hub/Qwen/Qwen2.5-0.5B-Instruct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:modelscope] Creating symbolic link /mnt/workspace/.cache/modelscope/hub/Qwen/Qwen2___5-0___5B-Instruct -> /mnt/workspace/.cache/modelscope/hub/Qwen/Qwen2.5-0.5B-Instruct.\n",
      "[WARNING:modelscope] Failed to create symbolic link /mnt/workspace/.cache/modelscope/hub/Qwen/Qwen2___5-0___5B-Instruct -> /mnt/workspace/.cache/modelscope/hub/Qwen/Qwen2.5-0.5B-Instruct: [Errno 17] File exists: '/mnt/workspace/.cache/modelscope/hub/Qwen/Qwen2___5-0___5B-Instruct' -> '/mnt/workspace/.cache/modelscope/hub/Qwen/Qwen2.5-0.5B-Instruct'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Model to directory: /mnt/workspace/.cache/modelscope/hub/Qwen/Qwen2.5-0.5B-Instruct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:modelscope] Creating symbolic link /mnt/workspace/.cache/modelscope/hub/Qwen/Qwen2___5-0___5B-Instruct -> /mnt/workspace/.cache/modelscope/hub/Qwen/Qwen2.5-0.5B-Instruct.\n",
      "[WARNING:modelscope] Failed to create symbolic link /mnt/workspace/.cache/modelscope/hub/Qwen/Qwen2___5-0___5B-Instruct -> /mnt/workspace/.cache/modelscope/hub/Qwen/Qwen2.5-0.5B-Instruct: [Errno 17] File exists: '/mnt/workspace/.cache/modelscope/hub/Qwen/Qwen2___5-0___5B-Instruct' -> '/mnt/workspace/.cache/modelscope/hub/Qwen/Qwen2.5-0.5B-Instruct'\n",
      "/usr/local/lib/python3.10/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/site-packages/swift/trainers/mixin.py:93: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(\n",
      "Detected kernel version 4.19.91, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfa7a9a86d574a8bb87f644134d385eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/375 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.16273766, 'acc': 0.36791363, 'grad_norm': 2.265625, 'learning_rate': 1e-05, 'memory(GiB)': 3.89, 'train_speed(iter/s)': 0.635844, 'epoch': 0.08, 'global_step/max_steps': '10/375', 'percentage': '2.67%', 'elapsed_time': '15s', 'remaining_time': '9m 17s'}\n",
      "{'loss': 2.66615562, 'acc': 0.40595317, 'grad_norm': 1.5234375, 'learning_rate': 2e-05, 'memory(GiB)': 2.74, 'train_speed(iter/s)': 0.657411, 'epoch': 0.16, 'global_step/max_steps': '20/375', 'percentage': '5.33%', 'elapsed_time': '29s', 'remaining_time': '8m 51s'}\n",
      "{'loss': 2.37051659, 'acc': 0.44810405, 'grad_norm': 1.1015625, 'learning_rate': 3e-05, 'memory(GiB)': 2.74, 'train_speed(iter/s)': 0.660203, 'epoch': 0.24, 'global_step/max_steps': '30/375', 'percentage': '8.00%', 'elapsed_time': '44s', 'remaining_time': '8m 37s'}\n",
      "{'loss': 2.3178812, 'acc': 0.4581409, 'grad_norm': 1.1875, 'learning_rate': 4e-05, 'memory(GiB)': 2.95, 'train_speed(iter/s)': 0.663176, 'epoch': 0.32, 'global_step/max_steps': '40/375', 'percentage': '10.67%', 'elapsed_time': '59s', 'remaining_time': '8m 21s'}\n",
      "{'loss': 2.21636047, 'acc': 0.48253217, 'grad_norm': 1.1171875, 'learning_rate': 5e-05, 'memory(GiB)': 2.95, 'train_speed(iter/s)': 0.662964, 'epoch': 0.4, 'global_step/max_steps': '50/375', 'percentage': '13.33%', 'elapsed_time': '1m 14s', 'remaining_time': '8m 7s'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "[INFO:swift] Saving model checkpoint to output/checkpoint-50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.26447353, 'acc': 0.47125001, 'grad_norm': 1.03125, 'learning_rate': 6e-05, 'memory(GiB)': 2.95, 'train_speed(iter/s)': 0.660237, 'epoch': 0.48, 'global_step/max_steps': '60/375', 'percentage': '16.00%', 'elapsed_time': '1m 30s', 'remaining_time': '7m 54s'}\n",
      "{'loss': 2.23509712, 'acc': 0.47491426, 'grad_norm': 1.140625, 'learning_rate': 7e-05, 'memory(GiB)': 2.95, 'train_speed(iter/s)': 0.661359, 'epoch': 0.56, 'global_step/max_steps': '70/375', 'percentage': '18.67%', 'elapsed_time': '1m 45s', 'remaining_time': '7m 39s'}\n",
      "{'loss': 2.25556984, 'acc': 0.46766953, 'grad_norm': 1.09375, 'learning_rate': 8e-05, 'memory(GiB)': 2.95, 'train_speed(iter/s)': 0.663391, 'epoch': 0.64, 'global_step/max_steps': '80/375', 'percentage': '21.33%', 'elapsed_time': '2m 0s', 'remaining_time': '7m 22s'}\n",
      "{'loss': 2.19830151, 'acc': 0.48366022, 'grad_norm': 1.2734375, 'learning_rate': 9e-05, 'memory(GiB)': 2.95, 'train_speed(iter/s)': 0.664236, 'epoch': 0.72, 'global_step/max_steps': '90/375', 'percentage': '24.00%', 'elapsed_time': '2m 15s', 'remaining_time': '7m 7s'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.22010479, 'acc': 0.48171463, 'grad_norm': 1.3359375, 'learning_rate': 0.0001, 'memory(GiB)': 2.95, 'train_speed(iter/s)': 0.665719, 'epoch': 0.8, 'global_step/max_steps': '100/375', 'percentage': '26.67%', 'elapsed_time': '2m 29s', 'remaining_time': '6m 51s'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:swift] Saving model checkpoint to output/checkpoint-100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.23189449, 'acc': 0.48216848, 'grad_norm': 1.0234375, 'learning_rate': 9.636e-05, 'memory(GiB)': 2.95, 'train_speed(iter/s)': 0.664199, 'epoch': 0.88, 'global_step/max_steps': '110/375', 'percentage': '29.33%', 'elapsed_time': '2m 45s', 'remaining_time': '6m 37s'}\n",
      "{'loss': 2.16457577, 'acc': 0.49156933, 'grad_norm': 1.0390625, 'learning_rate': 9.273e-05, 'memory(GiB)': 2.95, 'train_speed(iter/s)': 0.665198, 'epoch': 0.96, 'global_step/max_steps': '120/375', 'percentage': '32.00%', 'elapsed_time': '2m 59s', 'remaining_time': '6m 22s'}\n",
      "{'loss': 2.08620472, 'acc': 0.50530415, 'grad_norm': 0.97265625, 'learning_rate': 8.909e-05, 'memory(GiB)': 2.95, 'train_speed(iter/s)': 0.663604, 'epoch': 1.04, 'global_step/max_steps': '130/375', 'percentage': '34.67%', 'elapsed_time': '3m 15s', 'remaining_time': '6m 8s'}\n",
      "{'loss': 2.07479706, 'acc': 0.50889835, 'grad_norm': 1.125, 'learning_rate': 8.545e-05, 'memory(GiB)': 2.95, 'train_speed(iter/s)': 0.664653, 'epoch': 1.12, 'global_step/max_steps': '140/375', 'percentage': '37.33%', 'elapsed_time': '3m 30s', 'remaining_time': '5m 52s'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.93797283, 'acc': 0.52688403, 'grad_norm': 1.1328125, 'learning_rate': 8.182e-05, 'memory(GiB)': 2.95, 'train_speed(iter/s)': 0.664799, 'epoch': 1.2, 'global_step/max_steps': '150/375', 'percentage': '40.00%', 'elapsed_time': '3m 45s', 'remaining_time': '5m 37s'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:swift] Saving model checkpoint to output/checkpoint-150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.04726257, 'acc': 0.50751038, 'grad_norm': 1.125, 'learning_rate': 7.818e-05, 'memory(GiB)': 2.95, 'train_speed(iter/s)': 0.664657, 'epoch': 1.28, 'global_step/max_steps': '160/375', 'percentage': '42.67%', 'elapsed_time': '4m 0s', 'remaining_time': '5m 22s'}\n",
      "{'loss': 1.97016907, 'acc': 0.5296308, 'grad_norm': 1.09375, 'learning_rate': 7.455e-05, 'memory(GiB)': 2.95, 'train_speed(iter/s)': 0.664821, 'epoch': 1.36, 'global_step/max_steps': '170/375', 'percentage': '45.33%', 'elapsed_time': '4m 15s', 'remaining_time': '5m 7s'}\n",
      "{'loss': 2.06854839, 'acc': 0.50773869, 'grad_norm': 1.2109375, 'learning_rate': 7.091e-05, 'memory(GiB)': 2.95, 'train_speed(iter/s)': 0.665608, 'epoch': 1.44, 'global_step/max_steps': '180/375', 'percentage': '48.00%', 'elapsed_time': '4m 29s', 'remaining_time': '4m 52s'}\n",
      "{'loss': 1.97801056, 'acc': 0.51696234, 'grad_norm': 1.3046875, 'learning_rate': 6.727e-05, 'memory(GiB)': 2.95, 'train_speed(iter/s)': 0.665672, 'epoch': 1.52, 'global_step/max_steps': '190/375', 'percentage': '50.67%', 'elapsed_time': '4m 44s', 'remaining_time': '4m 37s'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.99477005, 'acc': 0.52524137, 'grad_norm': 1.1796875, 'learning_rate': 6.364e-05, 'memory(GiB)': 2.95, 'train_speed(iter/s)': 0.666304, 'epoch': 1.6, 'global_step/max_steps': '200/375', 'percentage': '53.33%', 'elapsed_time': '4m 59s', 'remaining_time': '4m 22s'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:swift] Saving model checkpoint to output/checkpoint-200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.06805267, 'acc': 0.51131949, 'grad_norm': 1.28125, 'learning_rate': 6e-05, 'memory(GiB)': 2.95, 'train_speed(iter/s)': 0.665839, 'epoch': 1.68, 'global_step/max_steps': '210/375', 'percentage': '56.00%', 'elapsed_time': '5m 14s', 'remaining_time': '4m 7s'}\n",
      "{'loss': 1.93453083, 'acc': 0.53590107, 'grad_norm': 1.2421875, 'learning_rate': 5.636e-05, 'memory(GiB)': 2.95, 'train_speed(iter/s)': 0.666412, 'epoch': 1.76, 'global_step/max_steps': '220/375', 'percentage': '58.67%', 'elapsed_time': '5m 29s', 'remaining_time': '3m 52s'}\n",
      "{'loss': 2.01098404, 'acc': 0.52221432, 'grad_norm': 1.359375, 'learning_rate': 5.273e-05, 'memory(GiB)': 2.95, 'train_speed(iter/s)': 0.66674, 'epoch': 1.84, 'global_step/max_steps': '230/375', 'percentage': '61.33%', 'elapsed_time': '5m 44s', 'remaining_time': '3m 37s'}\n",
      "{'loss': 2.007584, 'acc': 0.51915064, 'grad_norm': 1.1015625, 'learning_rate': 4.909e-05, 'memory(GiB)': 2.95, 'train_speed(iter/s)': 0.666855, 'epoch': 1.92, 'global_step/max_steps': '240/375', 'percentage': '64.00%', 'elapsed_time': '5m 59s', 'remaining_time': '3m 22s'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.01143417, 'acc': 0.52253165, 'grad_norm': 1.203125, 'learning_rate': 4.545e-05, 'memory(GiB)': 2.95, 'train_speed(iter/s)': 0.66687, 'epoch': 2.0, 'global_step/max_steps': '250/375', 'percentage': '66.67%', 'elapsed_time': '6m 14s', 'remaining_time': '3m 7s'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:swift] Saving model checkpoint to output/checkpoint-250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.80072651, 'acc': 0.56184654, 'grad_norm': 1.1015625, 'learning_rate': 4.182e-05, 'memory(GiB)': 2.95, 'train_speed(iter/s)': 0.666293, 'epoch': 2.08, 'global_step/max_steps': '260/375', 'percentage': '69.33%', 'elapsed_time': '6m 29s', 'remaining_time': '2m 52s'}\n",
      "{'loss': 1.79801025, 'acc': 0.56745129, 'grad_norm': 1.2890625, 'learning_rate': 3.818e-05, 'memory(GiB)': 2.95, 'train_speed(iter/s)': 0.666325, 'epoch': 2.16, 'global_step/max_steps': '270/375', 'percentage': '72.00%', 'elapsed_time': '6m 44s', 'remaining_time': '2m 37s'}\n",
      "{'loss': 1.79460678, 'acc': 0.55838823, 'grad_norm': 1.203125, 'learning_rate': 3.455e-05, 'memory(GiB)': 2.95, 'train_speed(iter/s)': 0.666679, 'epoch': 2.24, 'global_step/max_steps': '280/375', 'percentage': '74.67%', 'elapsed_time': '6m 59s', 'remaining_time': '2m 22s'}\n",
      "{'loss': 1.81602955, 'acc': 0.55986834, 'grad_norm': 1.1953125, 'learning_rate': 3.091e-05, 'memory(GiB)': 2.95, 'train_speed(iter/s)': 0.666871, 'epoch': 2.32, 'global_step/max_steps': '290/375', 'percentage': '77.33%', 'elapsed_time': '7m 14s', 'remaining_time': '2m 7s'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.80803165, 'acc': 0.56534204, 'grad_norm': 1.4375, 'learning_rate': 2.727e-05, 'memory(GiB)': 2.95, 'train_speed(iter/s)': 0.667114, 'epoch': 2.4, 'global_step/max_steps': '300/375', 'percentage': '80.00%', 'elapsed_time': '7m 29s', 'remaining_time': '1m 52s'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:swift] Saving model checkpoint to output/checkpoint-300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7729063, 'acc': 0.57090678, 'grad_norm': 1.1328125, 'learning_rate': 2.364e-05, 'memory(GiB)': 2.95, 'train_speed(iter/s)': 0.666537, 'epoch': 2.48, 'global_step/max_steps': '310/375', 'percentage': '82.67%', 'elapsed_time': '7m 44s', 'remaining_time': '1m 37s'}\n",
      "{'loss': 1.78933201, 'acc': 0.56588635, 'grad_norm': 1.2890625, 'learning_rate': 2e-05, 'memory(GiB)': 2.95, 'train_speed(iter/s)': 0.666757, 'epoch': 2.56, 'global_step/max_steps': '320/375', 'percentage': '85.33%', 'elapsed_time': '7m 59s', 'remaining_time': '1m 22s'}\n",
      "{'loss': 1.76110058, 'acc': 0.57279301, 'grad_norm': 1.2265625, 'learning_rate': 1.636e-05, 'memory(GiB)': 2.95, 'train_speed(iter/s)': 0.666659, 'epoch': 2.64, 'global_step/max_steps': '330/375', 'percentage': '88.00%', 'elapsed_time': '8m 14s', 'remaining_time': '1m 7s'}\n",
      "{'loss': 1.81827374, 'acc': 0.55887861, 'grad_norm': 1.4140625, 'learning_rate': 1.273e-05, 'memory(GiB)': 2.95, 'train_speed(iter/s)': 0.666953, 'epoch': 2.72, 'global_step/max_steps': '340/375', 'percentage': '90.67%', 'elapsed_time': '8m 29s', 'remaining_time': '52s'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8113409, 'acc': 0.56370974, 'grad_norm': 1.2421875, 'learning_rate': 9.09e-06, 'memory(GiB)': 2.95, 'train_speed(iter/s)': 0.667178, 'epoch': 2.8, 'global_step/max_steps': '350/375', 'percentage': '93.33%', 'elapsed_time': '8m 44s', 'remaining_time': '37s'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:swift] Saving model checkpoint to output/checkpoint-350\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.69155102, 'acc': 0.58140535, 'grad_norm': 1.28125, 'learning_rate': 5.45e-06, 'memory(GiB)': 2.95, 'train_speed(iter/s)': 0.666877, 'epoch': 2.88, 'global_step/max_steps': '360/375', 'percentage': '96.00%', 'elapsed_time': '8m 59s', 'remaining_time': '22s'}\n",
      "{'loss': 1.83493042, 'acc': 0.55252657, 'grad_norm': 1.2890625, 'learning_rate': 1.82e-06, 'memory(GiB)': 2.95, 'train_speed(iter/s)': 0.667086, 'epoch': 2.96, 'global_step/max_steps': '370/375', 'percentage': '98.67%', 'elapsed_time': '9m 14s', 'remaining_time': '7s'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "[INFO:swift] Saving model checkpoint to output/checkpoint-375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 562.2168, 'train_samples_per_second': 10.672, 'train_steps_per_second': 0.667, 'train_loss': 2.05023495, 'epoch': 3.0, 'global_step/max_steps': '375/375', 'percentage': '100.00%', 'elapsed_time': '9m 22s', 'remaining_time': '0s'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=375, training_loss=2.0502349497477215, metrics={'train_runtime': 562.2168, 'train_samples_per_second': 10.672, 'train_steps_per_second': 0.667, 'total_flos': 1078855936012800.0, 'train_loss': 2.05023495, 'epoch': 3.0, 'global_step/max_steps': '375/375', 'percentage': '100.00%', 'elapsed_time': '9m 22s', 'remaining_time': '0s'})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from modelscope import AutoModelForCausalLM, AutoTokenizer, MsDataset\n",
    "from swift import Seq2SeqTrainer, Seq2SeqTrainingArguments, Swift, LoRAConfig\n",
    "from swift.llm import get_template, TemplateType\n",
    "import json\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "model_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "file_path = 'fineture.jsonl'\n",
    "\n",
    "# 数据加载函数\n",
    "def load_jsonl(file_path):\n",
    "    data = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                record = json.loads(line.strip())\n",
    "                data.append(record)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Skipping invalid line: {line}, error: {e}\")\n",
    "    return data\n",
    "\n",
    "def save_as_csv(data, csv_file_path):\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv(csv_file_path, index=False, encoding='utf-8')\n",
    "\n",
    "def create_msdataset_from_csv(csv_file_path):\n",
    "    return MsDataset.load(csv_file_path)\n",
    "\n",
    "# 数据预处理\n",
    "csv_file_path = 'output.csv'\n",
    "data = load_jsonl(file_path)\n",
    "save_as_csv(data, csv_file_path)\n",
    "dataset = create_msdataset_from_csv(csv_file_path)\n",
    "\n",
    "# 加载模型和 tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, \n",
    "    torch_dtype=torch.bfloat16, \n",
    "    device_map='auto', \n",
    "    trust_remote_code=True\n",
    ")\n",
    "lora_config = LoRAConfig(\n",
    "    r=16,\n",
    "    target_modules=['q_proj', 'k_proj', 'v_proj', 'o_proj'],\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05\n",
    ")\n",
    "model = Swift.prepare_model(model, lora_config)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "# 模板设置\n",
    "template = get_template(TemplateType.chatglm3, tokenizer, max_length=1024)\n",
    "\n",
    "# 数据编码函数\n",
    "def encode(example):\n",
    "    question = example.get('question', '')\n",
    "    answer = example.get('answer', '')\n",
    "    if not answer:\n",
    "        return None\n",
    "    example, kwargs = template.encode({'query': question, 'response': answer})\n",
    "    return example\n",
    "\n",
    "# 手动映射数据\n",
    "encoded_data = []\n",
    "for record in dataset:\n",
    "    try:\n",
    "        encoded_example = encode(record)\n",
    "        if encoded_example:  # 确保不为空\n",
    "            encoded_data.append(encoded_example)\n",
    "    except Exception as e:\n",
    "        print(f\"Error encoding record {record}: {e}\")\n",
    "\n",
    "# 保存处理后的数据为新的 CSV\n",
    "processed_csv_file = 'processed_dataset.csv'\n",
    "pd.DataFrame(encoded_data).to_csv(processed_csv_file, index=False, encoding='utf-8')\n",
    "\n",
    "# 从新的 CSV 文件加载 MsDataset\n",
    "\n",
    "# 数据集划分\n",
    "# dataset_split = encoded_dataset.train_test_split(test_size=0.001)\n",
    "# train_dataset, val_dataset = dataset_split['train'], dataset_split['test']\n",
    "\n",
    "# 训练参数\n",
    "train_args = Seq2SeqTrainingArguments(\n",
    "    output_dir='output',\n",
    "    learning_rate=1e-4,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=16,\n",
    "    eval_steps=50,\n",
    "    save_steps=50,\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy='no',\n",
    "    save_strategy='steps',\n",
    "    save_total_limit=2,\n",
    "    dataloader_num_workers=4,\n",
    "    warmup_steps=100\n",
    ")\n",
    "\n",
    "# 创建 Trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=train_args,\n",
    "    data_collator=template.data_collator,\n",
    "    train_dataset=encoded_data,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# 开始训练\n",
    "trainer.train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-11-16T14:18:17.059905Z",
     "iopub.status.busy": "2024-11-16T14:18:17.059562Z",
     "iopub.status.idle": "2024-11-16T14:18:17.144795Z",
     "shell.execute_reply": "2024-11-16T14:18:17.144304Z",
     "shell.execute_reply.started": "2024-11-16T14:18:17.059883Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_directory = 'modified_model'\n",
    "model.save_pretrained(output_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-11-16T14:29:05.935057Z",
     "iopub.status.busy": "2024-11-16T14:29:05.934542Z",
     "iopub.status.idle": "2024-11-16T14:49:09.889022Z",
     "shell.execute_reply": "2024-11-16T14:49:09.888008Z",
     "shell.execute_reply.started": "2024-11-16T14:29:05.935037Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Model to directory: /mnt/workspace/.cache/modelscope/hub/Qwen/Qwen2.5-0.5B-Instruct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:modelscope] Creating symbolic link /mnt/workspace/.cache/modelscope/hub/Qwen/Qwen2___5-0___5B-Instruct -> /mnt/workspace/.cache/modelscope/hub/Qwen/Qwen2.5-0.5B-Instruct.\n",
      "[WARNING:modelscope] Failed to create symbolic link /mnt/workspace/.cache/modelscope/hub/Qwen/Qwen2___5-0___5B-Instruct -> /mnt/workspace/.cache/modelscope/hub/Qwen/Qwen2.5-0.5B-Instruct: [Errno 17] File exists: '/mnt/workspace/.cache/modelscope/hub/Qwen/Qwen2___5-0___5B-Instruct' -> '/mnt/workspace/.cache/modelscope/hub/Qwen/Qwen2.5-0.5B-Instruct'\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 0 个数据未匹配到足够的内容，重新提问...\n",
      "第 2 个数据未匹配到足够的内容，重新提问...\n",
      "第 3 个数据未匹配到足够的内容，重新提问...\n",
      "第 4 个数据未匹配到足够的内容，重新提问...\n",
      "第 5 个数据未匹配到足够的内容，重新提问...\n",
      "第 6 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 9 个数据未匹配到足够的内容，重新提问...\n",
      "第 10 个数据未匹配到足够的内容，重新提问...\n",
      "第 11 个数据未匹配到足够的内容，重新提问...\n",
      "第 12 个数据未匹配到足够的内容，重新提问...\n",
      "第 15 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 16 个数据未匹配到足够的内容，重新提问...\n",
      "第 20 个数据未匹配到足够的内容，重新提问...\n",
      "第 21 个数据未匹配到足够的内容，重新提问...\n",
      "第 23 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 25 个数据未匹配到足够的内容，重新提问...\n",
      "第 27 个数据未匹配到足够的内容，重新提问...\n",
      "第 28 个数据未匹配到足够的内容，重新提问...\n",
      "第 29 个数据未匹配到足够的内容，重新提问...\n",
      "第 30 个数据未匹配到足够的内容，重新提问...\n",
      "第 31 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 33 个数据未匹配到足够的内容，重新提问...\n",
      "第 35 个数据未匹配到足够的内容，重新提问...\n",
      "第 36 个数据未匹配到足够的内容，重新提问...\n",
      "第 37 个数据未匹配到足够的内容，重新提问...\n",
      "第 39 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 6...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 40 个数据未匹配到足够的内容，重新提问...\n",
      "第 42 个数据未匹配到足够的内容，重新提问...\n",
      "第 43 个数据未匹配到足够的内容，重新提问...\n",
      "第 44 个数据未匹配到足够的内容，重新提问...\n",
      "第 46 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 7...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 49 个数据未匹配到足够的内容，重新提问...\n",
      "第 50 个数据未匹配到足够的内容，重新提问...\n",
      "第 52 个数据未匹配到足够的内容，重新提问...\n",
      "第 53 个数据未匹配到足够的内容，重新提问...\n",
      "第 55 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 8...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 56 个数据未匹配到足够的内容，重新提问...\n",
      "第 58 个数据未匹配到足够的内容，重新提问...\n",
      "第 59 个数据未匹配到足够的内容，重新提问...\n",
      "第 60 个数据未匹配到足够的内容，重新提问...\n",
      "第 61 个数据未匹配到足够的内容，重新提问...\n",
      "第 62 个数据未匹配到足够的内容，重新提问...\n",
      "第 63 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 9...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 64 个数据未匹配到足够的内容，重新提问...\n",
      "第 65 个数据未匹配到足够的内容，重新提问...\n",
      "第 66 个数据未匹配到足够的内容，重新提问...\n",
      "第 67 个数据未匹配到足够的内容，重新提问...\n",
      "第 68 个数据未匹配到足够的内容，重新提问...\n",
      "第 69 个数据未匹配到足够的内容，重新提问...\n",
      "第 70 个数据未匹配到足够的内容，重新提问...\n",
      "第 71 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 10...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 74 个数据未匹配到足够的内容，重新提问...\n",
      "第 76 个数据未匹配到足够的内容，重新提问...\n",
      "第 78 个数据未匹配到足够的内容，重新提问...\n",
      "第 79 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 11...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 81 个数据未匹配到足够的内容，重新提问...\n",
      "第 82 个数据未匹配到足够的内容，重新提问...\n",
      "第 83 个数据未匹配到足够的内容，重新提问...\n",
      "第 84 个数据未匹配到足够的内容，重新提问...\n",
      "第 86 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 12...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 88 个数据未匹配到足够的内容，重新提问...\n",
      "第 89 个数据未匹配到足够的内容，重新提问...\n",
      "第 90 个数据未匹配到足够的内容，重新提问...\n",
      "第 91 个数据未匹配到足够的内容，重新提问...\n",
      "第 92 个数据未匹配到足够的内容，重新提问...\n",
      "第 94 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 13...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 96 个数据未匹配到足够的内容，重新提问...\n",
      "第 97 个数据未匹配到足够的内容，重新提问...\n",
      "第 100 个数据未匹配到足够的内容，重新提问...\n",
      "第 103 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 14...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 105 个数据未匹配到足够的内容，重新提问...\n",
      "第 106 个数据未匹配到足够的内容，重新提问...\n",
      "第 107 个数据未匹配到足够的内容，重新提问...\n",
      "第 108 个数据未匹配到足够的内容，重新提问...\n",
      "第 111 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 15...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 112 个数据未匹配到足够的内容，重新提问...\n",
      "第 115 个数据未匹配到足够的内容，重新提问...\n",
      "第 116 个数据未匹配到足够的内容，重新提问...\n",
      "第 117 个数据未匹配到足够的内容，重新提问...\n",
      "第 118 个数据未匹配到足够的内容，重新提问...\n",
      "第 119 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 16...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 120 个数据未匹配到足够的内容，重新提问...\n",
      "第 121 个数据未匹配到足够的内容，重新提问...\n",
      "第 122 个数据未匹配到足够的内容，重新提问...\n",
      "第 123 个数据未匹配到足够的内容，重新提问...\n",
      "第 125 个数据未匹配到足够的内容，重新提问...\n",
      "第 126 个数据未匹配到足够的内容，重新提问...\n",
      "第 127 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 17...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 129 个数据未匹配到足够的内容，重新提问...\n",
      "第 131 个数据未匹配到足够的内容，重新提问...\n",
      "第 132 个数据未匹配到足够的内容，重新提问...\n",
      "第 135 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 18...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 136 个数据未匹配到足够的内容，重新提问...\n",
      "第 137 个数据未匹配到足够的内容，重新提问...\n",
      "第 143 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 19...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 146 个数据未匹配到足够的内容，重新提问...\n",
      "第 148 个数据未匹配到足够的内容，重新提问...\n",
      "第 149 个数据未匹配到足够的内容，重新提问...\n",
      "第 151 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 20...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 152 个数据未匹配到足够的内容，重新提问...\n",
      "第 153 个数据未匹配到足够的内容，重新提问...\n",
      "第 157 个数据未匹配到足够的内容，重新提问...\n",
      "第 158 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 21...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 160 个数据未匹配到足够的内容，重新提问...\n",
      "第 161 个数据未匹配到足够的内容，重新提问...\n",
      "第 162 个数据未匹配到足够的内容，重新提问...\n",
      "第 163 个数据未匹配到足够的内容，重新提问...\n",
      "第 165 个数据未匹配到足够的内容，重新提问...\n",
      "第 167 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 22...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 168 个数据未匹配到足够的内容，重新提问...\n",
      "第 169 个数据未匹配到足够的内容，重新提问...\n",
      "第 171 个数据未匹配到足够的内容，重新提问...\n",
      "第 173 个数据未匹配到足够的内容，重新提问...\n",
      "第 174 个数据未匹配到足够的内容，重新提问...\n",
      "第 175 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 23...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 178 个数据未匹配到足够的内容，重新提问...\n",
      "第 179 个数据未匹配到足够的内容，重新提问...\n",
      "第 181 个数据未匹配到足够的内容，重新提问...\n",
      "第 182 个数据未匹配到足够的内容，重新提问...\n",
      "第 183 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 24...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 184 个数据未匹配到足够的内容，重新提问...\n",
      "第 187 个数据未匹配到足够的内容，重新提问...\n",
      "第 188 个数据未匹配到足够的内容，重新提问...\n",
      "第 189 个数据未匹配到足够的内容，重新提问...\n",
      "第 190 个数据未匹配到足够的内容，重新提问...\n",
      "第 191 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 25...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 192 个数据未匹配到足够的内容，重新提问...\n",
      "第 194 个数据未匹配到足够的内容，重新提问...\n",
      "第 195 个数据未匹配到足够的内容，重新提问...\n",
      "第 197 个数据未匹配到足够的内容，重新提问...\n",
      "第 198 个数据未匹配到足够的内容，重新提问...\n",
      "第 199 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 26...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 202 个数据未匹配到足够的内容，重新提问...\n",
      "第 203 个数据未匹配到足够的内容，重新提问...\n",
      "第 204 个数据未匹配到足够的内容，重新提问...\n",
      "第 206 个数据未匹配到足够的内容，重新提问...\n",
      "第 207 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 27...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 212 个数据未匹配到足够的内容，重新提问...\n",
      "第 213 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 28...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 216 个数据未匹配到足够的内容，重新提问...\n",
      "第 217 个数据未匹配到足够的内容，重新提问...\n",
      "第 220 个数据未匹配到足够的内容，重新提问...\n",
      "第 221 个数据未匹配到足够的内容，重新提问...\n",
      "第 223 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 29...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 224 个数据未匹配到足够的内容，重新提问...\n",
      "第 225 个数据未匹配到足够的内容，重新提问...\n",
      "第 226 个数据未匹配到足够的内容，重新提问...\n",
      "第 227 个数据未匹配到足够的内容，重新提问...\n",
      "第 229 个数据未匹配到足够的内容，重新提问...\n",
      "第 231 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 30...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 232 个数据未匹配到足够的内容，重新提问...\n",
      "第 234 个数据未匹配到足够的内容，重新提问...\n",
      "第 235 个数据未匹配到足够的内容，重新提问...\n",
      "第 236 个数据未匹配到足够的内容，重新提问...\n",
      "第 237 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 31...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 240 个数据未匹配到足够的内容，重新提问...\n",
      "第 241 个数据未匹配到足够的内容，重新提问...\n",
      "第 242 个数据未匹配到足够的内容，重新提问...\n",
      "第 243 个数据未匹配到足够的内容，重新提问...\n",
      "第 244 个数据未匹配到足够的内容，重新提问...\n",
      "第 245 个数据未匹配到足够的内容，重新提问...\n",
      "第 246 个数据未匹配到足够的内容，重新提问...\n",
      "第 247 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 32...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 248 个数据未匹配到足够的内容，重新提问...\n",
      "第 249 个数据未匹配到足够的内容，重新提问...\n",
      "第 250 个数据未匹配到足够的内容，重新提问...\n",
      "第 251 个数据未匹配到足够的内容，重新提问...\n",
      "第 253 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 33...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 257 个数据未匹配到足够的内容，重新提问...\n",
      "第 260 个数据未匹配到足够的内容，重新提问...\n",
      "第 261 个数据未匹配到足够的内容，重新提问...\n",
      "第 263 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 34...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 265 个数据未匹配到足够的内容，重新提问...\n",
      "第 268 个数据未匹配到足够的内容，重新提问...\n",
      "第 271 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 35...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 273 个数据未匹配到足够的内容，重新提问...\n",
      "第 274 个数据未匹配到足够的内容，重新提问...\n",
      "第 275 个数据未匹配到足够的内容，重新提问...\n",
      "第 276 个数据未匹配到足够的内容，重新提问...\n",
      "第 277 个数据未匹配到足够的内容，重新提问...\n",
      "第 278 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 36...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 280 个数据未匹配到足够的内容，重新提问...\n",
      "第 281 个数据未匹配到足够的内容，重新提问...\n",
      "第 282 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 37...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 289 个数据未匹配到足够的内容，重新提问...\n",
      "第 290 个数据未匹配到足够的内容，重新提问...\n",
      "第 292 个数据未匹配到足够的内容，重新提问...\n",
      "第 293 个数据未匹配到足够的内容，重新提问...\n",
      "第 295 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 38...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 297 个数据未匹配到足够的内容，重新提问...\n",
      "第 298 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 39...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 304 个数据未匹配到足够的内容，重新提问...\n",
      "第 306 个数据未匹配到足够的内容，重新提问...\n",
      "第 307 个数据未匹配到足够的内容，重新提问...\n",
      "第 308 个数据未匹配到足够的内容，重新提问...\n",
      "第 310 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 40...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 312 个数据未匹配到足够的内容，重新提问...\n",
      "第 313 个数据未匹配到足够的内容，重新提问...\n",
      "第 314 个数据未匹配到足够的内容，重新提问...\n",
      "第 316 个数据未匹配到足够的内容，重新提问...\n",
      "第 317 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 41...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 323 个数据未匹配到足够的内容，重新提问...\n",
      "第 325 个数据未匹配到足够的内容，重新提问...\n",
      "第 326 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 42...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 328 个数据未匹配到足够的内容，重新提问...\n",
      "第 332 个数据未匹配到足够的内容，重新提问...\n",
      "第 333 个数据未匹配到足够的内容，重新提问...\n",
      "第 334 个数据未匹配到足够的内容，重新提问...\n",
      "第 335 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 43...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 337 个数据未匹配到足够的内容，重新提问...\n",
      "第 338 个数据未匹配到足够的内容，重新提问...\n",
      "第 339 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 44...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 345 个数据未匹配到足够的内容，重新提问...\n",
      "第 347 个数据未匹配到足够的内容，重新提问...\n",
      "第 349 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 45...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 353 个数据未匹配到足够的内容，重新提问...\n",
      "第 355 个数据未匹配到足够的内容，重新提问...\n",
      "第 356 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 46...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 360 个数据未匹配到足够的内容，重新提问...\n",
      "第 361 个数据未匹配到足够的内容，重新提问...\n",
      "第 365 个数据未匹配到足够的内容，重新提问...\n",
      "第 366 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 47...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 368 个数据未匹配到足够的内容，重新提问...\n",
      "第 369 个数据未匹配到足够的内容，重新提问...\n",
      "第 370 个数据未匹配到足够的内容，重新提问...\n",
      "第 372 个数据未匹配到足够的内容，重新提问...\n",
      "第 374 个数据未匹配到足够的内容，重新提问...\n",
      "第 375 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 48...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 376 个数据未匹配到足够的内容，重新提问...\n",
      "第 379 个数据未匹配到足够的内容，重新提问...\n",
      "第 380 个数据未匹配到足够的内容，重新提问...\n",
      "第 381 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 49...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 386 个数据未匹配到足够的内容，重新提问...\n",
      "第 387 个数据未匹配到足够的内容，重新提问...\n",
      "第 388 个数据未匹配到足够的内容，重新提问...\n",
      "第 389 个数据未匹配到足够的内容，重新提问...\n",
      "第 391 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 50...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 392 个数据未匹配到足够的内容，重新提问...\n",
      "第 393 个数据未匹配到足够的内容，重新提问...\n",
      "第 394 个数据未匹配到足够的内容，重新提问...\n",
      "第 396 个数据未匹配到足够的内容，重新提问...\n",
      "第 397 个数据未匹配到足够的内容，重新提问...\n",
      "第 399 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 51...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 400 个数据未匹配到足够的内容，重新提问...\n",
      "第 401 个数据未匹配到足够的内容，重新提问...\n",
      "第 402 个数据未匹配到足够的内容，重新提问...\n",
      "第 403 个数据未匹配到足够的内容，重新提问...\n",
      "第 404 个数据未匹配到足够的内容，重新提问...\n",
      "第 405 个数据未匹配到足够的内容，重新提问...\n",
      "第 406 个数据未匹配到足够的内容，重新提问...\n",
      "第 407 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 52...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 410 个数据未匹配到足够的内容，重新提问...\n",
      "第 411 个数据未匹配到足够的内容，重新提问...\n",
      "第 413 个数据未匹配到足够的内容，重新提问...\n",
      "第 414 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 53...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 416 个数据未匹配到足够的内容，重新提问...\n",
      "第 418 个数据未匹配到足够的内容，重新提问...\n",
      "第 420 个数据未匹配到足够的内容，重新提问...\n",
      "第 421 个数据未匹配到足够的内容，重新提问...\n",
      "第 423 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 54...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 424 个数据未匹配到足够的内容，重新提问...\n",
      "第 425 个数据未匹配到足够的内容，重新提问...\n",
      "第 426 个数据未匹配到足够的内容，重新提问...\n",
      "第 427 个数据未匹配到足够的内容，重新提问...\n",
      "第 431 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 55...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 432 个数据未匹配到足够的内容，重新提问...\n",
      "第 434 个数据未匹配到足够的内容，重新提问...\n",
      "第 435 个数据未匹配到足够的内容，重新提问...\n",
      "第 437 个数据未匹配到足够的内容，重新提问...\n",
      "第 439 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 56...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 440 个数据未匹配到足够的内容，重新提问...\n",
      "第 442 个数据未匹配到足够的内容，重新提问...\n",
      "第 443 个数据未匹配到足够的内容，重新提问...\n",
      "第 444 个数据未匹配到足够的内容，重新提问...\n",
      "第 445 个数据未匹配到足够的内容，重新提问...\n",
      "第 447 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 57...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 448 个数据未匹配到足够的内容，重新提问...\n",
      "第 450 个数据未匹配到足够的内容，重新提问...\n",
      "第 452 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 58...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 459 个数据未匹配到足够的内容，重新提问...\n",
      "第 460 个数据未匹配到足够的内容，重新提问...\n",
      "第 461 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 59...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 464 个数据未匹配到足够的内容，重新提问...\n",
      "第 465 个数据未匹配到足够的内容，重新提问...\n",
      "第 467 个数据未匹配到足够的内容，重新提问...\n",
      "第 470 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 60...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 472 个数据未匹配到足够的内容，重新提问...\n",
      "第 474 个数据未匹配到足够的内容，重新提问...\n",
      "第 475 个数据未匹配到足够的内容，重新提问...\n",
      "第 476 个数据未匹配到足够的内容，重新提问...\n",
      "第 478 个数据未匹配到足够的内容，重新提问...\n",
      "第 479 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 61...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 480 个数据未匹配到足够的内容，重新提问...\n",
      "第 483 个数据未匹配到足够的内容，重新提问...\n",
      "第 485 个数据未匹配到足够的内容，重新提问...\n",
      "第 487 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 62...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 488 个数据未匹配到足够的内容，重新提问...\n",
      "第 489 个数据未匹配到足够的内容，重新提问...\n",
      "第 491 个数据未匹配到足够的内容，重新提问...\n",
      "第 492 个数据未匹配到足够的内容，重新提问...\n",
      "第 493 个数据未匹配到足够的内容，重新提问...\n",
      "第 495 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 63...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 497 个数据未匹配到足够的内容，重新提问...\n",
      "第 500 个数据未匹配到足够的内容，重新提问...\n",
      "第 501 个数据未匹配到足够的内容，重新提问...\n",
      "第 502 个数据未匹配到足够的内容，重新提问...\n",
      "第 503 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 64...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 506 个数据未匹配到足够的内容，重新提问...\n",
      "第 507 个数据未匹配到足够的内容，重新提问...\n",
      "第 509 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 65...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 512 个数据未匹配到足够的内容，重新提问...\n",
      "第 514 个数据未匹配到足够的内容，重新提问...\n",
      "第 515 个数据未匹配到足够的内容，重新提问...\n",
      "第 516 个数据未匹配到足够的内容，重新提问...\n",
      "第 519 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 66...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 520 个数据未匹配到足够的内容，重新提问...\n",
      "第 521 个数据未匹配到足够的内容，重新提问...\n",
      "第 524 个数据未匹配到足够的内容，重新提问...\n",
      "第 526 个数据未匹配到足够的内容，重新提问...\n",
      "第 527 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 67...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 528 个数据未匹配到足够的内容，重新提问...\n",
      "第 529 个数据未匹配到足够的内容，重新提问...\n",
      "第 531 个数据未匹配到足够的内容，重新提问...\n",
      "第 532 个数据未匹配到足够的内容，重新提问...\n",
      "第 535 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 68...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 536 个数据未匹配到足够的内容，重新提问...\n",
      "第 537 个数据未匹配到足够的内容，重新提问...\n",
      "第 538 个数据未匹配到足够的内容，重新提问...\n",
      "第 541 个数据未匹配到足够的内容，重新提问...\n",
      "第 543 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 69...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 544 个数据未匹配到足够的内容，重新提问...\n",
      "第 545 个数据未匹配到足够的内容，重新提问...\n",
      "第 546 个数据未匹配到足够的内容，重新提问...\n",
      "第 549 个数据未匹配到足够的内容，重新提问...\n",
      "第 551 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 70...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 553 个数据未匹配到足够的内容，重新提问...\n",
      "第 554 个数据未匹配到足够的内容，重新提问...\n",
      "第 555 个数据未匹配到足够的内容，重新提问...\n",
      "第 556 个数据未匹配到足够的内容，重新提问...\n",
      "第 557 个数据未匹配到足够的内容，重新提问...\n",
      "第 558 个数据未匹配到足够的内容，重新提问...\n",
      "第 559 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 71...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 560 个数据未匹配到足够的内容，重新提问...\n",
      "第 561 个数据未匹配到足够的内容，重新提问...\n",
      "第 562 个数据未匹配到足够的内容，重新提问...\n",
      "第 567 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 72...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 568 个数据未匹配到足够的内容，重新提问...\n",
      "第 569 个数据未匹配到足够的内容，重新提问...\n",
      "第 570 个数据未匹配到足够的内容，重新提问...\n",
      "第 571 个数据未匹配到足够的内容，重新提问...\n",
      "第 573 个数据未匹配到足够的内容，重新提问...\n",
      "第 575 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 73...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 577 个数据未匹配到足够的内容，重新提问...\n",
      "第 579 个数据未匹配到足够的内容，重新提问...\n",
      "第 580 个数据未匹配到足够的内容，重新提问...\n",
      "第 581 个数据未匹配到足够的内容，重新提问...\n",
      "第 582 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 74...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 584 个数据未匹配到足够的内容，重新提问...\n",
      "第 588 个数据未匹配到足够的内容，重新提问...\n",
      "第 589 个数据未匹配到足够的内容，重新提问...\n",
      "第 590 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 75...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 592 个数据未匹配到足够的内容，重新提问...\n",
      "第 593 个数据未匹配到足够的内容，重新提问...\n",
      "第 594 个数据未匹配到足够的内容，重新提问...\n",
      "第 595 个数据未匹配到足够的内容，重新提问...\n",
      "第 597 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 76...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 600 个数据未匹配到足够的内容，重新提问...\n",
      "第 601 个数据未匹配到足够的内容，重新提问...\n",
      "第 602 个数据未匹配到足够的内容，重新提问...\n",
      "第 603 个数据未匹配到足够的内容，重新提问...\n",
      "第 605 个数据未匹配到足够的内容，重新提问...\n",
      "第 606 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 77...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 608 个数据未匹配到足够的内容，重新提问...\n",
      "第 610 个数据未匹配到足够的内容，重新提问...\n",
      "第 611 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 78...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 616 个数据未匹配到足够的内容，重新提问...\n",
      "第 619 个数据未匹配到足够的内容，重新提问...\n",
      "第 621 个数据未匹配到足够的内容，重新提问...\n",
      "第 622 个数据未匹配到足够的内容，重新提问...\n",
      "第 623 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 79...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 624 个数据未匹配到足够的内容，重新提问...\n",
      "第 625 个数据未匹配到足够的内容，重新提问...\n",
      "第 626 个数据未匹配到足够的内容，重新提问...\n",
      "第 630 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 80...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 635 个数据未匹配到足够的内容，重新提问...\n",
      "第 637 个数据未匹配到足够的内容，重新提问...\n",
      "第 638 个数据未匹配到足够的内容，重新提问...\n",
      "第 639 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 81...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 640 个数据未匹配到足够的内容，重新提问...\n",
      "第 641 个数据未匹配到足够的内容，重新提问...\n",
      "第 642 个数据未匹配到足够的内容，重新提问...\n",
      "第 645 个数据未匹配到足够的内容，重新提问...\n",
      "第 647 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 82...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 649 个数据未匹配到足够的内容，重新提问...\n",
      "第 650 个数据未匹配到足够的内容，重新提问...\n",
      "第 651 个数据未匹配到足够的内容，重新提问...\n",
      "第 652 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 83...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 656 个数据未匹配到足够的内容，重新提问...\n",
      "第 657 个数据未匹配到足够的内容，重新提问...\n",
      "第 658 个数据未匹配到足够的内容，重新提问...\n",
      "第 660 个数据未匹配到足够的内容，重新提问...\n",
      "第 661 个数据未匹配到足够的内容，重新提问...\n",
      "第 663 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 84...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 667 个数据未匹配到足够的内容，重新提问...\n",
      "第 668 个数据未匹配到足够的内容，重新提问...\n",
      "第 670 个数据未匹配到足够的内容，重新提问...\n",
      "第 671 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 85...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 673 个数据未匹配到足够的内容，重新提问...\n",
      "第 676 个数据未匹配到足够的内容，重新提问...\n",
      "第 677 个数据未匹配到足够的内容，重新提问...\n",
      "第 679 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 86...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 681 个数据未匹配到足够的内容，重新提问...\n",
      "第 684 个数据未匹配到足够的内容，重新提问...\n",
      "第 686 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 87...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 688 个数据未匹配到足够的内容，重新提问...\n",
      "第 689 个数据未匹配到足够的内容，重新提问...\n",
      "第 690 个数据未匹配到足够的内容，重新提问...\n",
      "第 692 个数据未匹配到足够的内容，重新提问...\n",
      "第 693 个数据未匹配到足够的内容，重新提问...\n",
      "第 695 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 88...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 698 个数据未匹配到足够的内容，重新提问...\n",
      "第 699 个数据未匹配到足够的内容，重新提问...\n",
      "第 700 个数据未匹配到足够的内容，重新提问...\n",
      "第 702 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 89...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 704 个数据未匹配到足够的内容，重新提问...\n",
      "第 705 个数据未匹配到足够的内容，重新提问...\n",
      "第 706 个数据未匹配到足够的内容，重新提问...\n",
      "第 709 个数据未匹配到足够的内容，重新提问...\n",
      "第 711 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 90...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 713 个数据未匹配到足够的内容，重新提问...\n",
      "第 714 个数据未匹配到足够的内容，重新提问...\n",
      "第 715 个数据未匹配到足够的内容，重新提问...\n",
      "第 716 个数据未匹配到足够的内容，重新提问...\n",
      "第 718 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 91...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 720 个数据未匹配到足够的内容，重新提问...\n",
      "第 724 个数据未匹配到足够的内容，重新提问...\n",
      "第 726 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 92...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 729 个数据未匹配到足够的内容，重新提问...\n",
      "第 730 个数据未匹配到足够的内容，重新提问...\n",
      "第 731 个数据未匹配到足够的内容，重新提问...\n",
      "第 732 个数据未匹配到足够的内容，重新提问...\n",
      "第 733 个数据未匹配到足够的内容，重新提问...\n",
      "第 734 个数据未匹配到足够的内容，重新提问...\n",
      "第 735 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 93...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 737 个数据未匹配到足够的内容，重新提问...\n",
      "第 738 个数据未匹配到足够的内容，重新提问...\n",
      "第 740 个数据未匹配到足够的内容，重新提问...\n",
      "第 741 个数据未匹配到足够的内容，重新提问...\n",
      "第 743 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 94...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 748 个数据未匹配到足够的内容，重新提问...\n",
      "第 749 个数据未匹配到足够的内容，重新提问...\n",
      "第 751 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 95...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 752 个数据未匹配到足够的内容，重新提问...\n",
      "第 756 个数据未匹配到足够的内容，重新提问...\n",
      "第 759 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 96...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 760 个数据未匹配到足够的内容，重新提问...\n",
      "第 762 个数据未匹配到足够的内容，重新提问...\n",
      "第 764 个数据未匹配到足够的内容，重新提问...\n",
      "第 765 个数据未匹配到足够的内容，重新提问...\n",
      "第 766 个数据未匹配到足够的内容，重新提问...\n",
      "第 767 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 97...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 768 个数据未匹配到足够的内容，重新提问...\n",
      "第 772 个数据未匹配到足够的内容，重新提问...\n",
      "第 773 个数据未匹配到足够的内容，重新提问...\n",
      "第 775 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 98...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 776 个数据未匹配到足够的内容，重新提问...\n",
      "第 778 个数据未匹配到足够的内容，重新提问...\n",
      "第 779 个数据未匹配到足够的内容，重新提问...\n",
      "第 780 个数据未匹配到足够的内容，重新提问...\n",
      "第 781 个数据未匹配到足够的内容，重新提问...\n",
      "第 782 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 99...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 785 个数据未匹配到足够的内容，重新提问...\n",
      "第 786 个数据未匹配到足够的内容，重新提问...\n",
      "第 787 个数据未匹配到足够的内容，重新提问...\n",
      "第 788 个数据未匹配到足够的内容，重新提问...\n",
      "第 790 个数据未匹配到足够的内容，重新提问...\n",
      "第 791 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 792 个数据未匹配到足够的内容，重新提问...\n",
      "第 794 个数据未匹配到足够的内容，重新提问...\n",
      "第 796 个数据未匹配到足够的内容，重新提问...\n",
      "第 797 个数据未匹配到足够的内容，重新提问...\n",
      "第 798 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 101...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 800 个数据未匹配到足够的内容，重新提问...\n",
      "第 802 个数据未匹配到足够的内容，重新提问...\n",
      "第 804 个数据未匹配到足够的内容，重新提问...\n",
      "第 806 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 102...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 810 个数据未匹配到足够的内容，重新提问...\n",
      "第 811 个数据未匹配到足够的内容，重新提问...\n",
      "第 812 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 103...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 816 个数据未匹配到足够的内容，重新提问...\n",
      "第 817 个数据未匹配到足够的内容，重新提问...\n",
      "第 818 个数据未匹配到足够的内容，重新提问...\n",
      "第 820 个数据未匹配到足够的内容，重新提问...\n",
      "第 821 个数据未匹配到足够的内容，重新提问...\n",
      "第 822 个数据未匹配到足够的内容，重新提问...\n",
      "第 823 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 104...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 825 个数据未匹配到足够的内容，重新提问...\n",
      "第 826 个数据未匹配到足够的内容，重新提问...\n",
      "第 827 个数据未匹配到足够的内容，重新提问...\n",
      "第 828 个数据未匹配到足够的内容，重新提问...\n",
      "第 831 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 105...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 832 个数据未匹配到足够的内容，重新提问...\n",
      "第 835 个数据未匹配到足够的内容，重新提问...\n",
      "第 836 个数据未匹配到足够的内容，重新提问...\n",
      "第 837 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 106...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 840 个数据未匹配到足够的内容，重新提问...\n",
      "第 842 个数据未匹配到足够的内容，重新提问...\n",
      "第 845 个数据未匹配到足够的内容，重新提问...\n",
      "第 846 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 107...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 848 个数据未匹配到足够的内容，重新提问...\n",
      "第 849 个数据未匹配到足够的内容，重新提问...\n",
      "第 850 个数据未匹配到足够的内容，重新提问...\n",
      "第 851 个数据未匹配到足够的内容，重新提问...\n",
      "第 852 个数据未匹配到足够的内容，重新提问...\n",
      "第 853 个数据未匹配到足够的内容，重新提问...\n",
      "第 854 个数据未匹配到足够的内容，重新提问...\n",
      "第 855 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 108...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 856 个数据未匹配到足够的内容，重新提问...\n",
      "第 857 个数据未匹配到足够的内容，重新提问...\n",
      "第 858 个数据未匹配到足够的内容，重新提问...\n",
      "第 859 个数据未匹配到足够的内容，重新提问...\n",
      "第 860 个数据未匹配到足够的内容，重新提问...\n",
      "第 861 个数据未匹配到足够的内容，重新提问...\n",
      "第 862 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 109...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 865 个数据未匹配到足够的内容，重新提问...\n",
      "第 866 个数据未匹配到足够的内容，重新提问...\n",
      "第 868 个数据未匹配到足够的内容，重新提问...\n",
      "第 869 个数据未匹配到足够的内容，重新提问...\n",
      "第 870 个数据未匹配到足够的内容，重新提问...\n",
      "第 871 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 110...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 872 个数据未匹配到足够的内容，重新提问...\n",
      "第 875 个数据未匹配到足够的内容，重新提问...\n",
      "第 876 个数据未匹配到足够的内容，重新提问...\n",
      "第 878 个数据未匹配到足够的内容，重新提问...\n",
      "第 879 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 111...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 881 个数据未匹配到足够的内容，重新提问...\n",
      "第 882 个数据未匹配到足够的内容，重新提问...\n",
      "第 883 个数据未匹配到足够的内容，重新提问...\n",
      "第 884 个数据未匹配到足够的内容，重新提问...\n",
      "第 885 个数据未匹配到足够的内容，重新提问...\n",
      "第 887 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 112...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 888 个数据未匹配到足够的内容，重新提问...\n",
      "第 890 个数据未匹配到足够的内容，重新提问...\n",
      "第 892 个数据未匹配到足够的内容，重新提问...\n",
      "第 895 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 113...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 896 个数据未匹配到足够的内容，重新提问...\n",
      "第 898 个数据未匹配到足够的内容，重新提问...\n",
      "第 899 个数据未匹配到足够的内容，重新提问...\n",
      "第 900 个数据未匹配到足够的内容，重新提问...\n",
      "第 901 个数据未匹配到足够的内容，重新提问...\n",
      "第 902 个数据未匹配到足够的内容，重新提问...\n",
      "第 903 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 114...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 904 个数据未匹配到足够的内容，重新提问...\n",
      "第 906 个数据未匹配到足够的内容，重新提问...\n",
      "第 907 个数据未匹配到足够的内容，重新提问...\n",
      "第 909 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 115...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 914 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 116...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 920 个数据未匹配到足够的内容，重新提问...\n",
      "第 922 个数据未匹配到足够的内容，重新提问...\n",
      "第 923 个数据未匹配到足够的内容，重新提问...\n",
      "第 925 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 117...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 928 个数据未匹配到足够的内容，重新提问...\n",
      "第 929 个数据未匹配到足够的内容，重新提问...\n",
      "第 930 个数据未匹配到足够的内容，重新提问...\n",
      "第 931 个数据未匹配到足够的内容，重新提问...\n",
      "第 934 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 118...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 936 个数据未匹配到足够的内容，重新提问...\n",
      "第 937 个数据未匹配到足够的内容，重新提问...\n",
      "第 938 个数据未匹配到足够的内容，重新提问...\n",
      "第 939 个数据未匹配到足够的内容，重新提问...\n",
      "第 940 个数据未匹配到足够的内容，重新提问...\n",
      "第 942 个数据未匹配到足够的内容，重新提问...\n",
      "第 943 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 119...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 944 个数据未匹配到足够的内容，重新提问...\n",
      "第 945 个数据未匹配到足够的内容，重新提问...\n",
      "第 946 个数据未匹配到足够的内容，重新提问...\n",
      "第 947 个数据未匹配到足够的内容，重新提问...\n",
      "第 948 个数据未匹配到足够的内容，重新提问...\n",
      "第 950 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 120...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 952 个数据未匹配到足够的内容，重新提问...\n",
      "第 953 个数据未匹配到足够的内容，重新提问...\n",
      "第 954 个数据未匹配到足够的内容，重新提问...\n",
      "第 955 个数据未匹配到足够的内容，重新提问...\n",
      "第 956 个数据未匹配到足够的内容，重新提问...\n",
      "第 957 个数据未匹配到足够的内容，重新提问...\n",
      "第 959 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 121...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 960 个数据未匹配到足够的内容，重新提问...\n",
      "第 965 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 122...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 969 个数据未匹配到足够的内容，重新提问...\n",
      "第 971 个数据未匹配到足够的内容，重新提问...\n",
      "第 974 个数据未匹配到足够的内容，重新提问...\n",
      "第 975 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 123...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 978 个数据未匹配到足够的内容，重新提问...\n",
      "第 979 个数据未匹配到足够的内容，重新提问...\n",
      "第 980 个数据未匹配到足够的内容，重新提问...\n",
      "第 981 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 124...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 984 个数据未匹配到足够的内容，重新提问...\n",
      "第 985 个数据未匹配到足够的内容，重新提问...\n",
      "第 986 个数据未匹配到足够的内容，重新提问...\n",
      "第 988 个数据未匹配到足够的内容，重新提问...\n",
      "第 991 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 125...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 992 个数据未匹配到足够的内容，重新提问...\n",
      "第 995 个数据未匹配到足够的内容，重新提问...\n",
      "第 996 个数据未匹配到足够的内容，重新提问...\n",
      "第 997 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 126...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1001 个数据未匹配到足够的内容，重新提问...\n",
      "第 1006 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 127...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1009 个数据未匹配到足够的内容，重新提问...\n",
      "第 1010 个数据未匹配到足够的内容，重新提问...\n",
      "第 1011 个数据未匹配到足够的内容，重新提问...\n",
      "第 1013 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 128...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1017 个数据未匹配到足够的内容，重新提问...\n",
      "第 1018 个数据未匹配到足够的内容，重新提问...\n",
      "第 1019 个数据未匹配到足够的内容，重新提问...\n",
      "第 1020 个数据未匹配到足够的内容，重新提问...\n",
      "第 1021 个数据未匹配到足够的内容，重新提问...\n",
      "第 1022 个数据未匹配到足够的内容，重新提问...\n",
      "第 1023 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 129...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1024 个数据未匹配到足够的内容，重新提问...\n",
      "第 1025 个数据未匹配到足够的内容，重新提问...\n",
      "第 1026 个数据未匹配到足够的内容，重新提问...\n",
      "第 1029 个数据未匹配到足够的内容，重新提问...\n",
      "第 1030 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 130...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1032 个数据未匹配到足够的内容，重新提问...\n",
      "第 1036 个数据未匹配到足够的内容，重新提问...\n",
      "第 1038 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 131...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1040 个数据未匹配到足够的内容，重新提问...\n",
      "第 1041 个数据未匹配到足够的内容，重新提问...\n",
      "第 1045 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 132...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1048 个数据未匹配到足够的内容，重新提问...\n",
      "第 1051 个数据未匹配到足够的内容，重新提问...\n",
      "第 1052 个数据未匹配到足够的内容，重新提问...\n",
      "第 1053 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 133...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1056 个数据未匹配到足够的内容，重新提问...\n",
      "第 1059 个数据未匹配到足够的内容，重新提问...\n",
      "第 1061 个数据未匹配到足够的内容，重新提问...\n",
      "第 1062 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 134...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1066 个数据未匹配到足够的内容，重新提问...\n",
      "第 1068 个数据未匹配到足够的内容，重新提问...\n",
      "第 1069 个数据未匹配到足够的内容，重新提问...\n",
      "第 1071 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 135...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1072 个数据未匹配到足够的内容，重新提问...\n",
      "第 1074 个数据未匹配到足够的内容，重新提问...\n",
      "第 1076 个数据未匹配到足够的内容，重新提问...\n",
      "第 1078 个数据未匹配到足够的内容，重新提问...\n",
      "第 1079 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 136...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1080 个数据未匹配到足够的内容，重新提问...\n",
      "第 1081 个数据未匹配到足够的内容，重新提问...\n",
      "第 1082 个数据未匹配到足够的内容，重新提问...\n",
      "第 1083 个数据未匹配到足够的内容，重新提问...\n",
      "第 1085 个数据未匹配到足够的内容，重新提问...\n",
      "第 1086 个数据未匹配到足够的内容，重新提问...\n",
      "第 1087 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 137...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1088 个数据未匹配到足够的内容，重新提问...\n",
      "第 1091 个数据未匹配到足够的内容，重新提问...\n",
      "第 1093 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 138...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1096 个数据未匹配到足够的内容，重新提问...\n",
      "第 1097 个数据未匹配到足够的内容，重新提问...\n",
      "第 1099 个数据未匹配到足够的内容，重新提问...\n",
      "第 1102 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 139...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1107 个数据未匹配到足够的内容，重新提问...\n",
      "第 1108 个数据未匹配到足够的内容，重新提问...\n",
      "第 1109 个数据未匹配到足够的内容，重新提问...\n",
      "第 1110 个数据未匹配到足够的内容，重新提问...\n",
      "第 1111 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 140...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1112 个数据未匹配到足够的内容，重新提问...\n",
      "第 1113 个数据未匹配到足够的内容，重新提问...\n",
      "第 1117 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 141...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1121 个数据未匹配到足够的内容，重新提问...\n",
      "第 1122 个数据未匹配到足够的内容，重新提问...\n",
      "第 1123 个数据未匹配到足够的内容，重新提问...\n",
      "第 1126 个数据未匹配到足够的内容，重新提问...\n",
      "第 1127 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 142...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1128 个数据未匹配到足够的内容，重新提问...\n",
      "第 1129 个数据未匹配到足够的内容，重新提问...\n",
      "第 1131 个数据未匹配到足够的内容，重新提问...\n",
      "第 1132 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 143...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1138 个数据未匹配到足够的内容，重新提问...\n",
      "第 1140 个数据未匹配到足够的内容，重新提问...\n",
      "第 1142 个数据未匹配到足够的内容，重新提问...\n",
      "第 1143 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 144...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1145 个数据未匹配到足够的内容，重新提问...\n",
      "第 1146 个数据未匹配到足够的内容，重新提问...\n",
      "第 1147 个数据未匹配到足够的内容，重新提问...\n",
      "第 1148 个数据未匹配到足够的内容，重新提问...\n",
      "第 1149 个数据未匹配到足够的内容，重新提问...\n",
      "第 1150 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 145...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1152 个数据未匹配到足够的内容，重新提问...\n",
      "第 1153 个数据未匹配到足够的内容，重新提问...\n",
      "第 1155 个数据未匹配到足够的内容，重新提问...\n",
      "第 1159 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 146...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1160 个数据未匹配到足够的内容，重新提问...\n",
      "第 1162 个数据未匹配到足够的内容，重新提问...\n",
      "第 1163 个数据未匹配到足够的内容，重新提问...\n",
      "第 1164 个数据未匹配到足够的内容，重新提问...\n",
      "第 1165 个数据未匹配到足够的内容，重新提问...\n",
      "第 1166 个数据未匹配到足够的内容，重新提问...\n",
      "第 1167 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 147...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1171 个数据未匹配到足够的内容，重新提问...\n",
      "第 1172 个数据未匹配到足够的内容，重新提问...\n",
      "第 1173 个数据未匹配到足够的内容，重新提问...\n",
      "第 1175 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 148...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1176 个数据未匹配到足够的内容，重新提问...\n",
      "第 1177 个数据未匹配到足够的内容，重新提问...\n",
      "第 1178 个数据未匹配到足够的内容，重新提问...\n",
      "第 1179 个数据未匹配到足够的内容，重新提问...\n",
      "第 1180 个数据未匹配到足够的内容，重新提问...\n",
      "第 1183 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 149...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1185 个数据未匹配到足够的内容，重新提问...\n",
      "第 1188 个数据未匹配到足够的内容，重新提问...\n",
      "第 1190 个数据未匹配到足够的内容，重新提问...\n",
      "第 1191 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 150...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1192 个数据未匹配到足够的内容，重新提问...\n",
      "第 1193 个数据未匹配到足够的内容，重新提问...\n",
      "第 1194 个数据未匹配到足够的内容，重新提问...\n",
      "第 1195 个数据未匹配到足够的内容，重新提问...\n",
      "第 1196 个数据未匹配到足够的内容，重新提问...\n",
      "第 1198 个数据未匹配到足够的内容，重新提问...\n",
      "第 1199 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 151...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1200 个数据未匹配到足够的内容，重新提问...\n",
      "第 1202 个数据未匹配到足够的内容，重新提问...\n",
      "第 1203 个数据未匹配到足够的内容，重新提问...\n",
      "第 1204 个数据未匹配到足够的内容，重新提问...\n",
      "第 1205 个数据未匹配到足够的内容，重新提问...\n",
      "第 1206 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 152...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1208 个数据未匹配到足够的内容，重新提问...\n",
      "第 1209 个数据未匹配到足够的内容，重新提问...\n",
      "第 1210 个数据未匹配到足够的内容，重新提问...\n",
      "第 1212 个数据未匹配到足够的内容，重新提问...\n",
      "第 1213 个数据未匹配到足够的内容，重新提问...\n",
      "第 1214 个数据未匹配到足够的内容，重新提问...\n",
      "第 1215 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 153...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1217 个数据未匹配到足够的内容，重新提问...\n",
      "第 1218 个数据未匹配到足够的内容，重新提问...\n",
      "第 1219 个数据未匹配到足够的内容，重新提问...\n",
      "第 1220 个数据未匹配到足够的内容，重新提问...\n",
      "第 1221 个数据未匹配到足够的内容，重新提问...\n",
      "第 1223 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 154...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1224 个数据未匹配到足够的内容，重新提问...\n",
      "第 1225 个数据未匹配到足够的内容，重新提问...\n",
      "第 1226 个数据未匹配到足够的内容，重新提问...\n",
      "第 1229 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 155...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1232 个数据未匹配到足够的内容，重新提问...\n",
      "第 1233 个数据未匹配到足够的内容，重新提问...\n",
      "第 1234 个数据未匹配到足够的内容，重新提问...\n",
      "第 1236 个数据未匹配到足够的内容，重新提问...\n",
      "第 1237 个数据未匹配到足够的内容，重新提问...\n",
      "第 1238 个数据未匹配到足够的内容，重新提问...\n",
      "第 1239 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 156...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1240 个数据未匹配到足够的内容，重新提问...\n",
      "第 1241 个数据未匹配到足够的内容，重新提问...\n",
      "第 1243 个数据未匹配到足够的内容，重新提问...\n",
      "第 1244 个数据未匹配到足够的内容，重新提问...\n",
      "第 1245 个数据未匹配到足够的内容，重新提问...\n",
      "第 1247 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 157...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1249 个数据未匹配到足够的内容，重新提问...\n",
      "第 1250 个数据未匹配到足够的内容，重新提问...\n",
      "第 1251 个数据未匹配到足够的内容，重新提问...\n",
      "第 1252 个数据未匹配到足够的内容，重新提问...\n",
      "第 1253 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 158...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1256 个数据未匹配到足够的内容，重新提问...\n",
      "第 1257 个数据未匹配到足够的内容，重新提问...\n",
      "第 1258 个数据未匹配到足够的内容，重新提问...\n",
      "第 1260 个数据未匹配到足够的内容，重新提问...\n",
      "第 1261 个数据未匹配到足够的内容，重新提问...\n",
      "第 1262 个数据未匹配到足够的内容，重新提问...\n",
      "第 1263 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 159...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1266 个数据未匹配到足够的内容，重新提问...\n",
      "第 1267 个数据未匹配到足够的内容，重新提问...\n",
      "第 1270 个数据未匹配到足够的内容，重新提问...\n",
      "第 1271 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 160...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1272 个数据未匹配到足够的内容，重新提问...\n",
      "第 1273 个数据未匹配到足够的内容，重新提问...\n",
      "第 1275 个数据未匹配到足够的内容，重新提问...\n",
      "第 1278 个数据未匹配到足够的内容，重新提问...\n",
      "第 1279 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 161...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1280 个数据未匹配到足够的内容，重新提问...\n",
      "第 1282 个数据未匹配到足够的内容，重新提问...\n",
      "第 1283 个数据未匹配到足够的内容，重新提问...\n",
      "第 1285 个数据未匹配到足够的内容，重新提问...\n",
      "第 1286 个数据未匹配到足够的内容，重新提问...\n",
      "第 1287 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 162...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1288 个数据未匹配到足够的内容，重新提问...\n",
      "第 1289 个数据未匹配到足够的内容，重新提问...\n",
      "第 1290 个数据未匹配到足够的内容，重新提问...\n",
      "第 1292 个数据未匹配到足够的内容，重新提问...\n",
      "第 1294 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 163...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1296 个数据未匹配到足够的内容，重新提问...\n",
      "第 1297 个数据未匹配到足够的内容，重新提问...\n",
      "第 1299 个数据未匹配到足够的内容，重新提问...\n",
      "第 1301 个数据未匹配到足够的内容，重新提问...\n",
      "第 1303 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 164...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1304 个数据未匹配到足够的内容，重新提问...\n",
      "第 1305 个数据未匹配到足够的内容，重新提问...\n",
      "第 1306 个数据未匹配到足够的内容，重新提问...\n",
      "第 1307 个数据未匹配到足够的内容，重新提问...\n",
      "第 1308 个数据未匹配到足够的内容，重新提问...\n",
      "第 1309 个数据未匹配到足够的内容，重新提问...\n",
      "第 1310 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 165...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1313 个数据未匹配到足够的内容，重新提问...\n",
      "第 1315 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 166...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1320 个数据未匹配到足够的内容，重新提问...\n",
      "第 1321 个数据未匹配到足够的内容，重新提问...\n",
      "第 1323 个数据未匹配到足够的内容，重新提问...\n",
      "第 1324 个数据未匹配到足够的内容，重新提问...\n",
      "第 1325 个数据未匹配到足够的内容，重新提问...\n",
      "第 1326 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 167...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1328 个数据未匹配到足够的内容，重新提问...\n",
      "第 1329 个数据未匹配到足够的内容，重新提问...\n",
      "第 1330 个数据未匹配到足够的内容，重新提问...\n",
      "第 1331 个数据未匹配到足够的内容，重新提问...\n",
      "第 1332 个数据未匹配到足够的内容，重新提问...\n",
      "第 1333 个数据未匹配到足够的内容，重新提问...\n",
      "第 1335 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 168...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1336 个数据未匹配到足够的内容，重新提问...\n",
      "第 1338 个数据未匹配到足够的内容，重新提问...\n",
      "第 1339 个数据未匹配到足够的内容，重新提问...\n",
      "第 1340 个数据未匹配到足够的内容，重新提问...\n",
      "第 1341 个数据未匹配到足够的内容，重新提问...\n",
      "第 1342 个数据未匹配到足够的内容，重新提问...\n",
      "第 1343 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 169...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1344 个数据未匹配到足够的内容，重新提问...\n",
      "第 1346 个数据未匹配到足够的内容，重新提问...\n",
      "第 1347 个数据未匹配到足够的内容，重新提问...\n",
      "第 1348 个数据未匹配到足够的内容，重新提问...\n",
      "第 1350 个数据未匹配到足够的内容，重新提问...\n",
      "第 1351 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 170...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1353 个数据未匹配到足够的内容，重新提问...\n",
      "第 1354 个数据未匹配到足够的内容，重新提问...\n",
      "第 1356 个数据未匹配到足够的内容，重新提问...\n",
      "第 1357 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 171...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1361 个数据未匹配到足够的内容，重新提问...\n",
      "第 1362 个数据未匹配到足够的内容，重新提问...\n",
      "第 1365 个数据未匹配到足够的内容，重新提问...\n",
      "第 1367 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 172...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1368 个数据未匹配到足够的内容，重新提问...\n",
      "第 1370 个数据未匹配到足够的内容，重新提问...\n",
      "第 1371 个数据未匹配到足够的内容，重新提问...\n",
      "第 1372 个数据未匹配到足够的内容，重新提问...\n",
      "第 1374 个数据未匹配到足够的内容，重新提问...\n",
      "第 1375 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 173...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1377 个数据未匹配到足够的内容，重新提问...\n",
      "第 1379 个数据未匹配到足够的内容，重新提问...\n",
      "第 1381 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 174...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1384 个数据未匹配到足够的内容，重新提问...\n",
      "第 1386 个数据未匹配到足够的内容，重新提问...\n",
      "第 1387 个数据未匹配到足够的内容，重新提问...\n",
      "第 1388 个数据未匹配到足够的内容，重新提问...\n",
      "第 1389 个数据未匹配到足够的内容，重新提问...\n",
      "第 1390 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 175...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1393 个数据未匹配到足够的内容，重新提问...\n",
      "第 1397 个数据未匹配到足够的内容，重新提问...\n",
      "第 1398 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 176...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1400 个数据未匹配到足够的内容，重新提问...\n",
      "第 1402 个数据未匹配到足够的内容，重新提问...\n",
      "第 1404 个数据未匹配到足够的内容，重新提问...\n",
      "第 1405 个数据未匹配到足够的内容，重新提问...\n",
      "第 1406 个数据未匹配到足够的内容，重新提问...\n",
      "第 1407 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 177...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1408 个数据未匹配到足够的内容，重新提问...\n",
      "第 1410 个数据未匹配到足够的内容，重新提问...\n",
      "第 1411 个数据未匹配到足够的内容，重新提问...\n",
      "第 1415 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 178...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1416 个数据未匹配到足够的内容，重新提问...\n",
      "第 1417 个数据未匹配到足够的内容，重新提问...\n",
      "第 1421 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 179...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1424 个数据未匹配到足够的内容，重新提问...\n",
      "第 1425 个数据未匹配到足够的内容，重新提问...\n",
      "第 1426 个数据未匹配到足够的内容，重新提问...\n",
      "第 1430 个数据未匹配到足够的内容，重新提问...\n",
      "第 1431 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 180...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1432 个数据未匹配到足够的内容，重新提问...\n",
      "第 1436 个数据未匹配到足够的内容，重新提问...\n",
      "第 1438 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 181...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1440 个数据未匹配到足够的内容，重新提问...\n",
      "第 1442 个数据未匹配到足够的内容，重新提问...\n",
      "第 1444 个数据未匹配到足够的内容，重新提问...\n",
      "第 1445 个数据未匹配到足够的内容，重新提问...\n",
      "第 1446 个数据未匹配到足够的内容，重新提问...\n",
      "第 1447 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 182...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1448 个数据未匹配到足够的内容，重新提问...\n",
      "第 1449 个数据未匹配到足够的内容，重新提问...\n",
      "第 1451 个数据未匹配到足够的内容，重新提问...\n",
      "第 1453 个数据未匹配到足够的内容，重新提问...\n",
      "第 1454 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 183...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1456 个数据未匹配到足够的内容，重新提问...\n",
      "第 1458 个数据未匹配到足够的内容，重新提问...\n",
      "第 1459 个数据未匹配到足够的内容，重新提问...\n",
      "第 1460 个数据未匹配到足够的内容，重新提问...\n",
      "第 1461 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 184...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1464 个数据未匹配到足够的内容，重新提问...\n",
      "第 1465 个数据未匹配到足够的内容，重新提问...\n",
      "第 1466 个数据未匹配到足够的内容，重新提问...\n",
      "第 1467 个数据未匹配到足够的内容，重新提问...\n",
      "第 1468 个数据未匹配到足够的内容，重新提问...\n",
      "第 1469 个数据未匹配到足够的内容，重新提问...\n",
      "第 1470 个数据未匹配到足够的内容，重新提问...\n",
      "第 1471 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 185...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1472 个数据未匹配到足够的内容，重新提问...\n",
      "第 1474 个数据未匹配到足够的内容，重新提问...\n",
      "第 1476 个数据未匹配到足够的内容，重新提问...\n",
      "第 1479 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 186...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1480 个数据未匹配到足够的内容，重新提问...\n",
      "第 1481 个数据未匹配到足够的内容，重新提问...\n",
      "第 1482 个数据未匹配到足够的内容，重新提问...\n",
      "第 1483 个数据未匹配到足够的内容，重新提问...\n",
      "第 1484 个数据未匹配到足够的内容，重新提问...\n",
      "第 1486 个数据未匹配到足够的内容，重新提问...\n",
      "第 1487 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 187...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1490 个数据未匹配到足够的内容，重新提问...\n",
      "第 1491 个数据未匹配到足够的内容，重新提问...\n",
      "第 1492 个数据未匹配到足够的内容，重新提问...\n",
      "第 1495 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 188...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1497 个数据未匹配到足够的内容，重新提问...\n",
      "第 1498 个数据未匹配到足够的内容，重新提问...\n",
      "第 1499 个数据未匹配到足够的内容，重新提问...\n",
      "第 1500 个数据未匹配到足够的内容，重新提问...\n",
      "第 1501 个数据未匹配到足够的内容，重新提问...\n",
      "第 1502 个数据未匹配到足够的内容，重新提问...\n",
      "第 1503 个数据未匹配到足够的内容，重新提问...\n",
      "Processing batch 189...\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 3.34 GiB. GPU ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 35\u001b[0m\n\u001b[1;32m     32\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer(batch_q, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# 生成输出\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m generated_ids \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\n\u001b[1;32m     38\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# 解码生成的文本\u001b[39;00m\n\u001b[1;32m     41\u001b[0m responses \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mbatch_decode(generated_ids, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/transformers/generation/utils.py:2215\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2207\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2208\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2209\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2210\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2211\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2212\u001b[0m     )\n\u001b[1;32m   2214\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2215\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2216\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2217\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2218\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2219\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2220\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2221\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2222\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2223\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2225\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2226\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   2227\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   2228\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   2229\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2234\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   2235\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/transformers/generation/utils.py:3206\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3203\u001b[0m model_inputs\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_hidden_states\u001b[39m\u001b[38;5;124m\"\u001b[39m: output_hidden_states} \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;28;01melse\u001b[39;00m {})\n\u001b[1;32m   3205\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 3206\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   3208\u001b[0m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[1;32m   3209\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_model_kwargs_for_generation(\n\u001b[1;32m   3210\u001b[0m     outputs,\n\u001b[1;32m   3211\u001b[0m     model_kwargs,\n\u001b[1;32m   3212\u001b[0m     is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   3213\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/transformers/models/qwen2/modeling_qwen2.py:1164\u001b[0m, in \u001b[0;36mQwen2ForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, num_logits_to_keep, **loss_kwargs)\u001b[0m\n\u001b[1;32m   1161\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1163\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1164\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1165\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1166\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1167\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1168\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1169\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1170\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1171\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1172\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1173\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1175\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1177\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1178\u001b[0m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/transformers/models/qwen2/modeling_qwen2.py:864\u001b[0m, in \u001b[0;36mQwen2Model.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    861\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m position_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    862\u001b[0m     position_ids \u001b[38;5;241m=\u001b[39m cache_position\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m--> 864\u001b[0m causal_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_causal_mask\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\n\u001b[1;32m    866\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    868\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m inputs_embeds\n\u001b[1;32m    870\u001b[0m \u001b[38;5;66;03m# create position embeddings to be shared across the decoder layers\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/transformers/models/qwen2/modeling_qwen2.py:984\u001b[0m, in \u001b[0;36mQwen2Model._update_causal_mask\u001b[0;34m(self, attention_mask, input_tensor, cache_position, past_key_values, output_attentions)\u001b[0m\n\u001b[1;32m    977\u001b[0m     target_length \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    978\u001b[0m         attention_mask\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    979\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(attention_mask, torch\u001b[38;5;241m.\u001b[39mTensor)\n\u001b[1;32m    980\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m past_seen_tokens \u001b[38;5;241m+\u001b[39m sequence_length \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    981\u001b[0m     )\n\u001b[1;32m    983\u001b[0m \u001b[38;5;66;03m# In case the provided `attention` mask is 2D, we generate a causal mask here (4D).\u001b[39;00m\n\u001b[0;32m--> 984\u001b[0m causal_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_4d_causal_attention_mask_with_cache_position\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    985\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    986\u001b[0m \u001b[43m    \u001b[49m\u001b[43msequence_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msequence_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    987\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    988\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    989\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    990\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    991\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_tensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    992\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    993\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    994\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    996\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    997\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39m_attn_implementation \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msdpa\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    998\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1003\u001b[0m     \u001b[38;5;66;03m# using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path.\u001b[39;00m\n\u001b[1;32m   1004\u001b[0m     \u001b[38;5;66;03m# Details: https://github.com/pytorch/pytorch/issues/110213\u001b[39;00m\n\u001b[1;32m   1005\u001b[0m     causal_mask \u001b[38;5;241m=\u001b[39m AttentionMaskConverter\u001b[38;5;241m.\u001b[39m_unmask_unattended(causal_mask, min_dtype)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/transformers/models/qwen2/modeling_qwen2.py:1071\u001b[0m, in \u001b[0;36mQwen2Model._prepare_4d_causal_attention_mask_with_cache_position\u001b[0;34m(attention_mask, sequence_length, target_length, dtype, device, cache_position, batch_size, config, past_key_values)\u001b[0m\n\u001b[1;32m   1069\u001b[0m         mask_length \u001b[38;5;241m=\u001b[39m attention_mask\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   1070\u001b[0m         padding_mask \u001b[38;5;241m=\u001b[39m causal_mask[:, :, :, :mask_length] \u001b[38;5;241m+\u001b[39m attention_mask[:, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, :]\n\u001b[0;32m-> 1071\u001b[0m         padding_mask \u001b[38;5;241m=\u001b[39m \u001b[43mpadding_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\n\u001b[1;32m   1072\u001b[0m         causal_mask[:, :, :, :mask_length] \u001b[38;5;241m=\u001b[39m causal_mask[:, :, :, :mask_length]\u001b[38;5;241m.\u001b[39mmasked_fill(\n\u001b[1;32m   1073\u001b[0m             padding_mask, min_dtype\n\u001b[1;32m   1074\u001b[0m         )\n\u001b[1;32m   1075\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m causal_mask\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 3.34 GiB. GPU "
     ]
    }
   ],
   "source": [
    "prompt_task = '''我需要你做中文概括任务，以下是一个示例\n",
    "原句 : 24岁 的 男子 小罗 ( 化名 ) 因 患有 精神 病 被 送进 医院 , 为 防止 他 伤害 自己 , 医院 用 保护 带 将 其 绑 在床上 。第二天 , 护士 发现 小罗 被 同室 病人 掐死 。\", \"为此 , 小罗 母亲 提起 刑事 自诉 , 要求 追究 当班 护士 的 刑事责任 。\", \"法 晚 记者 上午 获悉 , 市 一中 院 终审 判处 当班 护士 杨某 犯 医疗 事故 罪 , 但 由于 医院 已 与 家属 达成 协议 , 赔偿 了 50万 元 , 且 杨某 已 被 医院 开除 , 故 对 其 免予 刑事 处罚 。\", \"住院 当晚 被 同室 病人 掐死 1999年 11月 29 日 , 24岁 的 被害人 小罗 因 精神 受到 刺激 被 送往 本市 海淀区 北京大学 第六 医院 治疗 。\", \"经 诊断 系 “ 急性 精神病 症 ” , 医嘱 为 精神科 特护 ( 防 冲动 、 巡视 )\", \"当晚 , 因 小罗 拒绝 服药 打针 , 医院 用 保护 带 将 其 绑 在床上 , 后 又 因其 吵闹 , 医师 给 其 注射 了 安定 10mg 。\", \"次日 早晨 6 点 10分 , 小罗 被 发现 已 死亡 。\", \"海淀 法院 一审 判决书 查明 , 1999年 11月 30 日 , 杨某 在 本市 海淀区 北京 医科 大学 附属 第六 医院 住院部 , 作为 大夜班 副 班 护士 , 未 严格 履行 巡视 职责 , 对 已 约束 的 病人 未 按 规定 定时 松解 保护 带 。\", \"导致 小罗 在 杨某 值班 期间 , 被 同室 39 岁 的 精神病人 唐某 扼 颈 机械 性窒息 死亡 。\", \"经 司法 精神病 鉴定 , 唐某 实施 违法行为 时 丧失 辨认 、 控制 能力 , 无责任 能力 。\", \"2000年 1 月 19日 , 北京 医科 大学 附属 第六 医院 与 被害人 家属 达成 协议 , 赔偿 被害人 家属 50万 元 并 承担 其他 损失 费用 。\", \"医院 对 护士 杨某 和 郝某 予以 行政 处分 , 决定 对 杨某 给予 开除 行政 处分 。\", \"当班 护士 认为 “ 应 由 医院 担责 ” 杨某 供述 称 , 1999年 11月 29 日 下午 1 时 许 , 护士 站 急 收 了 病人 小罗 , 有 3 名 家属 护送 。\", \"当时 , 小罗 比较 兴奋 吵闹 , 杨某 就 腾出 一个 单间 安排 其 入住 , 并 对 小罗 采取 了 保护 措施 , 用 绑带 把 他 绑 在 病床 上 。\", \"把 门锁 上后 , 杨某 于 当日 下午 3 时 就 下班 了 。\", \"晚上 11点 多 , 杨某 到 单位 上夜班 。\", \"到了 次日 凌晨 1点 30 分 , 他 和 另外 一 名 实习 护士 郝某 与 前 班 护士 办理 交接 。\", \"杨某 首先 隔着 门 玻璃 查看 清点 了 一下 人数 , 郝某 清点 医用 物品 , 后 一起 听取 了 前 一班 护士 的 交班 报告 。\", \"在 清点 人数 时 , 杨某 发现 小罗 的 病房 里 又新 进 了 一个 病人 唐某 。\", \"当晚 , 杨某 在 交接班 之后 , 只 在 凌晨 2 点 左右 巡视 过 1 次 , 只是 清点 了 人数 , 没有 进 病房 查看 , 之后 就 再 没有 巡视 查看 过 。\", \"次日 早上 6 点 多钟 , 郝某 找到 杨某 说 小罗 有些 不对劲 。\", \"杨某 到 小罗 床 前 掀开 被子 , 发现 小罗 还 被 捆着 , 身体 已经 僵 了 。\", \"杨某 认为 自己 虽有 过错 , 但 尚 不 构成 犯罪 , 受害人 小罗 的 死亡 结果 应当 由 医院 承担 责任 。\", \"杨某 辩护人 认为 , 受害人 死亡 的 原因 是 多 方面 的 , 既有 唐某 的 直接 原因 , 也 有 医院 管理 混乱 , 工作 交接 脱节 , 上 一班 医护 人员 错误 安排 病房 、 没有 及时 松解 保护 带 等 方面 原因 。\", \"杨某 未 按 规定 巡视 并不 必然 导致 被害人 小罗 死亡 的 结果 发生 。\", \"家属 向 护士 提起 刑事 自诉 郝某 称 , 按规定 , 应该 对 病人 每隔 10分钟 巡视 1 次 , 对 小罗 这样 的 病人 应该 重点 观察 。\", \"此外 , 对于 小罗 这样 有 被 约束 病人 的 病房 里 , 是 不能 再 入住 没有 被 约束 的 病人 的 。\", \"医院 出具 的 特护 记录 显示 , 杨某 的 前 一班 护士 对 小罗 都 有 详细 且 明确 的 护理 记录 , 但 从 当晚 1 点 半 杨某 接班 以后 , 就 没有 任何 关于 对 小罗 的 护理 记录 , 杨某 一 晚上 都 没有 对 小罗 进行 过 巡查 , 也 没有 按规定 护理 。\", \"2006年 , 北京 医学会 、 中华 医学会 两次 进行 医疗 事故 鉴定 , 结论 均 为 : 本 例 医疗 事故 争议 属于 一级 甲等 医疗 事故 , 医院 在 患者 的 损伤 结果 中 承担 次要 责任 。\", \"检察机关 认定 被告人 杨某 涉嫌 医疗 事故 罪 , 但 情节 轻微 , 决定 对 被告人 杨某 不起诉 。\", \"为此 , 小罗 的 母亲 赵某 向 海淀 法院 提起 刑事 自诉 。\", \"构成 医疗 事故 罪 免予 刑事 处罚 海淀 法院 认为 , 多项 证据 显示 , 虽然 被害人 小罗 的 死亡 是 由 第三人 病态 行为 直接 造成 , 但 该 死亡 结果 是 在 被害人 就诊 期间 发生 , 由于 医院 管理 不当 和 严重 失职 所 导致 的 。\", \"杨某 身为 当晚 值班 副 班 护士 , 是 事发 当时 代表 医院 承担 巡视 、 护理 职责 , 其 严重 不 负责 的 行为 是 医院 没有 履行 好 保护 患者 安全 职责 的 表现 之一 , 与 该 死亡 结果 存在 着 重要 的 因果关系 , 应当 以 医疗 事故 罪 追究 其 刑事责任 。\", \"鉴于 此次 医疗 事故 系 由 多 原因 所 造成 , 医院 多 方面 的 过失 并非 仅限于 杨某 单个 因素 。\", \"故 杨某 虽 构成 犯罪 , 但 情节 相对 轻微 , 且 系 初犯 , 到案 后 能 如实 供认 其 失职 行为 , 具有 悔罪 表现 , 同时 事后 杨某 已 被 医院 处以 开除 的 行政 处分 , 受到 相应 惩罚 , 已 无 判处 刑罚 之 必要 , 故 法院 认为 可以 对 其 免予 刑事 处罚 。\", \"据此 , 海淀 法院 作出 一审 判决 , 杨某 犯 医疗 事故 罪 , 免予 刑事 处罚 。\", \"一审 判决 后 , 小罗 家属 提起 上诉 , 认为 原判 对 杨某 的 处罚 过轻 。\", \"杨某 也 提起 了 上诉 , 杨某 认为 被害人 的 死亡 后果 并非 其 造成 的 , 其 不 构成 犯罪 。\", \"市 一中 院 审理 后 , 驳回 杨某 和 赵 女士 的 上诉 , 维持 原判 。\"\n",
    "概括 : 北京 精神病人 被 用 束缚 带 绑 床上 , 遭 病友 掐死 ; 法院 认定 医院 管理 不当 、 值班 护士 失职 , 构成 医疗 事故 罪 。<END>\n",
    "用简短的语句概括以下内容,概括的句子 <END> 结尾\n",
    "原句 :'''\n",
    "import re\n",
    "batch_size = 8  # 设置批处理大小（可以根据GPU的内存进行调整）\n",
    "all_answer = []\n",
    "all_pre = []\n",
    "output_model_dir = 'modified_model'\n",
    "from modelscope import Model\n",
    "from swift import Swift\n",
    "import torch\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, \n",
    "    torch_dtype=torch.bfloat16, \n",
    "    device_map='auto', \n",
    "    trust_remote_code=True\n",
    ")\n",
    "model = Swift.from_pretrained(model, './output')\n",
    "\n",
    "# 将数据分成小批次\n",
    "for i in range(0, len(Summary_set), batch_size):\n",
    "    batch_data = Summary_set[i:i+batch_size]\n",
    "    print(f\"Processing batch {i//batch_size + 1}...\")\n",
    "\n",
    "    # 准备输入数据\n",
    "    batch_q = [prompt_task + ''.join(data['article']) + \"\\n概括 :\" for data in batch_data]\n",
    "    batch_a = [data['summary'] for data in batch_data]\n",
    "    # print(batch_q)\n",
    "    # 将批量数据转化为模型输入\n",
    "    inputs = tokenizer(batch_q, padding=True, truncation=True, return_tensors=\"pt\").to('cuda')\n",
    "    \n",
    "    # 生成输出\n",
    "    generated_ids = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=128\n",
    "    )\n",
    "\n",
    "    # 解码生成的文本\n",
    "    responses = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "   \n",
    "    \n",
    "    # 处理每个生成的文本\n",
    "    for j, response in enumerate(responses):\n",
    "        matches = re.findall(r\"概括 :(.*?)\\<END\\>\", response, re.DOTALL)\n",
    "        if len(matches) > 1:\n",
    "            all_pre.append(matches[1])\n",
    "            all_answer.append(batch_a[j])\n",
    "        else:\n",
    "            print(f\"第 {i + j} 个数据未匹配到足够的内容，重新提问...\")\n",
    "\n",
    "    \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-11-16T15:00:42.331615Z",
     "iopub.status.busy": "2024-11-16T15:00:42.331239Z",
     "iopub.status.idle": "2024-11-16T15:00:42.362543Z",
     "shell.execute_reply": "2024-11-16T15:00:42.361959Z",
     "shell.execute_reply.started": "2024-11-16T15:00:42.331594Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of predictions: 584\n",
      "Number of references: 584\n",
      "ROUGE Scores:\n",
      "ROUGE-1: 0.0526\n",
      "ROUGE-2: 0.0113\n",
      "ROUGE-L: 0.0535\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import evaluate\n",
    "\n",
    "# 从文件中读取数据\n",
    "with open('predictions.json', 'r') as f:\n",
    "    all_pre = json.load(f)\n",
    "\n",
    "with open('references.json', 'r') as f:\n",
    "    all_answer = json.load(f)\n",
    "\n",
    "# 确保数据已经正确读取\n",
    "print(f\"Number of predictions: {len(all_pre)}\")\n",
    "print(f\"Number of references: {len(all_answer)}\")\n",
    "\n",
    "# 加载 ROUGE 评估器\n",
    "rouge = evaluate.load('rouge')\n",
    "\n",
    "# 计算 ROUGE 分数\n",
    "results = rouge.compute(predictions=all_pre, references=all_answer)\n",
    "\n",
    "# 输出结果\n",
    "print(\"ROUGE Scores:\")\n",
    "print(f\"ROUGE-1: {results['rouge1']:.4f}\")\n",
    "print(f\"ROUGE-2: {results['rouge2']:.4f}\")\n",
    "print(f\"ROUGE-L: {results['rougeL']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 神经网络模型与LLM在NER任务上的对比\n",
    "本质上：一个是分类器 一个是生成模型\n",
    "在训练过程中\n",
    "* LLM F1-macro  \n",
    "  * 数次实验都低于 0.1；调整prompt并没有起到很好的效果；很大取决于模型能力与提示词的引导\n",
    "  * 在预训练模型上，只需要少量数据，就可以达到一定效果,但整体效果不如BiLSTM模型\n",
    "  * 泛化能力强，自身携带大量的知识，但是基于Prompt输出效果的提高具有随机性，是一个黑盒\n",
    "  * 需要确定最后的输出内容，然后通过数据处理手段去提取生成的答案\n",
    "  * 生成答案的长度也有可能与序列不一致，需要去调整，这是大模型本身就存在的问题，对数字不敏感\n",
    "* BiLSTM模型 F1-macro 0.7\n",
    "  * 需要大量数据训练\n",
    "  * 在固定任务上，有明确的优化目标，更能够去调试模型\n",
    "  * 结果明确，可以直接使用\n",
    "\n",
    "根据先前的实验结果，有以下对比：\n",
    "\n",
    "| 特性                    | BiLSTM                      | LLM + Prompt                  |\n",
    "|-----------------------|--------------------------------------------|--------------------------------------------|\n",
    "| **模型结构**          | Glove + BiLSTM      **分类模型**            | Qwen  **生成式模型**      |\n",
    "| **输入数据处理**      | 需要标签化数据，配合词嵌入        |依赖 tokenizer 将文本转为 token 序列，再引导生成基于上下文的输出        |\n",
    "| **特征提取方式**      | 预训练嵌入向量，已经LSTM提取语义             | 自动从上下文中提取特征，无需手动特征设计      |\n",
    "| **训练复杂度**        | 较低，参数量相对较少，易于调参               | 非常高，参数量大，依赖大规模计算资源        |\n",
    "| **训练数据需求**      | 需要大量标签数据以提高效果                   | 可以通过few-shot并配合 prompt 调整效果，少量数据更强   |\n",
    "| **推理速度**          | 快速，适合实时应用                           | 较慢，尤其是在大型模型情况下                 |\n",
    "| **适应性**            | 针对任务训练，较难泛化到其他任务              | 通过 prompt 调整，具备一定的跨任务泛化能力   |\n",
    "| **性能**              | 表现稳定，准确性依赖于标注数据和训练         | 数据稀少情况下，由于自身的知识更多，比LSTM表现好，整体上准确率提高太依赖prompt了，有随机性       |\n",
    "| **输出控制**          | 明确控制输出，通过模型结构和损失函数调节       | 通过 prompt 控制输出，灵活性较高但难于完全控制；需要最后通过数据处理手段去提取答案 |\n",
    "| **解释性**            | 结构清晰，易于理解输出和调试                 | 黑盒性强，输出依赖模型内部推断过程           |\n",
    "| **资源需求**          | 资源需求相对较低                             | 资源需求大，需要 GPU 等计算设备支持         |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 总结\n",
    "#### 结果\n",
    "* NER \n",
    "  * Test1\n",
    "    F1 Score: 0.0239\n",
    "  * Test2\n",
    "    F1 Score: 0.0344\n",
    "    \n",
    "* Summary\n",
    "  * Test1\n",
    "    ROUGE Scores:\n",
    "    ROUGE-1: 0.0695\n",
    "    ROUGE-2: 0.0099\n",
    "    ROUGE-L: 0.0690\n",
    "  * Test2\n",
    "    ROUGE Scores:\n",
    "    ROUGE-1: 0.0526\n",
    "    ROUGE-2: 0.0113\n",
    "    ROUGE-L: 0.0535\n",
    "#### 想法\n",
    "* 预训练模型能力很大决定了任务的效果上限，GPT3.5输出的结果明显比Qwen更符合要求\n",
    "* prompt只能起到锦上添花的作用，总不能让奔波儿灞去除掉唐僧师徒吧；GPT3.5都干不好的活，Qwen 0.5b....\n",
    "* prompt可能会起到反作用 ，例如NER任务的例子第一个tag是S-PRE，那么很容易的结果的第一个就会被引导成S-PRE\n",
    "* 微调模型需要更为细致的数据集分配，粗糙的数据集可以让模型输出内容更符合要求，但质量不一定能提高\n",
    "* 训练模型需要的资源太大了，Cuda OOM 真的让人痛苦\n",
    "* 对固定的NLP任务，传统方法是具有一定优势的，而大模型泛化能力更强\n",
    "  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
